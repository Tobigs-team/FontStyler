{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 124
    },
    "colab_type": "code",
    "id": "Uda6cF4tEjxW",
    "outputId": "0fe99d9a-edc8-4da3-e574-5a013a3f51c1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Vpzhx1eNEkgN"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '/content/drive/My Drive/Colab Notebooks/FontStyler/src/data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h1PbUnhPFAXM"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import io\n",
    "import csv\n",
    "from common.dataset import PickledImageProvider\n",
    "from common.dataset import KoreanFontDataset\n",
    "from common.utils import pad_seq, bytes_to_file, \\\n",
    "    read_split_image, shift_and_resize_image, normalize_image, \\\n",
    "    tight_crop_image, add_padding\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "w_nYJYp_FATS"
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "import os.path\n",
    "\n",
    "import torch.utils.data\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "def default_image_loader(path):\n",
    "    return Image.open(path).convert('RGB')\n",
    "\n",
    "class TripletImageLoader(torch.utils.data.Dataset): \n",
    "    def __init__(self, pickled, triplets_file_name, base_path=None, filenames_filename=None, transform=None,\n",
    "                 loader=default_image_loader):\n",
    "        \"\"\" \n",
    "        filenames_filename: \n",
    "            A text file with each line containing the path to an image e.g.,\n",
    "            images/class1/sample.jpg\n",
    "                \n",
    "        triplets_file_name: \n",
    "            A text file with each line containing three integers, \n",
    "            where integer i refers to the i-th image in the filenames file. \n",
    "            For a line of intergers 'a b c', a triplet is defined such that image a is more \n",
    "            similar to image c than it is to image b, \n",
    "            e.g., 0 2017 42 \n",
    "        \"\"\"\n",
    "        self.dset = pickled.examples\n",
    "#         self.base_path = base_path  \n",
    "#         self.filenamelist = []\n",
    "#         for line in open(filenames_filename):\n",
    "#             self.filenamelist.append(line.rstrip('\\n'))\n",
    "        triplets = []\n",
    "        anchor_labels = [] #\n",
    "        for line in open(triplets_file_name):\n",
    "            triplets.append((line.split()[0], line.split()[1], line.split()[2])) # anchor, far, close\n",
    "            anchor_labels.append(int(line.split()[3])) #\n",
    "        self.triplets = triplets\n",
    "        self.labels = anchor_labels #\n",
    "        self.transform = transform\n",
    "        self.loader = loader\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        path1, path2, path3 = self.triplets[index]\n",
    "        anchor_label = self.labels[index]\n",
    "        img1_tuple = self.dset[int(path1)]\n",
    "        img2_tuple = self.dset[int(path2)]\n",
    "        img3_tuple = self.dset[int(path3)]\n",
    "        \n",
    "        info = {                         # clustering을 위해 anchor_index도 추가하였다.\n",
    "            'anchor_index': int(path1),\n",
    "            'anchor_label': anchor_label\n",
    "        }\n",
    "        \n",
    "        # byte만 사용할 예정\n",
    "        img1, byte_1 = img1_tuple[0], img1_tuple[1]\n",
    "        img2, byte_2 = img2_tuple[0], img2_tuple[1]\n",
    "        img3, byte_3 = img3_tuple[0], img3_tuple[1]\n",
    "        \n",
    "        # bytes 타입을 numpy array로 변경 후 normalize\n",
    "        img_arr_1 = np.array(Image.open(io.BytesIO(byte_1)))\n",
    "        img_arr_1 = normalize_image(img_arr_1)\n",
    "        \n",
    "        img_arr_2 = np.array(Image.open(io.BytesIO(byte_2)))\n",
    "        img_arr_2 = normalize_image(img_arr_2)\n",
    "        \n",
    "        img_arr_3 = np.array(Image.open(io.BytesIO(byte_3)))\n",
    "        img_arr_3 = normalize_image(img_arr_3)\n",
    "\n",
    "        cropped_image_1, cropped_image_size_1 = tight_crop_image(img_arr_1, verbose=False)\n",
    "        centered_image_1 = add_padding(cropped_image_1, verbose=False)\n",
    "        \n",
    "        cropped_image_2, cropped_image_size_2 = tight_crop_image(img_arr_2, verbose=False)\n",
    "        centered_image_2 = add_padding(cropped_image_2, verbose=False)\n",
    "        \n",
    "        cropped_image_3, cropped_image_size_3 = tight_crop_image(img_arr_3, verbose=False)\n",
    "        centered_image_3 = add_padding(cropped_image_3, verbose=False)\n",
    "            \n",
    "        return (centered_image_1, centered_image_2, centered_image_3), info #\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.triplets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BwklBemVFAQe"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import shutil\n",
    "import os\n",
    "import io\n",
    "\n",
    "class Tripletnet(nn.Module):\n",
    "    def __init__(self, embeddingnet):\n",
    "        super(Tripletnet, self).__init__()\n",
    "        self.embeddingnet = embeddingnet\n",
    "\n",
    "    def forward(self, x, y, z):\n",
    "        embedded_x = self.embeddingnet(x)\n",
    "        embedded_y = self.embeddingnet(y)\n",
    "        embedded_z = self.embeddingnet(z)\n",
    "        dist_a = F.pairwise_distance(embedded_x, embedded_y, 2)\n",
    "        dist_b = F.pairwise_distance(embedded_x, embedded_z, 2)\n",
    "        return dist_a, dist_b, embedded_x, embedded_y, embedded_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "08N3mTjiFAOJ"
   },
   "outputs": [],
   "source": [
    "# FontStyler의 convAE 코드 (layers.py)\n",
    "class Encoder_conv(nn.Module):\n",
    "    \n",
    "    def __init__(self, img_dim=1, conv_dim=16): # output dim은 128이 될 것\n",
    "        super(Encoder_conv, self).__init__()\n",
    "        self.conv1 = conv2d(img_dim, conv_dim, k_size=5, stride=2, pad=2, dilation=2, lrelu=False, bn=False)\n",
    "        self.conv2 = conv2d(conv_dim, conv_dim*2, k_size=5, stride=4, pad=2, dilation=2)\n",
    "        self.conv3 = conv2d(conv_dim*2, conv_dim*4, k_size=4, stride=4, pad=1, dilation=1)\n",
    "        self.conv4 = conv2d(conv_dim*4, conv_dim*8)\n",
    "        self.conv5 = conv2d(conv_dim*8, conv_dim*8)\n",
    "    \n",
    "    def forward(self, images):\n",
    "        # |images| = (batch, img, img)\n",
    "        # print(images.shape)\n",
    "        images = images.unsqueeze(dim=1)\n",
    "        # |images| = (batch, 1, 128, 128)\n",
    "        # print(images.shape)\n",
    "        e1 = self.conv1(images)\n",
    "        # |e1| = (batch, conv_dim, 64, 64)\n",
    "        # print(e1.shape)\n",
    "        e2 = self.conv2(e1)\n",
    "        # |e2| = (batch, conv_dim*2, 16, 16)\n",
    "        # print(e2.shape)\n",
    "        e3 = self.conv3(e2)\n",
    "        # |e3| = (batch, conv_dim*4, 4, 4)\n",
    "        # print(e3.shape)\n",
    "        e4 = self.conv4(e3)\n",
    "        # |e4| = (batch, conv_dim*8, 2, 2)\n",
    "        # print(e4.shape)\n",
    "        encoded_source = self.conv5(e4)\n",
    "        # |encoded_source| = (batch, conv_dim*8, 1, 1)\n",
    "        # print(encoded_source.shape)\n",
    "        \n",
    "        return encoded_source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FBZlfX_3Gnk9"
   },
   "outputs": [],
   "source": [
    "# function.py\n",
    "import torch.nn as nn\n",
    "\n",
    "def batch_norm(c_out, momentum=0.1):\n",
    "    return nn.BatchNorm2d(c_out, momentum=momentum)\n",
    "\n",
    "def conv2d(c_in, c_out, k_size=3, stride=2, pad=1, dilation=1, bn=True, lrelu=True, leak=0.2):\n",
    "    layers = []\n",
    "    if lrelu:\n",
    "        layers.append(nn.LeakyReLU(leak))\n",
    "    layers.append(nn.Conv2d(c_in, c_out, k_size, stride, pad))\n",
    "    if bn:\n",
    "        layers.append(nn.BatchNorm2d(c_out))\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "def deconv2d(c_in, c_out, k_size=3, stride=1, pad=1, dilation=1, bn=True, dropout=False, p=0.5):\n",
    "    layers = []\n",
    "    layers.append(nn.LeakyReLU(0.2))\n",
    "    layers.append(nn.ConvTranspose2d(c_in, c_out, k_size, stride, pad))\n",
    "    if bn:\n",
    "        layers.append(nn.BatchNorm2d(c_out))\n",
    "    if dropout:\n",
    "        layers.append(nn.Dropout(p))\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "def lrelu(leak=0.2):\n",
    "    return nn.LeakyReLU(leak)\n",
    "\n",
    "def dropout(p=0.2):\n",
    "    return nn.Dropout(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "lH1b-ZoUFALM",
    "outputId": "23ebf54c-c858-44ce-d3b1-6d608e35d9ac"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 9,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint_pth = '/content/drive/My Drive/Colab Notebooks/FontStyler/src/data/runs/TripleNet/model_best.pth.tar'\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "best = torch.load(checkpoint_pth)\n",
    "\n",
    "# 학습된 TripletNet 로드\n",
    "model = Encoder_conv()\n",
    "tnet  = Tripletnet(model)\n",
    "tnet.load_state_dict(best['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 484
    },
    "colab_type": "code",
    "id": "zRFOlw6jFAI8",
    "outputId": "d5b1ee04-21d8-4ab5-8f01-3fbe3ba5f068"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tripletnet(\n",
       "  (embeddingnet): Encoder_conv(\n",
       "    (conv1): Sequential(\n",
       "      (0): Conv2d(1, 16, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
       "    )\n",
       "    (conv2): Sequential(\n",
       "      (0): LeakyReLU(negative_slope=0.2)\n",
       "      (1): Conv2d(16, 32, kernel_size=(5, 5), stride=(4, 4), padding=(2, 2))\n",
       "      (2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (conv3): Sequential(\n",
       "      (0): LeakyReLU(negative_slope=0.2)\n",
       "      (1): Conv2d(32, 64, kernel_size=(4, 4), stride=(4, 4), padding=(1, 1))\n",
       "      (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (conv4): Sequential(\n",
       "      (0): LeakyReLU(negative_slope=0.2)\n",
       "      (1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (conv5): Sequential(\n",
       "      (0): LeakyReLU(negative_slope=0.2)\n",
       "      (1): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tnet.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4swmrWx4GzYU"
   },
   "source": [
    "## K-means clustering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hIqS6sHqOTF8"
   },
   "outputs": [],
   "source": [
    "dset = KoreanFontDataset(PickledImageProvider('/content/drive/My Drive/Colab Notebooks/FontStyler/src/data/dataset/kor/latent.obj'), vector_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 345
    },
    "colab_type": "code",
    "id": "sdk71ErnOXcV",
    "outputId": "f6cc7761-63ca-49b8-cfe4-923bdc29d03d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'font_doc2vec': [2.2403063999999997,\n",
       "   -1.4756056999999998,\n",
       "   -0.593018,\n",
       "   -0.18605323,\n",
       "   -1.2381212,\n",
       "   -1.161201,\n",
       "   2.8255024,\n",
       "   0.10454782,\n",
       "   -0.16260550000000001,\n",
       "   1.2440913999999998],\n",
       "  'font_index': 0,\n",
       "  'word_index': 0},\n",
       " array([[1., 1., 1., ..., 1., 1., 1.],\n",
       "        [1., 1., 1., ..., 1., 1., 1.],\n",
       "        [1., 1., 1., ..., 1., 1., 1.],\n",
       "        ...,\n",
       "        [1., 1., 1., ..., 1., 1., 1.],\n",
       "        [1., 1., 1., ..., 1., 1., 1.],\n",
       "        [1., 1., 1., ..., 1., 1., 1.]]))"
      ]
     },
     "execution_count": 48,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y6EZiR1gG62X"
   },
   "outputs": [],
   "source": [
    "pickled = PickledImageProvider('/content/drive/My Drive/Colab Notebooks/FontStyler/src/data/dataset/kor/latent.obj')\n",
    "triplet_loader = TripletImageLoader(pickled, '/content/drive/My Drive/Colab Notebooks/FontStyler/src/data/triplet_list_with_label.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o0W2tIQlFAFT"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "idxs = list(range(len(triplet_loader)))\n",
    "np.random.shuffle(idxs)\n",
    "batch_size = 32\n",
    "\n",
    "sampler = SubsetRandomSampler(idxs) # 전체 샘플링\n",
    "loader = torch.utils.data.DataLoader(\n",
    "        dset,\n",
    "        batch_size = batch_size,\n",
    "        sampler = None\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3HZQ302MPFB_"
   },
   "outputs": [],
   "source": [
    "# 순차적으로 데이터를 호출하도록 수정\n",
    "tnet.eval()\n",
    "with torch.no_grad():\n",
    "    anchors = []\n",
    "    fonts  = []\n",
    "    letters = []\n",
    "    for idx, (info, img) in enumerate(loader): \n",
    "        if device == 'cuda':\n",
    "            img = img.cuda()\n",
    "        img = img.float()\n",
    "        dummy_0, dummy_1 = img, img\n",
    "\n",
    "        _, _, anchor, _, _ = tnet(img, dummy_0, dummy_1) # img2, img3에는 더미데이터\n",
    "        anchors.append(anchor) \n",
    "        fonts.append(info['font_index'])\n",
    "        letters.append(info['word_index'])\n",
    "        # if idx == 1000:\n",
    "        #     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8eBV3uINFAAl"
   },
   "outputs": [],
   "source": [
    "# # 순차적으로 데이터를 호출하도록 수정\n",
    "# tnet.eval()\n",
    "# with torch.no_grad():\n",
    "#     anchors = []\n",
    "#     labels  = []\n",
    "#     indexes = []\n",
    "#     for idx, ((img1, img2, img3), info) in enumerate(loader): \n",
    "#         if device == 'cuda':\n",
    "#             img1, img2, img3 = img1.cuda(), img2.cuda(), img3.cuda()\n",
    "#         img1, img2, img3 = img1.float(), img2.float(), img3.float()\n",
    "\n",
    "#         _, _, anchor, _, _ = tnet(img1, img2, img3) \n",
    "#         anchors.append(anchor) \n",
    "#         labels.append(info['anchor_label'])\n",
    "#         indexes.append(info['anchor_index'])\n",
    "#         # if idx == 1000:\n",
    "#         #     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "mj4V-ibfE_-B",
    "outputId": "21f95932-0ff0-4e32-fa54-002199adf48b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7858, 7858, 7858)"
      ]
     },
     "execution_count": 62,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(anchors), len(fonts), len(letters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "iYM7DqD2E_7p",
    "outputId": "f88c3032-332f-493e-c484-a1ac8753ff6e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 128, 1, 1])"
      ]
     },
     "execution_count": 63,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anchors[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "nnn_8FCDE_2h",
    "outputId": "3ac1bfdb-db45-4425-9716-54c080267a3c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "latent shape: (251450, 128, 1, 1)\n",
      "fonts shape: (251450,)\n",
      "letter shape: (251450,)\n"
     ]
    }
   ],
   "source": [
    "latent = np.zeros((1, 128, 1, 1))\n",
    "for i in range(len(anchors)):\n",
    "    latent = np.concatenate([latent, anchors[i].cpu()])\n",
    "latent = latent[1:]\n",
    "print('latent shape: {}'.format(latent.shape))\n",
    "\n",
    "tmp_font = np.zeros((1))\n",
    "for i in range(len(fonts)):\n",
    "    tmp_font = np.concatenate([tmp_font, fonts[i].cpu()])\n",
    "tmp_font = tmp_font[1:]\n",
    "print('fonts shape: {}'.format(tmp_font.shape))\n",
    "\n",
    "tmp_letter = np.zeros((1))\n",
    "for i in range(len(letters)):\n",
    "    tmp_letter = np.concatenate([tmp_letter, letters[i].cpu()])\n",
    "tmp_letter = tmp_letter[1:]\n",
    "print('letter shape: {}'.format(tmp_letter.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "YscugcWeHT5N",
    "outputId": "b98e897b-3ed6-44a7-d0fa-2049b7a16f13"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(251450, 128)"
      ]
     },
     "execution_count": 65,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latent = latent.reshape(251450,128)\n",
    "latent.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JcwR0jfgHU2B"
   },
   "outputs": [],
   "source": [
    "# clustering\n",
    "model = KMeans(n_clusters=5)\n",
    "model.fit(latent)\n",
    "\n",
    "y_predict = model.fit_predict(latent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "ACDGL3fbHUz5",
    "outputId": "a3b7c423-e023-4207-f7ab-0b8b1ceca153"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 3 3 3 3 3 3 3 3 3 3 3 3 3 4 3 3 3 3 3 3 3 3 3 3 3 3 4 4 3 3 3 4 3 3 3 3\n",
      " 3 4 4 3 3 3 3 3 3 3 3 3 3 4 4 3 3 4 4 3 3 4 3 3 4 3 4 4 4 4 3 4 3 4 0 4 4\n",
      " 4 4 4 4 4 4 3 3 4 3 3 4 3 3 4 4 3 4 3 3 3 3 3 3 3 3]\n"
     ]
    }
   ],
   "source": [
    "print(y_predict[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-GLaHV0CHl7t"
   },
   "source": [
    "---\n",
    "## \"폰트 idx\" : \"클러스터 평균 벡터\" 매핑\n",
    "\n",
    "### 평균 벡터 구하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g73lsLdiHUxW"
   },
   "outputs": [],
   "source": [
    "cluster_vector = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "d1YqQU82HUu9",
    "outputId": "8918895d-5bff-4c02-c8e0-8371dbff1a19"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average vector for cluster 0 with 51661 data\n"
     ]
    }
   ],
   "source": [
    "# cluster 0\n",
    "count = 0\n",
    "cluster = 0\n",
    "vector_size = 128\n",
    "vectors = np.zeros(vector_size,)\n",
    "\n",
    "for i, (idx, c) in enumerate(zip(tmp_index, y_predict)): \n",
    "    if c == cluster:\n",
    "        vectors += latent[int(idx)]\n",
    "        count += 1\n",
    "avg_vector_0 = vectors / count\n",
    "print('average vector for cluster {} with {} data'.format(cluster, count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "h7t15TrSIZyr",
    "outputId": "e5cfc9f2-ecc5-4128-cceb-df2860ed38ba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average vector for cluster 1 with 74310 data\n"
     ]
    }
   ],
   "source": [
    "# cluster 1\n",
    "count = 0\n",
    "cluster = 1\n",
    "vector_size = 128\n",
    "vectors = np.zeros(vector_size,)\n",
    "\n",
    "for i, (idx, c) in enumerate(zip(tmp_index, y_predict)): \n",
    "    if c == cluster:\n",
    "        vectors += latent[int(idx)]\n",
    "        count += 1\n",
    "avg_vector_1 = vectors / count\n",
    "print('average vector for cluster {} with {} data'.format(cluster, count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "7Iy9BKFOHUsj",
    "outputId": "8dfead7b-8ac0-480e-f37e-9327184738ab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average vector for cluster 2 with 27733 data\n"
     ]
    }
   ],
   "source": [
    "# cluster 2\n",
    "count = 0\n",
    "cluster = 2\n",
    "vector_size = 128\n",
    "vectors = np.zeros(vector_size,)\n",
    "\n",
    "for i, (idx, c) in enumerate(zip(tmp_index, y_predict)): \n",
    "    if c == cluster:\n",
    "        vectors += latent[int(idx)]\n",
    "        count += 1\n",
    "avg_vector_2 = vectors / count\n",
    "print('average vector for cluster {} with {} data'.format(cluster, count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "3n01OyWfHUp7",
    "outputId": "18f20b08-6662-4eab-e89d-386f51ce89e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average vector for cluster 3 with 39621 data\n"
     ]
    }
   ],
   "source": [
    "# cluster 3\n",
    "count = 0\n",
    "cluster = 3\n",
    "vector_size = 128\n",
    "vectors = np.zeros(vector_size,)\n",
    "\n",
    "for i, (idx, c) in enumerate(zip(tmp_index, y_predict)): \n",
    "    if c == cluster:\n",
    "        vectors += latent[int(idx)]\n",
    "        count += 1\n",
    "avg_vector_3 = vectors / count\n",
    "print('average vector for cluster {} with {} data'.format(cluster, count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "FprdeIifHUna",
    "outputId": "fea6c5c1-45af-41c0-988d-de84e7be932e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average vector for cluster 4 with 58125 data\n"
     ]
    }
   ],
   "source": [
    "# cluster 4\n",
    "count = 0\n",
    "cluster = 4\n",
    "vector_size = 128\n",
    "vectors = np.zeros(vector_size,)\n",
    "\n",
    "for i, (idx, c) in enumerate(zip(tmp_index, y_predict)): \n",
    "    if c == cluster:\n",
    "        vectors += latent[int(idx)]\n",
    "        count += 1\n",
    "avg_vector_4 = vectors / count\n",
    "print('average vector for cluster {} with {} data'.format(cluster, count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z02AviCeHUkx"
   },
   "outputs": [],
   "source": [
    "cluster_vector = {\n",
    "    0: avg_vector_0,\n",
    "    1: avg_vector_1,\n",
    "    2: avg_vector_2,\n",
    "    3: avg_vector_3,\n",
    "    4: avg_vector_4,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sL_7Q6P2Inx0"
   },
   "source": [
    "### 폰트 인덱스에 할당"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "hG__J9tuYoh8",
    "outputId": "44845225-8fa0-40b7-ffa5-af25d9d4658b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "# 가장 높은 빈도 가진 클러스터 출력 (test)\n",
    "import collections\n",
    "cc = collections.Counter(y_predict[:2350])\n",
    "print(cc.most_common(1)[0][0]) # 3번 클러스터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RXCYI3cOI1JI"
   },
   "outputs": [],
   "source": [
    "font_idx_with_cluster_vec = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "yjoNRV8aImOp",
    "outputId": "33ab49ed-3664-48be-d249-ef40af317260"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "font 0's cluster: 3\n",
      "font 1's cluster: 1\n",
      "font 2's cluster: 3\n",
      "font 3's cluster: 4\n",
      "font 4's cluster: 4\n",
      "font 5's cluster: 4\n",
      "font 6's cluster: 0\n",
      "font 7's cluster: 2\n",
      "font 8's cluster: 1\n",
      "font 9's cluster: 0\n",
      "font 10's cluster: 2\n",
      "font 11's cluster: 1\n",
      "font 12's cluster: 1\n",
      "font 13's cluster: 4\n",
      "font 14's cluster: 4\n",
      "font 15's cluster: 0\n",
      "font 16's cluster: 0\n",
      "font 17's cluster: 1\n",
      "font 18's cluster: 2\n",
      "font 19's cluster: 1\n",
      "font 20's cluster: 0\n",
      "font 21's cluster: 0\n",
      "font 22's cluster: 3\n",
      "font 23's cluster: 0\n",
      "font 24's cluster: 1\n",
      "font 25's cluster: 4\n",
      "font 26's cluster: 1\n",
      "font 27's cluster: 3\n",
      "font 28's cluster: 4\n",
      "font 29's cluster: 0\n",
      "font 30's cluster: 1\n",
      "font 31's cluster: 0\n",
      "font 32's cluster: 1\n",
      "font 33's cluster: 4\n",
      "font 34's cluster: 1\n",
      "font 35's cluster: 0\n",
      "font 36's cluster: 0\n",
      "font 37's cluster: 3\n",
      "font 38's cluster: 0\n",
      "font 39's cluster: 1\n",
      "font 40's cluster: 3\n",
      "font 41's cluster: 4\n",
      "font 42's cluster: 1\n",
      "font 43's cluster: 3\n",
      "font 44's cluster: 4\n",
      "font 45's cluster: 0\n",
      "font 46's cluster: 1\n",
      "font 47's cluster: 0\n",
      "font 48's cluster: 1\n",
      "font 49's cluster: 1\n",
      "font 50's cluster: 0\n",
      "font 51's cluster: 0\n",
      "font 52's cluster: 2\n",
      "font 53's cluster: 4\n",
      "font 54's cluster: 4\n",
      "font 55's cluster: 0\n",
      "font 56's cluster: 2\n",
      "font 57's cluster: 1\n",
      "font 58's cluster: 0\n",
      "font 59's cluster: 2\n",
      "font 60's cluster: 1\n",
      "font 61's cluster: 4\n",
      "font 62's cluster: 2\n",
      "font 63's cluster: 4\n",
      "font 64's cluster: 1\n",
      "font 65's cluster: 1\n",
      "font 66's cluster: 3\n",
      "font 67's cluster: 0\n",
      "font 68's cluster: 2\n",
      "font 69's cluster: 3\n",
      "font 70's cluster: 4\n",
      "font 71's cluster: 0\n",
      "font 72's cluster: 3\n",
      "font 73's cluster: 4\n",
      "font 74's cluster: 4\n",
      "font 75's cluster: 1\n",
      "font 76's cluster: 3\n",
      "font 77's cluster: 4\n",
      "font 78's cluster: 4\n",
      "font 79's cluster: 3\n",
      "font 80's cluster: 3\n",
      "font 81's cluster: 2\n",
      "font 82's cluster: 0\n",
      "font 83's cluster: 3\n",
      "font 84's cluster: 1\n",
      "font 85's cluster: 1\n",
      "font 86's cluster: 4\n",
      "font 87's cluster: 0\n",
      "font 88's cluster: 1\n",
      "font 89's cluster: 1\n",
      "font 90's cluster: 1\n",
      "font 91's cluster: 4\n",
      "font 92's cluster: 2\n",
      "font 93's cluster: 4\n",
      "font 94's cluster: 1\n",
      "font 95's cluster: 1\n",
      "font 96's cluster: 0\n",
      "font 97's cluster: 4\n",
      "font 98's cluster: 1\n",
      "font 99's cluster: 1\n",
      "font 100's cluster: 0\n",
      "font 101's cluster: 4\n",
      "font 102's cluster: 1\n",
      "font 103's cluster: 1\n",
      "font 104's cluster: 3\n",
      "font 105's cluster: 2\n",
      "font 106's cluster: 4\n"
     ]
    }
   ],
   "source": [
    "count_0 = 0\n",
    "count_1 = 0\n",
    "count_2 = 0\n",
    "count_3 = 0\n",
    "count_4 = 0\n",
    "\n",
    "vector_size = 128\n",
    "'''\n",
    "    같은 폰트라도 글자마다 다른 cluster로 분류될 수 있다.\n",
    "    2350자 중 가장 많은 클러스터로 분류된 쪽으로 평균 latent vector를 할당한다.\n",
    "'''\n",
    "\n",
    "total = len(dset)\n",
    "font_idx = 0\n",
    "\n",
    "for i in range(0, total, 2350):\n",
    "    counter = collections.Counter(y_predict[i : i+2350])\n",
    "    clust = counter.most_common(1)[0][0]\n",
    "    font_idx_with_cluster_vec[font_idx] = cluster_vector[clust]\n",
    "\n",
    "    print(\"font {}'s cluster: {}\".format(font_idx, clust))\n",
    "    font_idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "jt1aqueqIm_a",
    "outputId": "0e3c1a04-911d-4072-a54b-f62e103d4f22"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "107"
      ]
     },
     "execution_count": 91,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(font_idx_with_cluster_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9PWm3wSjbAKJ"
   },
   "source": [
    "## Pickle로 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1va1oFlYIm9Q"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('/content/drive/My Drive/Colab Notebooks/FontStyler/src/data/font_idx_with_cluster_vec.p', 'wb') as file:\n",
    "    pickle.dump(font_idx_with_cluster_vec, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "xJWv3OYjIm6x",
    "outputId": "0896df89-e880-4da2-f694-27525fab5771"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "107\n"
     ]
    }
   ],
   "source": [
    "# 확인\n",
    "with open('/content/drive/My Drive/Colab Notebooks/FontStyler/src/data/font_idx_with_cluster_vec.p', 'rb') as file:\n",
    "    vec_test = pickle.load(file)\n",
    "    print(len(vec_test))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "14_final_average_vector.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
