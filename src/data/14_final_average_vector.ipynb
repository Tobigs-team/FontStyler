{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "14_final_average_vector.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uda6cF4tEjxW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "outputId": "0fe99d9a-edc8-4da3-e574-5a013a3f51c1"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vpzhx1eNEkgN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "sys.path.insert(0, '/content/drive/My Drive/Colab Notebooks/FontStyler/src/data/')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h1PbUnhPFAXM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import io\n",
        "import csv\n",
        "from common.dataset import PickledImageProvider\n",
        "from common.dataset import KoreanFontDataset\n",
        "from common.utils import pad_seq, bytes_to_file, \\\n",
        "    read_split_image, shift_and_resize_image, normalize_image, \\\n",
        "    tight_crop_image, add_padding\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w_nYJYp_FATS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from PIL import Image\n",
        "import os\n",
        "import os.path\n",
        "\n",
        "import torch.utils.data\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "def default_image_loader(path):\n",
        "    return Image.open(path).convert('RGB')\n",
        "\n",
        "class TripletImageLoader(torch.utils.data.Dataset): \n",
        "    def __init__(self, pickled, triplets_file_name, base_path=None, filenames_filename=None, transform=None,\n",
        "                 loader=default_image_loader):\n",
        "        \"\"\" \n",
        "        filenames_filename: \n",
        "            A text file with each line containing the path to an image e.g.,\n",
        "            images/class1/sample.jpg\n",
        "                \n",
        "        triplets_file_name: \n",
        "            A text file with each line containing three integers, \n",
        "            where integer i refers to the i-th image in the filenames file. \n",
        "            For a line of intergers 'a b c', a triplet is defined such that image a is more \n",
        "            similar to image c than it is to image b, \n",
        "            e.g., 0 2017 42 \n",
        "        \"\"\"\n",
        "        self.dset = pickled.examples\n",
        "#         self.base_path = base_path  \n",
        "#         self.filenamelist = []\n",
        "#         for line in open(filenames_filename):\n",
        "#             self.filenamelist.append(line.rstrip('\\n'))\n",
        "        triplets = []\n",
        "        anchor_labels = [] #\n",
        "        for line in open(triplets_file_name):\n",
        "            triplets.append((line.split()[0], line.split()[1], line.split()[2])) # anchor, far, close\n",
        "            anchor_labels.append(int(line.split()[3])) #\n",
        "        self.triplets = triplets\n",
        "        self.labels = anchor_labels #\n",
        "        self.transform = transform\n",
        "        self.loader = loader\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        path1, path2, path3 = self.triplets[index]\n",
        "        anchor_label = self.labels[index]\n",
        "        img1_tuple = self.dset[int(path1)]\n",
        "        img2_tuple = self.dset[int(path2)]\n",
        "        img3_tuple = self.dset[int(path3)]\n",
        "        \n",
        "        info = {                         # clustering을 위해 anchor_index도 추가하였다.\n",
        "            'anchor_index': int(path1),\n",
        "            'anchor_label': anchor_label\n",
        "        }\n",
        "        \n",
        "        # byte만 사용할 예정\n",
        "        img1, byte_1 = img1_tuple[0], img1_tuple[1]\n",
        "        img2, byte_2 = img2_tuple[0], img2_tuple[1]\n",
        "        img3, byte_3 = img3_tuple[0], img3_tuple[1]\n",
        "        \n",
        "        # bytes 타입을 numpy array로 변경 후 normalize\n",
        "        img_arr_1 = np.array(Image.open(io.BytesIO(byte_1)))\n",
        "        img_arr_1 = normalize_image(img_arr_1)\n",
        "        \n",
        "        img_arr_2 = np.array(Image.open(io.BytesIO(byte_2)))\n",
        "        img_arr_2 = normalize_image(img_arr_2)\n",
        "        \n",
        "        img_arr_3 = np.array(Image.open(io.BytesIO(byte_3)))\n",
        "        img_arr_3 = normalize_image(img_arr_3)\n",
        "\n",
        "        cropped_image_1, cropped_image_size_1 = tight_crop_image(img_arr_1, verbose=False)\n",
        "        centered_image_1 = add_padding(cropped_image_1, verbose=False)\n",
        "        \n",
        "        cropped_image_2, cropped_image_size_2 = tight_crop_image(img_arr_2, verbose=False)\n",
        "        centered_image_2 = add_padding(cropped_image_2, verbose=False)\n",
        "        \n",
        "        cropped_image_3, cropped_image_size_3 = tight_crop_image(img_arr_3, verbose=False)\n",
        "        centered_image_3 = add_padding(cropped_image_3, verbose=False)\n",
        "            \n",
        "        return (centered_image_1, centered_image_2, centered_image_3), info #\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.triplets)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BwklBemVFAQe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import shutil\n",
        "import os\n",
        "import io\n",
        "\n",
        "class Tripletnet(nn.Module):\n",
        "    def __init__(self, embeddingnet):\n",
        "        super(Tripletnet, self).__init__()\n",
        "        self.embeddingnet = embeddingnet\n",
        "\n",
        "    def forward(self, x, y, z):\n",
        "        embedded_x = self.embeddingnet(x)\n",
        "        embedded_y = self.embeddingnet(y)\n",
        "        embedded_z = self.embeddingnet(z)\n",
        "        dist_a = F.pairwise_distance(embedded_x, embedded_y, 2)\n",
        "        dist_b = F.pairwise_distance(embedded_x, embedded_z, 2)\n",
        "        return dist_a, dist_b, embedded_x, embedded_y, embedded_z"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "08N3mTjiFAOJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# FontStyler의 convAE 코드 (layers.py)\n",
        "class Encoder_conv(nn.Module):\n",
        "    \n",
        "    def __init__(self, img_dim=1, conv_dim=16): # output dim은 128이 될 것\n",
        "        super(Encoder_conv, self).__init__()\n",
        "        self.conv1 = conv2d(img_dim, conv_dim, k_size=5, stride=2, pad=2, dilation=2, lrelu=False, bn=False)\n",
        "        self.conv2 = conv2d(conv_dim, conv_dim*2, k_size=5, stride=4, pad=2, dilation=2)\n",
        "        self.conv3 = conv2d(conv_dim*2, conv_dim*4, k_size=4, stride=4, pad=1, dilation=1)\n",
        "        self.conv4 = conv2d(conv_dim*4, conv_dim*8)\n",
        "        self.conv5 = conv2d(conv_dim*8, conv_dim*8)\n",
        "    \n",
        "    def forward(self, images):\n",
        "        # |images| = (batch, img, img)\n",
        "        # print(images.shape)\n",
        "        images = images.unsqueeze(dim=1)\n",
        "        # |images| = (batch, 1, 128, 128)\n",
        "        # print(images.shape)\n",
        "        e1 = self.conv1(images)\n",
        "        # |e1| = (batch, conv_dim, 64, 64)\n",
        "        # print(e1.shape)\n",
        "        e2 = self.conv2(e1)\n",
        "        # |e2| = (batch, conv_dim*2, 16, 16)\n",
        "        # print(e2.shape)\n",
        "        e3 = self.conv3(e2)\n",
        "        # |e3| = (batch, conv_dim*4, 4, 4)\n",
        "        # print(e3.shape)\n",
        "        e4 = self.conv4(e3)\n",
        "        # |e4| = (batch, conv_dim*8, 2, 2)\n",
        "        # print(e4.shape)\n",
        "        encoded_source = self.conv5(e4)\n",
        "        # |encoded_source| = (batch, conv_dim*8, 1, 1)\n",
        "        # print(encoded_source.shape)\n",
        "        \n",
        "        return encoded_source"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FBZlfX_3Gnk9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# function.py\n",
        "import torch.nn as nn\n",
        "\n",
        "def batch_norm(c_out, momentum=0.1):\n",
        "    return nn.BatchNorm2d(c_out, momentum=momentum)\n",
        "\n",
        "def conv2d(c_in, c_out, k_size=3, stride=2, pad=1, dilation=1, bn=True, lrelu=True, leak=0.2):\n",
        "    layers = []\n",
        "    if lrelu:\n",
        "        layers.append(nn.LeakyReLU(leak))\n",
        "    layers.append(nn.Conv2d(c_in, c_out, k_size, stride, pad))\n",
        "    if bn:\n",
        "        layers.append(nn.BatchNorm2d(c_out))\n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "def deconv2d(c_in, c_out, k_size=3, stride=1, pad=1, dilation=1, bn=True, dropout=False, p=0.5):\n",
        "    layers = []\n",
        "    layers.append(nn.LeakyReLU(0.2))\n",
        "    layers.append(nn.ConvTranspose2d(c_in, c_out, k_size, stride, pad))\n",
        "    if bn:\n",
        "        layers.append(nn.BatchNorm2d(c_out))\n",
        "    if dropout:\n",
        "        layers.append(nn.Dropout(p))\n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "def lrelu(leak=0.2):\n",
        "    return nn.LeakyReLU(leak)\n",
        "\n",
        "def dropout(p=0.2):\n",
        "    return nn.Dropout(p)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lH1b-ZoUFALM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "23ebf54c-c858-44ce-d3b1-6d608e35d9ac"
      },
      "source": [
        "checkpoint_pth = '/content/drive/My Drive/Colab Notebooks/FontStyler/src/data/runs/TripleNet/model_best.pth.tar'\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "best = torch.load(checkpoint_pth)\n",
        "\n",
        "# 학습된 TripletNet 로드\n",
        "model = Encoder_conv()\n",
        "tnet  = Tripletnet(model)\n",
        "tnet.load_state_dict(best['state_dict'])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zRFOlw6jFAI8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 484
        },
        "outputId": "d5b1ee04-21d8-4ab5-8f01-3fbe3ba5f068"
      },
      "source": [
        "tnet.cuda()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Tripletnet(\n",
              "  (embeddingnet): Encoder_conv(\n",
              "    (conv1): Sequential(\n",
              "      (0): Conv2d(1, 16, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
              "    )\n",
              "    (conv2): Sequential(\n",
              "      (0): LeakyReLU(negative_slope=0.2)\n",
              "      (1): Conv2d(16, 32, kernel_size=(5, 5), stride=(4, 4), padding=(2, 2))\n",
              "      (2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (conv3): Sequential(\n",
              "      (0): LeakyReLU(negative_slope=0.2)\n",
              "      (1): Conv2d(32, 64, kernel_size=(4, 4), stride=(4, 4), padding=(1, 1))\n",
              "      (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (conv4): Sequential(\n",
              "      (0): LeakyReLU(negative_slope=0.2)\n",
              "      (1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
              "      (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (conv5): Sequential(\n",
              "      (0): LeakyReLU(negative_slope=0.2)\n",
              "      (1): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
              "      (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4swmrWx4GzYU",
        "colab_type": "text"
      },
      "source": [
        "## K-means clustering\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hIqS6sHqOTF8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dset = KoreanFontDataset(PickledImageProvider('/content/drive/My Drive/Colab Notebooks/FontStyler/src/data/dataset/kor/latent.obj'), vector_size=10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sdk71ErnOXcV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345
        },
        "outputId": "f6cc7761-63ca-49b8-cfe4-923bdc29d03d"
      },
      "source": [
        "dset[0]"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "({'font_doc2vec': [2.2403063999999997,\n",
              "   -1.4756056999999998,\n",
              "   -0.593018,\n",
              "   -0.18605323,\n",
              "   -1.2381212,\n",
              "   -1.161201,\n",
              "   2.8255024,\n",
              "   0.10454782,\n",
              "   -0.16260550000000001,\n",
              "   1.2440913999999998],\n",
              "  'font_index': 0,\n",
              "  'word_index': 0},\n",
              " array([[1., 1., 1., ..., 1., 1., 1.],\n",
              "        [1., 1., 1., ..., 1., 1., 1.],\n",
              "        [1., 1., 1., ..., 1., 1., 1.],\n",
              "        ...,\n",
              "        [1., 1., 1., ..., 1., 1., 1.],\n",
              "        [1., 1., 1., ..., 1., 1., 1.],\n",
              "        [1., 1., 1., ..., 1., 1., 1.]]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y6EZiR1gG62X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pickled = PickledImageProvider('/content/drive/My Drive/Colab Notebooks/FontStyler/src/data/dataset/kor/latent.obj')\n",
        "triplet_loader = TripletImageLoader(pickled, '/content/drive/My Drive/Colab Notebooks/FontStyler/src/data/triplet_list_with_label.txt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o0W2tIQlFAFT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.cluster import KMeans\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "\n",
        "idxs = list(range(len(triplet_loader)))\n",
        "np.random.shuffle(idxs)\n",
        "batch_size = 32\n",
        "\n",
        "sampler = SubsetRandomSampler(idxs) # 전체 샘플링\n",
        "loader = torch.utils.data.DataLoader(\n",
        "        dset,\n",
        "        batch_size = batch_size,\n",
        "        sampler = None\n",
        "    )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3HZQ302MPFB_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 순차적으로 데이터를 호출하도록 수정\n",
        "tnet.eval()\n",
        "with torch.no_grad():\n",
        "    anchors = []\n",
        "    fonts  = []\n",
        "    letters = []\n",
        "    for idx, (info, img) in enumerate(loader): \n",
        "        if device == 'cuda':\n",
        "            img = img.cuda()\n",
        "        img = img.float()\n",
        "        dummy_0, dummy_1 = img, img\n",
        "\n",
        "        _, _, anchor, _, _ = tnet(img, dummy_0, dummy_1) # img2, img3에는 더미데이터\n",
        "        anchors.append(anchor) \n",
        "        fonts.append(info['font_index'])\n",
        "        letters.append(info['word_index'])\n",
        "        # if idx == 1000:\n",
        "        #     break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8eBV3uINFAAl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # 순차적으로 데이터를 호출하도록 수정\n",
        "# tnet.eval()\n",
        "# with torch.no_grad():\n",
        "#     anchors = []\n",
        "#     labels  = []\n",
        "#     indexes = []\n",
        "#     for idx, ((img1, img2, img3), info) in enumerate(loader): \n",
        "#         if device == 'cuda':\n",
        "#             img1, img2, img3 = img1.cuda(), img2.cuda(), img3.cuda()\n",
        "#         img1, img2, img3 = img1.float(), img2.float(), img3.float()\n",
        "\n",
        "#         _, _, anchor, _, _ = tnet(img1, img2, img3) \n",
        "#         anchors.append(anchor) \n",
        "#         labels.append(info['anchor_label'])\n",
        "#         indexes.append(info['anchor_index'])\n",
        "#         # if idx == 1000:\n",
        "#         #     break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mj4V-ibfE_-B",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "21f95932-0ff0-4e32-fa54-002199adf48b"
      },
      "source": [
        "len(anchors), len(fonts), len(letters)"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(7858, 7858, 7858)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iYM7DqD2E_7p",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "f88c3032-332f-493e-c484-a1ac8753ff6e"
      },
      "source": [
        "anchors[0].shape"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 128, 1, 1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nnn_8FCDE_2h",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "3ac1bfdb-db45-4425-9716-54c080267a3c"
      },
      "source": [
        "latent = np.zeros((1, 128, 1, 1))\n",
        "for i in range(len(anchors)):\n",
        "    latent = np.concatenate([latent, anchors[i].cpu()])\n",
        "latent = latent[1:]\n",
        "print('latent shape: {}'.format(latent.shape))\n",
        "\n",
        "tmp_font = np.zeros((1))\n",
        "for i in range(len(fonts)):\n",
        "    tmp_font = np.concatenate([tmp_font, fonts[i].cpu()])\n",
        "tmp_font = tmp_font[1:]\n",
        "print('fonts shape: {}'.format(tmp_font.shape))\n",
        "\n",
        "tmp_letter = np.zeros((1))\n",
        "for i in range(len(letters)):\n",
        "    tmp_letter = np.concatenate([tmp_letter, letters[i].cpu()])\n",
        "tmp_letter = tmp_letter[1:]\n",
        "print('letter shape: {}'.format(tmp_letter.shape))"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "latent shape: (251450, 128, 1, 1)\n",
            "fonts shape: (251450,)\n",
            "letter shape: (251450,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YscugcWeHT5N",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "b98e897b-3ed6-44a7-d0fa-2049b7a16f13"
      },
      "source": [
        "latent = latent.reshape(251450,128)\n",
        "latent.shape"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(251450, 128)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JcwR0jfgHU2B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# clustering\n",
        "model = KMeans(n_clusters=5)\n",
        "model.fit(latent)\n",
        "\n",
        "y_predict = model.fit_predict(latent)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ACDGL3fbHUz5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "a3b7c423-e023-4207-f7ab-0b8b1ceca153"
      },
      "source": [
        "print(y_predict[:100])"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[3 3 3 3 3 3 3 3 3 3 3 3 3 3 4 3 3 3 3 3 3 3 3 3 3 3 3 4 4 3 3 3 4 3 3 3 3\n",
            " 3 4 4 3 3 3 3 3 3 3 3 3 3 4 4 3 3 4 4 3 3 4 3 3 4 3 4 4 4 4 3 4 3 4 0 4 4\n",
            " 4 4 4 4 4 4 3 3 4 3 3 4 3 3 4 4 3 4 3 3 3 3 3 3 3 3]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-GLaHV0CHl7t",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "## \"폰트 idx\" : \"클러스터 평균 벡터\" 매핑\n",
        "\n",
        "### 평균 벡터 구하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g73lsLdiHUxW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cluster_vector = dict()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d1YqQU82HUu9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "8918895d-5bff-4c02-c8e0-8371dbff1a19"
      },
      "source": [
        "# cluster 0\n",
        "count = 0\n",
        "cluster = 0\n",
        "vector_size = 128\n",
        "vectors = np.zeros(vector_size,)\n",
        "\n",
        "for i, (idx, c) in enumerate(zip(tmp_index, y_predict)): \n",
        "    if c == cluster:\n",
        "        vectors += latent[int(idx)]\n",
        "        count += 1\n",
        "avg_vector_0 = vectors / count\n",
        "print('average vector for cluster {} with {} data'.format(cluster, count))"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "average vector for cluster 0 with 51661 data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h7t15TrSIZyr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "e5cfc9f2-ecc5-4128-cceb-df2860ed38ba"
      },
      "source": [
        "# cluster 1\n",
        "count = 0\n",
        "cluster = 1\n",
        "vector_size = 128\n",
        "vectors = np.zeros(vector_size,)\n",
        "\n",
        "for i, (idx, c) in enumerate(zip(tmp_index, y_predict)): \n",
        "    if c == cluster:\n",
        "        vectors += latent[int(idx)]\n",
        "        count += 1\n",
        "avg_vector_1 = vectors / count\n",
        "print('average vector for cluster {} with {} data'.format(cluster, count))"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "average vector for cluster 1 with 74310 data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Iy9BKFOHUsj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "8dfead7b-8ac0-480e-f37e-9327184738ab"
      },
      "source": [
        "# cluster 2\n",
        "count = 0\n",
        "cluster = 2\n",
        "vector_size = 128\n",
        "vectors = np.zeros(vector_size,)\n",
        "\n",
        "for i, (idx, c) in enumerate(zip(tmp_index, y_predict)): \n",
        "    if c == cluster:\n",
        "        vectors += latent[int(idx)]\n",
        "        count += 1\n",
        "avg_vector_2 = vectors / count\n",
        "print('average vector for cluster {} with {} data'.format(cluster, count))"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "average vector for cluster 2 with 27733 data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3n01OyWfHUp7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "18f20b08-6662-4eab-e89d-386f51ce89e7"
      },
      "source": [
        "# cluster 3\n",
        "count = 0\n",
        "cluster = 3\n",
        "vector_size = 128\n",
        "vectors = np.zeros(vector_size,)\n",
        "\n",
        "for i, (idx, c) in enumerate(zip(tmp_index, y_predict)): \n",
        "    if c == cluster:\n",
        "        vectors += latent[int(idx)]\n",
        "        count += 1\n",
        "avg_vector_3 = vectors / count\n",
        "print('average vector for cluster {} with {} data'.format(cluster, count))"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "average vector for cluster 3 with 39621 data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FprdeIifHUna",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "fea6c5c1-45af-41c0-988d-de84e7be932e"
      },
      "source": [
        "# cluster 4\n",
        "count = 0\n",
        "cluster = 4\n",
        "vector_size = 128\n",
        "vectors = np.zeros(vector_size,)\n",
        "\n",
        "for i, (idx, c) in enumerate(zip(tmp_index, y_predict)): \n",
        "    if c == cluster:\n",
        "        vectors += latent[int(idx)]\n",
        "        count += 1\n",
        "avg_vector_4 = vectors / count\n",
        "print('average vector for cluster {} with {} data'.format(cluster, count))"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "average vector for cluster 4 with 58125 data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z02AviCeHUkx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cluster_vector = {\n",
        "    0: avg_vector_0,\n",
        "    1: avg_vector_1,\n",
        "    2: avg_vector_2,\n",
        "    3: avg_vector_3,\n",
        "    4: avg_vector_4,\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sL_7Q6P2Inx0",
        "colab_type": "text"
      },
      "source": [
        "### 폰트 인덱스에 할당"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hG__J9tuYoh8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "44845225-8fa0-40b7-ffa5-af25d9d4658b"
      },
      "source": [
        "# 가장 높은 빈도 가진 클러스터 출력 (test)\n",
        "import collections\n",
        "cc = collections.Counter(y_predict[:2350])\n",
        "print(cc.most_common(1)[0][0]) # 3번 클러스터"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RXCYI3cOI1JI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "font_idx_with_cluster_vec = dict()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yjoNRV8aImOp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "33ab49ed-3664-48be-d249-ef40af317260"
      },
      "source": [
        "count_0 = 0\n",
        "count_1 = 0\n",
        "count_2 = 0\n",
        "count_3 = 0\n",
        "count_4 = 0\n",
        "\n",
        "vector_size = 128\n",
        "'''\n",
        "    같은 폰트라도 글자마다 다른 cluster로 분류될 수 있다.\n",
        "    2350자 중 가장 많은 클러스터로 분류된 쪽으로 평균 latent vector를 할당한다.\n",
        "'''\n",
        "\n",
        "total = len(dset)\n",
        "font_idx = 0\n",
        "\n",
        "for i in range(0, total, 2350):\n",
        "    counter = collections.Counter(y_predict[i : i+2350])\n",
        "    clust = counter.most_common(1)[0][0]\n",
        "    font_idx_with_cluster_vec[font_idx] = cluster_vector[clust]\n",
        "\n",
        "    print(\"font {}'s cluster: {}\".format(font_idx, clust))\n",
        "    font_idx += 1"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "font 0's cluster: 3\n",
            "font 1's cluster: 1\n",
            "font 2's cluster: 3\n",
            "font 3's cluster: 4\n",
            "font 4's cluster: 4\n",
            "font 5's cluster: 4\n",
            "font 6's cluster: 0\n",
            "font 7's cluster: 2\n",
            "font 8's cluster: 1\n",
            "font 9's cluster: 0\n",
            "font 10's cluster: 2\n",
            "font 11's cluster: 1\n",
            "font 12's cluster: 1\n",
            "font 13's cluster: 4\n",
            "font 14's cluster: 4\n",
            "font 15's cluster: 0\n",
            "font 16's cluster: 0\n",
            "font 17's cluster: 1\n",
            "font 18's cluster: 2\n",
            "font 19's cluster: 1\n",
            "font 20's cluster: 0\n",
            "font 21's cluster: 0\n",
            "font 22's cluster: 3\n",
            "font 23's cluster: 0\n",
            "font 24's cluster: 1\n",
            "font 25's cluster: 4\n",
            "font 26's cluster: 1\n",
            "font 27's cluster: 3\n",
            "font 28's cluster: 4\n",
            "font 29's cluster: 0\n",
            "font 30's cluster: 1\n",
            "font 31's cluster: 0\n",
            "font 32's cluster: 1\n",
            "font 33's cluster: 4\n",
            "font 34's cluster: 1\n",
            "font 35's cluster: 0\n",
            "font 36's cluster: 0\n",
            "font 37's cluster: 3\n",
            "font 38's cluster: 0\n",
            "font 39's cluster: 1\n",
            "font 40's cluster: 3\n",
            "font 41's cluster: 4\n",
            "font 42's cluster: 1\n",
            "font 43's cluster: 3\n",
            "font 44's cluster: 4\n",
            "font 45's cluster: 0\n",
            "font 46's cluster: 1\n",
            "font 47's cluster: 0\n",
            "font 48's cluster: 1\n",
            "font 49's cluster: 1\n",
            "font 50's cluster: 0\n",
            "font 51's cluster: 0\n",
            "font 52's cluster: 2\n",
            "font 53's cluster: 4\n",
            "font 54's cluster: 4\n",
            "font 55's cluster: 0\n",
            "font 56's cluster: 2\n",
            "font 57's cluster: 1\n",
            "font 58's cluster: 0\n",
            "font 59's cluster: 2\n",
            "font 60's cluster: 1\n",
            "font 61's cluster: 4\n",
            "font 62's cluster: 2\n",
            "font 63's cluster: 4\n",
            "font 64's cluster: 1\n",
            "font 65's cluster: 1\n",
            "font 66's cluster: 3\n",
            "font 67's cluster: 0\n",
            "font 68's cluster: 2\n",
            "font 69's cluster: 3\n",
            "font 70's cluster: 4\n",
            "font 71's cluster: 0\n",
            "font 72's cluster: 3\n",
            "font 73's cluster: 4\n",
            "font 74's cluster: 4\n",
            "font 75's cluster: 1\n",
            "font 76's cluster: 3\n",
            "font 77's cluster: 4\n",
            "font 78's cluster: 4\n",
            "font 79's cluster: 3\n",
            "font 80's cluster: 3\n",
            "font 81's cluster: 2\n",
            "font 82's cluster: 0\n",
            "font 83's cluster: 3\n",
            "font 84's cluster: 1\n",
            "font 85's cluster: 1\n",
            "font 86's cluster: 4\n",
            "font 87's cluster: 0\n",
            "font 88's cluster: 1\n",
            "font 89's cluster: 1\n",
            "font 90's cluster: 1\n",
            "font 91's cluster: 4\n",
            "font 92's cluster: 2\n",
            "font 93's cluster: 4\n",
            "font 94's cluster: 1\n",
            "font 95's cluster: 1\n",
            "font 96's cluster: 0\n",
            "font 97's cluster: 4\n",
            "font 98's cluster: 1\n",
            "font 99's cluster: 1\n",
            "font 100's cluster: 0\n",
            "font 101's cluster: 4\n",
            "font 102's cluster: 1\n",
            "font 103's cluster: 1\n",
            "font 104's cluster: 3\n",
            "font 105's cluster: 2\n",
            "font 106's cluster: 4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jt1aqueqIm_a",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "0e3c1a04-911d-4072-a54b-f62e103d4f22"
      },
      "source": [
        "len(font_idx_with_cluster_vec)"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "107"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9PWm3wSjbAKJ",
        "colab_type": "text"
      },
      "source": [
        "## Pickle로 저장"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1va1oFlYIm9Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle\n",
        "\n",
        "with open('/content/drive/My Drive/Colab Notebooks/FontStyler/src/data/font_idx_with_cluster_vec.p', 'wb') as file:\n",
        "    pickle.dump(font_idx_with_cluster_vec, file)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xJWv3OYjIm6x",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "0896df89-e880-4da2-f694-27525fab5771"
      },
      "source": [
        "# 확인\n",
        "with open('/content/drive/My Drive/Colab Notebooks/FontStyler/src/data/font_idx_with_cluster_vec.p', 'rb') as file:\n",
        "    vec_test = pickle.load(file)\n",
        "    print(len(vec_test))"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "107\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}