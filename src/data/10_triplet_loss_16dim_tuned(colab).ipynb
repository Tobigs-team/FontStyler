{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8KWtANEG13jA"
   },
   "source": [
    "# Triplet Loss\n",
    "해당 loss의 유효성을 확인하기 위한 실험으로, 코드가 지저분합니다^^*\n",
    "\n",
    "\n",
    "참고) https://github.com/andreasveit/triplet-network-pytorch\n",
    "\n",
    "### 설계\n",
    "- 두 방향의 triplet loss를 고려하고 있다.\n",
    "    1. 같은 폰트를 구분하는가\n",
    "    2. 같은 글자를 구분하는가 <br/>\n",
    "    \n",
    "    \n",
    "2번의 경우 데이터가 많이 부족하다. (같은 글자는 전체 데이터셋에서 107개 뿐) <br/>\n",
    "그러나 MNIST와 유사한 형태이기 때문에 데이터 수만 확보된다면 가능할 것 같긴 하다.\n",
    "\n",
    "\n",
    "1번의 경우 각 폰트마다 2350개의 글자가 있기 때문에 이 방향으로 구현할 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 55
    },
    "colab_type": "code",
    "id": "aRYNA8cd2nbr",
    "outputId": "c302674a-7def-4b4b-a676-d86a0a677b20"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m7mVktAw4Xn-"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '/content/drive/My Drive/Colab Notebooks/FontStyler/src/data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V92HZ5gu13jC"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "class Tripletnet(nn.Module):\n",
    "    def __init__(self, embeddingnet):\n",
    "        super(Tripletnet, self).__init__()\n",
    "        self.embeddingnet = embeddingnet\n",
    "\n",
    "    def forward(self, x, y, z):\n",
    "        embedded_x = self.embeddingnet(x)\n",
    "        embedded_y = self.embeddingnet(y)\n",
    "        embedded_z = self.embeddingnet(z)\n",
    "        dist_a = F.pairwise_distance(embedded_x, embedded_y, 2)\n",
    "        dist_b = F.pairwise_distance(embedded_x, embedded_z, 2)\n",
    "        return dist_a, dist_b, embedded_x, embedded_y, embedded_z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t_Eov3Lu13jY"
   },
   "source": [
    "## TrainLoader 커스텀\n",
    "MNIST DataLoader를 커스텀하여 사용하기로 한다. <br/>\n",
    "https://github.com/andreasveit/triplet-network-pytorch/blob/master/triplet_image_loader.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jIqBeDUC13jY"
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "import os.path\n",
    "\n",
    "import torch.utils.data\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "def default_image_loader(path):\n",
    "    return Image.open(path).convert('RGB')\n",
    "\n",
    "class TripletImageLoader(torch.utils.data.Dataset): \n",
    "    def __init__(self, pickled, triplets_file_name, base_path=None, filenames_filename=None, transform=None,\n",
    "                 loader=default_image_loader):\n",
    "        \"\"\" \n",
    "        filenames_filename: \n",
    "            A text file with each line containing the path to an image e.g.,\n",
    "            images/class1/sample.jpg\n",
    "                \n",
    "        triplets_file_name: \n",
    "            A text file with each line containing three integers, \n",
    "            where integer i refers to the i-th image in the filenames file. \n",
    "            For a line of intergers 'a b c', a triplet is defined such that image a is more \n",
    "            similar to image c than it is to image b, \n",
    "            e.g., 0 2017 42 \n",
    "        \"\"\"\n",
    "        self.dset = pickled.examples\n",
    "#         self.base_path = base_path  \n",
    "#         self.filenamelist = []\n",
    "#         for line in open(filenames_filename):\n",
    "#             self.filenamelist.append(line.rstrip('\\n'))\n",
    "        triplets = []\n",
    "        for line in open(triplets_file_name):\n",
    "            triplets.append((line.split()[0], line.split()[1], line.split()[2])) # anchor, far, close\n",
    "        self.triplets = triplets\n",
    "        self.transform = transform\n",
    "        self.loader = loader\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        path1, path2, path3 = self.triplets[index]\n",
    "        img1_tuple = self.dset[int(path1)]\n",
    "        img2_tuple = self.dset[int(path2)]\n",
    "        img3_tuple = self.dset[int(path3)]\n",
    "        \n",
    "        # byte만 사용할 예정\n",
    "        img1, byte_1 = img1_tuple[0], img1_tuple[1]\n",
    "        img2, byte_2 = img2_tuple[0], img2_tuple[1]\n",
    "        img3, byte_3 = img3_tuple[0], img3_tuple[1]\n",
    "        \n",
    "        # bytes 타입을 numpy array로 변경 후 normalize\n",
    "        img_arr_1 = np.array(Image.open(io.BytesIO(byte_1)))\n",
    "        img_arr_1 = normalize_image(img_arr_1)\n",
    "        \n",
    "        img_arr_2 = np.array(Image.open(io.BytesIO(byte_2)))\n",
    "        img_arr_2 = normalize_image(img_arr_2)\n",
    "        \n",
    "        img_arr_3 = np.array(Image.open(io.BytesIO(byte_3)))\n",
    "        img_arr_3 = normalize_image(img_arr_3)\n",
    "\n",
    "        cropped_image_1, cropped_image_size_1 = tight_crop_image(img_arr_1, verbose=False)\n",
    "        centered_image_1 = add_padding(cropped_image_1, verbose=False)\n",
    "        \n",
    "        cropped_image_2, cropped_image_size_2 = tight_crop_image(img_arr_2, verbose=False)\n",
    "        centered_image_2 = add_padding(cropped_image_2, verbose=False)\n",
    "        \n",
    "        cropped_image_3, cropped_image_size_3 = tight_crop_image(img_arr_3, verbose=False)\n",
    "        centered_image_3 = add_padding(cropped_image_3, verbose=False)\n",
    "            \n",
    "        return centered_image_1, centered_image_2, centered_image_3\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.triplets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ux09eKbo13ja"
   },
   "outputs": [],
   "source": [
    "from common.utils import pad_seq, bytes_to_file, \\\n",
    "    read_split_image, shift_and_resize_image, normalize_image, \\\n",
    "    tight_crop_image, add_padding\n",
    "\n",
    "from common.dataset import PickledImageProvider\n",
    "from common.dataset import KoreanFontDataset\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "5VN4FLGV13kH",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "93d19228-f4b5-44d3-bcf2-dad57df27d17"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 10000 examples\n",
      "processed 20000 examples\n",
      "processed 30000 examples\n",
      "processed 40000 examples\n",
      "processed 50000 examples\n",
      "processed 60000 examples\n",
      "processed 70000 examples\n",
      "processed 80000 examples\n",
      "processed 90000 examples\n",
      "processed 100000 examples\n",
      "processed 110000 examples\n",
      "processed 120000 examples\n",
      "processed 130000 examples\n",
      "processed 140000 examples\n",
      "processed 150000 examples\n",
      "processed 160000 examples\n",
      "processed 170000 examples\n",
      "processed 180000 examples\n",
      "processed 190000 examples\n",
      "processed 200000 examples\n",
      "processed 210000 examples\n",
      "processed 220000 examples\n",
      "processed 230000 examples\n",
      "processed 240000 examples\n",
      "processed 250000 examples\n",
      "processed 260000 examples\n",
      "processed 270000 examples\n",
      "processed 280000 examples\n",
      "processed 290000 examples\n",
      "processed 300000 examples\n",
      "processed 310000 examples\n",
      "processed 320000 examples\n",
      "processed 330000 examples\n",
      "processed 340000 examples\n",
      "processed 350000 examples\n",
      "processed 360000 examples\n",
      "processed 370000 examples\n",
      "processed 380000 examples\n",
      "processed 390000 examples\n",
      "processed 400000 examples\n",
      "processed 410000 examples\n",
      "processed 420000 examples\n",
      "processed 430000 examples\n",
      "processed 440000 examples\n",
      "processed 450000 examples\n",
      "processed 460000 examples\n",
      "processed 470000 examples\n",
      "processed 480000 examples\n",
      "processed 490000 examples\n",
      "processed 500000 examples\n",
      "processed 510000 examples\n",
      "processed 520000 examples\n",
      "processed 530000 examples\n",
      "processed 540000 examples\n",
      "processed 550000 examples\n",
      "processed 560000 examples\n",
      "processed 570000 examples\n",
      "processed 580000 examples\n",
      "processed 590000 examples\n",
      "processed 600000 examples\n",
      "processed 610000 examples\n",
      "processed 620000 examples\n",
      "processed 630000 examples\n",
      "processed 640000 examples\n",
      "processed 650000 examples\n",
      "processed 660000 examples\n",
      "processed 670000 examples\n",
      "processed 680000 examples\n",
      "processed 690000 examples\n",
      "processed 700000 examples\n",
      "processed 710000 examples\n",
      "processed 720000 examples\n",
      "processed 730000 examples\n",
      "processed 740000 examples\n",
      "processed 750000 examples\n",
      "processed 760000 examples\n",
      "processed 770000 examples\n",
      "processed 780000 examples\n",
      "processed 790000 examples\n",
      "processed 800000 examples\n",
      "processed 810000 examples\n",
      "processed 820000 examples\n",
      "processed 830000 examples\n",
      "processed 840000 examples\n",
      "processed 850000 examples\n",
      "processed 860000 examples\n",
      "processed 870000 examples\n",
      "processed 880000 examples\n",
      "processed 890000 examples\n",
      "processed 900000 examples\n",
      "processed 910000 examples\n",
      "processed 920000 examples\n",
      "processed 930000 examples\n",
      "processed 940000 examples\n",
      "processed 950000 examples\n",
      "processed 960000 examples\n",
      "processed 970000 examples\n",
      "processed 980000 examples\n",
      "processed 990000 examples\n",
      "processed 1000000 examples\n",
      "processed 1010000 examples\n",
      "processed 1020000 examples\n",
      "processed 1030000 examples\n",
      "processed 1040000 examples\n",
      "processed 1050000 examples\n",
      "processed 1060000 examples\n",
      "processed 1070000 examples\n",
      "processed 1080000 examples\n",
      "processed 1090000 examples\n",
      "processed 1100000 examples\n",
      "processed 1110000 examples\n",
      "processed 1120000 examples\n",
      "processed 1130000 examples\n",
      "processed 1140000 examples\n",
      "processed 1150000 examples\n",
      "processed 1160000 examples\n",
      "processed 1170000 examples\n",
      "processed 1180000 examples\n",
      "processed 1190000 examples\n",
      "processed 1200000 examples\n",
      "processed 1210000 examples\n",
      "processed 1220000 examples\n",
      "processed 1230000 examples\n",
      "processed 1240000 examples\n",
      "processed 1250000 examples\n",
      "unpickled total 1257250 examples\n",
      "saved total 251450 examples only for byte\n"
     ]
    }
   ],
   "source": [
    "pickled = PickledImageProvider('/content/drive/My Drive/Colab Notebooks/FontStyler/src/data/dataset/kor/latent.obj')\n",
    "triplet_loader_all = TripletImageLoader(pickled, '/content/drive/My Drive/Colab Notebooks/FontStyler/src/data/triplet_list_all.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "xnJnTUzg13kM",
    "outputId": "450be4e4-1689-4e46-8a30-a15b7542b3d7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "251450"
      ]
     },
     "execution_count": 35,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(triplet_loader_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Km6brkH313kO"
   },
   "source": [
    "---\n",
    "# Triplet loss with Convolutional AutoEncoder\n",
    "https://github.com/andreasveit/triplet-network-pytorch/blob/master/train.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XqDL2n9t13kO"
   },
   "source": [
    "### Layers & Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2_Bo70jY13kO"
   },
   "outputs": [],
   "source": [
    "# FontStyler의 convAE 코드 (layers.py)\n",
    "class Encoder_conv(nn.Module):\n",
    "    \n",
    "    def __init__(self, img_dim=1, conv_dim=16): # output dim은 128이 될 것\n",
    "        super(Encoder_conv, self).__init__()\n",
    "        self.conv1 = conv2d(img_dim, conv_dim, k_size=5, stride=2, pad=2, dilation=2, lrelu=False, bn=False)\n",
    "        self.conv2 = conv2d(conv_dim, conv_dim*2, k_size=5, stride=4, pad=2, dilation=2)\n",
    "        self.conv3 = conv2d(conv_dim*2, conv_dim*4, k_size=4, stride=4, pad=1, dilation=1)\n",
    "        self.conv4 = conv2d(conv_dim*4, conv_dim*8)\n",
    "        self.conv5 = conv2d(conv_dim*8, conv_dim*8)\n",
    "    \n",
    "    def forward(self, images):\n",
    "        # |images| = (batch, img, img)\n",
    "        # print(images.shape)\n",
    "        images = images.unsqueeze(dim=1)\n",
    "        # |images| = (batch, 1, 128, 128)\n",
    "        # print(images.shape)\n",
    "        e1 = self.conv1(images)\n",
    "        # |e1| = (batch, conv_dim, 64, 64)\n",
    "        # print(e1.shape)\n",
    "        e2 = self.conv2(e1)\n",
    "        # |e2| = (batch, conv_dim*2, 16, 16)\n",
    "        # print(e2.shape)\n",
    "        e3 = self.conv3(e2)\n",
    "        # |e3| = (batch, conv_dim*4, 4, 4)\n",
    "        # print(e3.shape)\n",
    "        e4 = self.conv4(e3)\n",
    "        # |e4| = (batch, conv_dim*8, 2, 2)\n",
    "        # print(e4.shape)\n",
    "        encoded_source = self.conv5(e4)\n",
    "        # |encoded_source| = (batch, conv_dim*8, 1, 1)\n",
    "        # print(encoded_source.shape)\n",
    "        \n",
    "        return encoded_source\n",
    "    \n",
    "    \n",
    "# class Decoder_conv(nn.Module):\n",
    "    \n",
    "#     def __init__(self, img_dim=1, embedded_dim=640, conv_dim=64):\n",
    "#         super(Decoder_conv, self).__init__()\n",
    "#         self.deconv1 = deconv2d(conv_dim*8, conv_dim*8, k_size=4, dilation=2, stride=2)\n",
    "#         self.deconv2 = deconv2d(conv_dim*8, conv_dim*4, k_size=4, dilation=2, stride=2)\n",
    "#         self.deconv3 = deconv2d(conv_dim*4, conv_dim*2, k_size=6, dilation=2, stride=4)\n",
    "#         self.deconv4 = deconv2d(conv_dim*2, conv_dim*1, k_size=6, dilation=2, stride=4)\n",
    "#         self.deconv5 = deconv2d(conv_dim*1, img_dim, k_size=4, dilation=2, stride=2, bn=False)\n",
    "    \n",
    "#     def forward(self, embedded):\n",
    "#         # |embedded| = (batch, conv_dim*8, 1, 1)\n",
    "#         d1 = self.deconv1(embedded)\n",
    "#         # |d1| = (batch, conv_dim*8, 2, 2)\n",
    "#         # print( 1.shape)\n",
    "#         d2 = self.deconv2(d1)\n",
    "#         # |d2| = (batch, conv_dim*4, 4, 4)\n",
    "#         # print(d2.s hape)\n",
    "#         d3 = self.deconv3(d2)\n",
    "#         # |d3| = (batch, conv_dim*2, 16, 16)\n",
    "#         # print(d3. shape)\n",
    "#         d4 = self.deconv4(d3)\n",
    "#         # |d4| = (batch, conv_dim*1, 64, 64)\n",
    "#         # print(d4 .shape)\n",
    "#         d5 = self.deconv5(d4)        \n",
    "#         # |d5| = (batch, 1, 128, 128)\n",
    "#         # print(d5. shape)\n",
    "#         fake_target = d5\n",
    "#         # |fake_target| = (batch_size, 1, img, img)\n",
    "#         fake_target = fake_target.squeeze(dim=1)\n",
    "#         # |fake_target| = (batch_size, img, img)\n",
    "        \n",
    "#         return fake_target\n",
    "    \n",
    "# class AE_conv(nn.Module):\n",
    "#     def __init__(self, img_dim=1, conv_dim=64):\n",
    "#         super(AE_conv, self).__init__()\n",
    "#         self.Encoder = Encoder_conv(img_dim=img_dim, conv_dim=conv_dim)\n",
    "#         self.Decoder = Decoder_conv(img_dim=img_dim, embedded_dim=conv_dim*8, conv_dim=conv_dim)\n",
    "    \n",
    "#     def forward(self, x):\n",
    "#         z = self.Encoder(x)\n",
    "#         x_hat = self.Decoder(z)\n",
    "        \n",
    "#         return x_hat, z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7uSgCU8113kQ"
   },
   "outputs": [],
   "source": [
    "# function.py\n",
    "import torch.nn as nn\n",
    "\n",
    "def batch_norm(c_out, momentum=0.1):\n",
    "    return nn.BatchNorm2d(c_out, momentum=momentum)\n",
    "\n",
    "def conv2d(c_in, c_out, k_size=3, stride=2, pad=1, dilation=1, bn=True, lrelu=True, leak=0.2):\n",
    "    layers = []\n",
    "    if lrelu:\n",
    "        layers.append(nn.LeakyReLU(leak))\n",
    "    layers.append(nn.Conv2d(c_in, c_out, k_size, stride, pad))\n",
    "    if bn:\n",
    "        layers.append(nn.BatchNorm2d(c_out))\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "def deconv2d(c_in, c_out, k_size=3, stride=1, pad=1, dilation=1, bn=True, dropout=False, p=0.5):\n",
    "    layers = []\n",
    "    layers.append(nn.LeakyReLU(0.2))\n",
    "    layers.append(nn.ConvTranspose2d(c_in, c_out, k_size, stride, pad))\n",
    "    if bn:\n",
    "        layers.append(nn.BatchNorm2d(c_out))\n",
    "    if dropout:\n",
    "        layers.append(nn.Dropout(p))\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "def lrelu(leak=0.2):\n",
    "    return nn.LeakyReLU(leak)\n",
    "\n",
    "def dropout(p=0.2):\n",
    "    return nn.Dropout(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WNXLyEjI13kR"
   },
   "source": [
    "### Train Process\n",
    "우선 latent.obj로 만든 triplet 데이터셋을 split한다. <br/>\n",
    "train / val / test = 202100 / 37600 / 11750"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "Dx3pek3-13kS",
    "outputId": "03d4caad-9c78-442a-d62b-553cb43644f8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "251450"
      ]
     },
     "execution_count": 38,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(triplet_loader_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h94ZIeTW13kU"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data.sampler import SubsetRandomSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bkORHTif13kX"
   },
   "outputs": [],
   "source": [
    "num_train = 202100\n",
    "num_val = 37600\n",
    "num_test = 11750\n",
    "\n",
    "idxs = list(range(len(triplet_loader_all)))\n",
    "np.random.shuffle(idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tFtZuL9V13kY"
   },
   "outputs": [],
   "source": [
    "train_idxs, val_idxs, test_idxs = \\\n",
    "    idxs[:num_train], idxs[num_train: num_train+num_val], idxs[num_train+num_val: ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "hVt6UiFo13ka",
    "outputId": "01994d2e-b0bb-4ee8-8adf-6578abe0bbb8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(202100, 37600, 11750)"
      ]
     },
     "execution_count": 42,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_idxs), len(val_idxs), len(test_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j6_bld-213kb"
   },
   "outputs": [],
   "source": [
    "train_sampler = SubsetRandomSampler(train_idxs)\n",
    "valid_sampler = SubsetRandomSampler(val_idxs)\n",
    "test_sampler = SubsetRandomSampler(test_idxs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W_6FdrhN13ke"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6yIJcfWe13kf"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### default learning rate가 너무 큰 것 같아 조절을 해 보기로 했다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "T6afwWF313kg"
   },
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "batch_size = 32 # 8\n",
    "validation_split = .15\n",
    "test_split = .05\n",
    "shuffle_dataset = True\n",
    "random_seed = 42\n",
    "lr = 0.005 # 0.01\n",
    "momentum = 0.5\n",
    "\n",
    "best_acc = 0\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "JIKDBQtj8JbJ",
    "outputId": "901f35f0-6b9b-4bbc-d189-cb13f4f484a2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 47,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qJq46p5M13kh"
   },
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "        triplet_loader_all,\n",
    "        batch_size = batch_size,\n",
    "        sampler = train_sampler\n",
    "    )\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "        triplet_loader_all,\n",
    "        batch_size = batch_size,\n",
    "        sampler = valid_sampler\n",
    "    )\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "        triplet_loader_all,\n",
    "        batch_size = batch_size,\n",
    "        sampler = test_sampler\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "op4lFGRy13ki"
   },
   "outputs": [],
   "source": [
    "model = Encoder_conv()\n",
    "tnet  = Tripletnet(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s-dIJamk13kk"
   },
   "outputs": [],
   "source": [
    "criterion = torch.nn.MarginRankingLoss(margin = 0.2)\n",
    "optimizer = optim.SGD(tnet.parameters(), lr=lr, momentum=momentum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "moS5-MYz13km"
   },
   "outputs": [],
   "source": [
    "def train(train_loader, tnet, criterion, optimizer, epoch):\n",
    "    losses = AverageMeter()\n",
    "    accs = AverageMeter()\n",
    "    emb_norms = AverageMeter()\n",
    "\n",
    "    # switch to train mode\n",
    "    tnet.train()\n",
    "    tnet = tnet.float()\n",
    "\n",
    "    for batch_idx, (data1, data2, data3) in enumerate(train_loader):\n",
    "        if device == 'cuda': #if args.cuda:                                     ## todo: args 말고 일반 cuda 변수로 바꾸기\n",
    "            data1, data2, data3 = data1.cuda(), data2.cuda(), data3.cuda()\n",
    "        data1, data2, data3 = data1.float(), data2.float(), data3.float()\n",
    "        # compute output\n",
    "        dista, distb, embedded_x, embedded_y, embedded_z = tnet(data1, data2, data3)\n",
    "        # 1 means, dista should be larger than distb\n",
    "        target = torch.FloatTensor(dista.size()).fill_(1)\n",
    "        if device == 'cuda': #if args.cuda:                                     ## todo: args 말고 일반 cuda 변수로 바꾸기\n",
    "            target = target.cuda()\n",
    "        \n",
    "        loss_triplet = criterion(dista, distb, target)\n",
    "        loss_embedd = embedded_x.norm(2) + embedded_y.norm(2) + embedded_z.norm(2)\n",
    "        loss = loss_triplet + 0.001 * loss_embedd\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        acc = accuracy(dista, distb)\n",
    "        losses.update(loss_triplet.item(), data1.size(0)) # .data[0]\n",
    "        accs.update(acc, data1.size(0))\n",
    "        emb_norms.update(loss_embedd.item()/3, data1.size(0)) # .data[0]\n",
    "\n",
    "        # compute gradient and do optimizer step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % 20 == 0:\n",
    "            print('Train Epoch: {} [{}/{}]\\t'\n",
    "                  'Loss: {:.4f} ({:.4f}) \\t'\n",
    "                  'Acc: {:.2f}% ({:.2f}%) \\t'\n",
    "                  'Emb_Norm: {:.2f} ({:.2f})'.format(\n",
    "                epoch, batch_idx * len(data1), len(train_loader.dataset),\n",
    "                losses.val, losses.avg, \n",
    "                100. * accs.val, 100. * accs.avg, emb_norms.val, emb_norms.avg))\n",
    "    # log avg values to somewhere\n",
    "    # visdom은 GCP에서 하자!!!!!!!!!!!\n",
    "    # plotter.plot('acc', 'train', epoch, accs.avg)\n",
    "    # plotter.plot('loss', 'train', epoch, losses.avg)\n",
    "    # plotter.plot('emb_norms', 'train', epoch, emb_norms.avg)\n",
    "    return losses.avg\n",
    "\n",
    "def test(test_loader, tnet, criterion, epoch):\n",
    "    losses = AverageMeter()\n",
    "    accs = AverageMeter()\n",
    "\n",
    "    # switch to evaluation mode\n",
    "    tnet.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data1, data2, data3) in enumerate(test_loader):\n",
    "            if device == 'cuda': # if args.cuda:                                    ## todo: args 말고 일반 cuda 변수로 바꾸기\n",
    "                data1, data2, data3 = data1.cuda(), data2.cuda(), data3.cuda()\n",
    "            data1, data2, data3 = data1.float(), data2.float(), data3.float()\n",
    "\n",
    "            # compute output\n",
    "            dista, distb, _, _, _ = tnet(data1, data2, data3)\n",
    "            target = torch.FloatTensor(dista.size()).fill_(1)\n",
    "            if device == 'cuda': #if args.cuda:                     ## todo: args 말고 일반 cuda 변수로 바꾸기\n",
    "                target = target.cuda()\n",
    "            test_loss =  criterion(dista, distb, target).item() #.data[0]\n",
    "\n",
    "            # measure accuracy and record loss\n",
    "            acc = accuracy(dista, distb)\n",
    "            accs.update(acc, data1.size(0))\n",
    "            losses.update(test_loss, data1.size(0))      \n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {:.2f}%\\n'.format(\n",
    "        losses.avg, 100. * accs.avg))\n",
    "    # plotter.plot('acc', 'test', epoch, accs.avg)\n",
    "    # plotter.plot('loss', 'test', epoch, losses.avg)\n",
    "    return accs.avg, losses.avg\n",
    "\n",
    "def save_checkpoint(state, is_best, filename='checkpoint.pth.tar'):\n",
    "    \"\"\"Saves checkpoint to disk\"\"\"\n",
    "    directory = \"/content/drive/My Drive/Colab Notebooks/FontStyler/src/data/runs/%s/\"%('TripleNet')\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    filename = directory + filename\n",
    "    torch.save(state, filename)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filename, '/content/drive/My Drive/Colab Notebooks/FontStyler/src/data/runs/%s/'%('TripleNet') + 'model_best.pth.tar')\n",
    "\n",
    "# class VisdomLinePlotter(object):\n",
    "#     \"\"\"Plots to Visdom\"\"\"\n",
    "#     def __init__(self, env_name='main'):\n",
    "#         self.viz = Visdom()\n",
    "#         self.env = env_name\n",
    "#         self.plots = {}\n",
    "#     def plot(self, var_name, split_name, x, y):\n",
    "#         if var_name not in self.plots:\n",
    "#             self.plots[var_name] = self.viz.line(X=np.array([x,x]), Y=np.array([y,y]), env=self.env, opts=dict(\n",
    "#                 legend=[split_name],\n",
    "#                 title=var_name,\n",
    "#                 xlabel='Epochs',\n",
    "#                 ylabel=var_name\n",
    "#             ))\n",
    "#         else:\n",
    "#             self.viz.updateTrace(X=np.array([x]), Y=np.array([y]), env=self.env, win=self.plots[var_name], name=split_name)\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "def accuracy(dista, distb):\n",
    "    margin = 0\n",
    "    pred = (dista - distb - margin).cpu().data\n",
    "    return (pred > 0).sum()*1.0/dista.size()[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "45W7oioc8744"
   },
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "# global plotter \n",
    "# plotter = VisdomLinePlotter(env_name='Triplet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q8MR0uJG13kn"
   },
   "outputs": [],
   "source": [
    "# train process\n",
    "if device == 'cuda':\n",
    "    tnet.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "CqXsdxXx8cfT",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "50c441f4-4796-420a-de73-e8fc19515d62"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============== TRAIN epoch 1 ===============\n",
      "Train Epoch: 1 [0/251450]\tLoss: 0.8623 (0.8623) \tAcc: 43.75% (43.75%) \tEmb_Norm: 63.99 (63.99)\n",
      "Train Epoch: 1 [640/251450]\tLoss: 0.5367 (0.8441) \tAcc: 62.50% (56.10%) \tEmb_Norm: 63.90 (63.95)\n",
      "Train Epoch: 1 [1280/251450]\tLoss: 0.3544 (0.7406) \tAcc: 75.00% (62.96%) \tEmb_Norm: 63.83 (63.91)\n",
      "Train Epoch: 1 [1920/251450]\tLoss: 0.5043 (0.7186) \tAcc: 65.62% (64.14%) \tEmb_Norm: 63.75 (63.87)\n",
      "Train Epoch: 1 [2560/251450]\tLoss: 0.2176 (0.6675) \tAcc: 75.00% (66.82%) \tEmb_Norm: 63.68 (63.83)\n",
      "Train Epoch: 1 [3200/251450]\tLoss: 0.1874 (0.6104) \tAcc: 87.50% (69.49%) \tEmb_Norm: 63.63 (63.79)\n",
      "Train Epoch: 1 [3840/251450]\tLoss: 0.3680 (0.5808) \tAcc: 81.25% (70.92%) \tEmb_Norm: 63.57 (63.76)\n",
      "Train Epoch: 1 [4480/251450]\tLoss: 0.3729 (0.5583) \tAcc: 87.50% (71.92%) \tEmb_Norm: 63.52 (63.73)\n",
      "Train Epoch: 1 [5120/251450]\tLoss: 0.7421 (0.5340) \tAcc: 78.12% (73.19%) \tEmb_Norm: 63.46 (63.70)\n",
      "Train Epoch: 1 [5760/251450]\tLoss: 0.2797 (0.5137) \tAcc: 84.38% (73.90%) \tEmb_Norm: 63.41 (63.67)\n",
      "Train Epoch: 1 [6400/251450]\tLoss: 0.1223 (0.4918) \tAcc: 90.62% (74.74%) \tEmb_Norm: 63.37 (63.64)\n",
      "Train Epoch: 1 [7040/251450]\tLoss: 0.2055 (0.4704) \tAcc: 84.38% (75.41%) \tEmb_Norm: 63.33 (63.62)\n",
      "Train Epoch: 1 [7680/251450]\tLoss: 0.2033 (0.4574) \tAcc: 81.25% (76.02%) \tEmb_Norm: 63.28 (63.59)\n",
      "Train Epoch: 1 [8320/251450]\tLoss: 0.0835 (0.4471) \tAcc: 90.62% (76.58%) \tEmb_Norm: 63.23 (63.56)\n",
      "Train Epoch: 1 [8960/251450]\tLoss: 0.2607 (0.4373) \tAcc: 90.62% (77.14%) \tEmb_Norm: 63.19 (63.54)\n",
      "Train Epoch: 1 [9600/251450]\tLoss: 0.1727 (0.4243) \tAcc: 78.12% (77.74%) \tEmb_Norm: 63.15 (63.51)\n",
      "Train Epoch: 1 [10240/251450]\tLoss: 0.1460 (0.4142) \tAcc: 90.62% (78.16%) \tEmb_Norm: 63.10 (63.49)\n",
      "Train Epoch: 1 [10880/251450]\tLoss: 0.3271 (0.4048) \tAcc: 81.25% (78.56%) \tEmb_Norm: 63.06 (63.47)\n",
      "Train Epoch: 1 [11520/251450]\tLoss: 0.0730 (0.3953) \tAcc: 90.62% (78.92%) \tEmb_Norm: 63.02 (63.44)\n",
      "Train Epoch: 1 [12160/251450]\tLoss: 0.1133 (0.3870) \tAcc: 90.62% (79.25%) \tEmb_Norm: 62.98 (63.42)\n",
      "Train Epoch: 1 [12800/251450]\tLoss: 0.0139 (0.3763) \tAcc: 96.88% (79.68%) \tEmb_Norm: 62.95 (63.40)\n",
      "Train Epoch: 1 [13440/251450]\tLoss: 0.0715 (0.3659) \tAcc: 87.50% (80.03%) \tEmb_Norm: 62.92 (63.37)\n",
      "Train Epoch: 1 [14080/251450]\tLoss: 0.2391 (0.3591) \tAcc: 87.50% (80.34%) \tEmb_Norm: 62.88 (63.35)\n",
      "Train Epoch: 1 [14720/251450]\tLoss: 0.3224 (0.3531) \tAcc: 81.25% (80.60%) \tEmb_Norm: 62.84 (63.33)\n",
      "Train Epoch: 1 [15360/251450]\tLoss: 0.1827 (0.3444) \tAcc: 87.50% (80.90%) \tEmb_Norm: 62.81 (63.31)\n",
      "Train Epoch: 1 [16000/251450]\tLoss: 0.2610 (0.3385) \tAcc: 75.00% (81.13%) \tEmb_Norm: 62.77 (63.29)\n",
      "Train Epoch: 1 [16640/251450]\tLoss: 0.0690 (0.3326) \tAcc: 90.62% (81.27%) \tEmb_Norm: 62.74 (63.27)\n",
      "Train Epoch: 1 [17280/251450]\tLoss: 0.0607 (0.3262) \tAcc: 90.62% (81.50%) \tEmb_Norm: 62.71 (63.25)\n",
      "Train Epoch: 1 [17920/251450]\tLoss: 0.0988 (0.3185) \tAcc: 90.62% (81.87%) \tEmb_Norm: 62.68 (63.23)\n",
      "Train Epoch: 1 [18560/251450]\tLoss: 0.2512 (0.3137) \tAcc: 84.38% (82.09%) \tEmb_Norm: 62.64 (63.21)\n",
      "Train Epoch: 1 [19200/251450]\tLoss: 0.1526 (0.3097) \tAcc: 84.38% (82.19%) \tEmb_Norm: 62.61 (63.19)\n",
      "Train Epoch: 1 [19840/251450]\tLoss: 0.0035 (0.3052) \tAcc: 100.00% (82.38%) \tEmb_Norm: 62.57 (63.17)\n",
      "Train Epoch: 1 [20480/251450]\tLoss: 0.1345 (0.3008) \tAcc: 84.38% (82.57%) \tEmb_Norm: 62.54 (63.15)\n",
      "Train Epoch: 1 [21120/251450]\tLoss: 0.2024 (0.2959) \tAcc: 87.50% (82.75%) \tEmb_Norm: 62.51 (63.13)\n",
      "Train Epoch: 1 [21760/251450]\tLoss: 0.1918 (0.2905) \tAcc: 81.25% (82.95%) \tEmb_Norm: 62.48 (63.11)\n",
      "Train Epoch: 1 [22400/251450]\tLoss: 0.2300 (0.2859) \tAcc: 84.38% (83.14%) \tEmb_Norm: 62.45 (63.09)\n",
      "Train Epoch: 1 [23040/251450]\tLoss: 0.0680 (0.2822) \tAcc: 93.75% (83.31%) \tEmb_Norm: 62.42 (63.08)\n",
      "Train Epoch: 1 [23680/251450]\tLoss: 0.0232 (0.2780) \tAcc: 96.88% (83.49%) \tEmb_Norm: 62.39 (63.06)\n",
      "Train Epoch: 1 [24320/251450]\tLoss: 0.1329 (0.2756) \tAcc: 90.62% (83.61%) \tEmb_Norm: 62.35 (63.04)\n",
      "Train Epoch: 1 [24960/251450]\tLoss: 0.2380 (0.2735) \tAcc: 84.38% (83.68%) \tEmb_Norm: 62.32 (63.02)\n",
      "Train Epoch: 1 [25600/251450]\tLoss: 0.2031 (0.2703) \tAcc: 87.50% (83.82%) \tEmb_Norm: 62.29 (63.00)\n",
      "Train Epoch: 1 [26240/251450]\tLoss: 0.0510 (0.2670) \tAcc: 96.88% (83.94%) \tEmb_Norm: 62.26 (62.99)\n",
      "Train Epoch: 1 [26880/251450]\tLoss: 0.0845 (0.2641) \tAcc: 90.62% (84.05%) \tEmb_Norm: 62.22 (62.97)\n",
      "Train Epoch: 1 [27520/251450]\tLoss: 0.4056 (0.2613) \tAcc: 75.00% (84.21%) \tEmb_Norm: 62.19 (62.95)\n",
      "Train Epoch: 1 [28160/251450]\tLoss: 0.5412 (0.2590) \tAcc: 84.38% (84.36%) \tEmb_Norm: 62.16 (62.93)\n",
      "Train Epoch: 1 [28800/251450]\tLoss: 0.0605 (0.2557) \tAcc: 93.75% (84.53%) \tEmb_Norm: 62.13 (62.92)\n",
      "Train Epoch: 1 [29440/251450]\tLoss: 0.1057 (0.2535) \tAcc: 90.62% (84.59%) \tEmb_Norm: 62.10 (62.90)\n",
      "Train Epoch: 1 [30080/251450]\tLoss: 0.1214 (0.2514) \tAcc: 93.75% (84.71%) \tEmb_Norm: 62.07 (62.88)\n",
      "Train Epoch: 1 [30720/251450]\tLoss: 0.0957 (0.2493) \tAcc: 87.50% (84.83%) \tEmb_Norm: 62.03 (62.86)\n",
      "Train Epoch: 1 [31360/251450]\tLoss: 0.2745 (0.2473) \tAcc: 84.38% (84.93%) \tEmb_Norm: 62.00 (62.85)\n",
      "Train Epoch: 1 [32000/251450]\tLoss: 0.0833 (0.2451) \tAcc: 96.88% (85.03%) \tEmb_Norm: 61.97 (62.83)\n",
      "Train Epoch: 1 [32640/251450]\tLoss: 0.0579 (0.2424) \tAcc: 93.75% (85.13%) \tEmb_Norm: 61.94 (62.81)\n",
      "Train Epoch: 1 [33280/251450]\tLoss: 0.0512 (0.2402) \tAcc: 96.88% (85.24%) \tEmb_Norm: 61.91 (62.79)\n",
      "Train Epoch: 1 [33920/251450]\tLoss: 0.3069 (0.2376) \tAcc: 87.50% (85.37%) \tEmb_Norm: 61.88 (62.78)\n",
      "Train Epoch: 1 [34560/251450]\tLoss: 0.0465 (0.2353) \tAcc: 90.62% (85.47%) \tEmb_Norm: 61.86 (62.76)\n",
      "Train Epoch: 1 [35200/251450]\tLoss: 0.3077 (0.2336) \tAcc: 78.12% (85.51%) \tEmb_Norm: 61.82 (62.74)\n",
      "Train Epoch: 1 [35840/251450]\tLoss: 0.1213 (0.2318) \tAcc: 93.75% (85.58%) \tEmb_Norm: 61.79 (62.73)\n",
      "Train Epoch: 1 [36480/251450]\tLoss: 0.2632 (0.2294) \tAcc: 84.38% (85.72%) \tEmb_Norm: 61.77 (62.71)\n",
      "Train Epoch: 1 [37120/251450]\tLoss: 0.1075 (0.2278) \tAcc: 90.62% (85.80%) \tEmb_Norm: 61.74 (62.69)\n",
      "Train Epoch: 1 [37760/251450]\tLoss: 0.0905 (0.2258) \tAcc: 90.62% (85.85%) \tEmb_Norm: 61.71 (62.68)\n",
      "Train Epoch: 1 [38400/251450]\tLoss: 0.2221 (0.2244) \tAcc: 78.12% (85.89%) \tEmb_Norm: 61.68 (62.66)\n",
      "Train Epoch: 1 [39040/251450]\tLoss: 0.3695 (0.2223) \tAcc: 81.25% (86.00%) \tEmb_Norm: 61.65 (62.65)\n",
      "Train Epoch: 1 [39680/251450]\tLoss: 0.1942 (0.2201) \tAcc: 81.25% (86.08%) \tEmb_Norm: 61.62 (62.63)\n",
      "Train Epoch: 1 [40320/251450]\tLoss: 0.0084 (0.2180) \tAcc: 96.88% (86.18%) \tEmb_Norm: 61.60 (62.61)\n",
      "Train Epoch: 1 [40960/251450]\tLoss: 0.1178 (0.2164) \tAcc: 93.75% (86.26%) \tEmb_Norm: 61.57 (62.60)\n",
      "Train Epoch: 1 [41600/251450]\tLoss: 0.0847 (0.2145) \tAcc: 93.75% (86.33%) \tEmb_Norm: 61.54 (62.58)\n",
      "Train Epoch: 1 [42240/251450]\tLoss: 0.0152 (0.2130) \tAcc: 96.88% (86.39%) \tEmb_Norm: 61.51 (62.56)\n",
      "Train Epoch: 1 [42880/251450]\tLoss: 0.0055 (0.2117) \tAcc: 100.00% (86.47%) \tEmb_Norm: 61.48 (62.55)\n",
      "Train Epoch: 1 [43520/251450]\tLoss: 0.1049 (0.2101) \tAcc: 93.75% (86.55%) \tEmb_Norm: 61.45 (62.53)\n",
      "Train Epoch: 1 [44160/251450]\tLoss: 0.4626 (0.2087) \tAcc: 75.00% (86.59%) \tEmb_Norm: 61.43 (62.52)\n",
      "Train Epoch: 1 [44800/251450]\tLoss: 0.0065 (0.2075) \tAcc: 96.88% (86.64%) \tEmb_Norm: 61.40 (62.50)\n",
      "Train Epoch: 1 [45440/251450]\tLoss: 0.0665 (0.2056) \tAcc: 93.75% (86.74%) \tEmb_Norm: 61.37 (62.49)\n",
      "Train Epoch: 1 [46080/251450]\tLoss: 0.1362 (0.2042) \tAcc: 87.50% (86.80%) \tEmb_Norm: 61.34 (62.47)\n",
      "Train Epoch: 1 [46720/251450]\tLoss: 0.1238 (0.2026) \tAcc: 84.38% (86.87%) \tEmb_Norm: 61.32 (62.45)\n",
      "Train Epoch: 1 [47360/251450]\tLoss: 0.0769 (0.2011) \tAcc: 96.88% (86.97%) \tEmb_Norm: 61.29 (62.44)\n",
      "Train Epoch: 1 [48000/251450]\tLoss: 0.0271 (0.1996) \tAcc: 96.88% (87.03%) \tEmb_Norm: 61.26 (62.42)\n",
      "Train Epoch: 1 [48640/251450]\tLoss: 0.1296 (0.1981) \tAcc: 87.50% (87.09%) \tEmb_Norm: 61.24 (62.41)\n",
      "Train Epoch: 1 [49280/251450]\tLoss: 0.0780 (0.1966) \tAcc: 96.88% (87.16%) \tEmb_Norm: 61.21 (62.39)\n",
      "Train Epoch: 1 [49920/251450]\tLoss: 0.0412 (0.1948) \tAcc: 90.62% (87.25%) \tEmb_Norm: 61.19 (62.38)\n",
      "Train Epoch: 1 [50560/251450]\tLoss: 0.0835 (0.1939) \tAcc: 96.88% (87.30%) \tEmb_Norm: 61.16 (62.36)\n",
      "Train Epoch: 1 [51200/251450]\tLoss: 0.0829 (0.1926) \tAcc: 87.50% (87.35%) \tEmb_Norm: 61.13 (62.35)\n",
      "Train Epoch: 1 [51840/251450]\tLoss: 0.2221 (0.1915) \tAcc: 81.25% (87.41%) \tEmb_Norm: 61.10 (62.33)\n",
      "Train Epoch: 1 [52480/251450]\tLoss: 0.1082 (0.1901) \tAcc: 87.50% (87.48%) \tEmb_Norm: 61.08 (62.32)\n",
      "Train Epoch: 1 [53120/251450]\tLoss: 0.0363 (0.1889) \tAcc: 93.75% (87.54%) \tEmb_Norm: 61.05 (62.30)\n",
      "Train Epoch: 1 [53760/251450]\tLoss: 0.0921 (0.1877) \tAcc: 87.50% (87.59%) \tEmb_Norm: 61.03 (62.29)\n",
      "Train Epoch: 1 [54400/251450]\tLoss: 0.1078 (0.1866) \tAcc: 87.50% (87.63%) \tEmb_Norm: 61.00 (62.27)\n",
      "Train Epoch: 1 [55040/251450]\tLoss: 0.0177 (0.1858) \tAcc: 96.88% (87.68%) \tEmb_Norm: 60.97 (62.26)\n",
      "Train Epoch: 1 [55680/251450]\tLoss: 0.0400 (0.1847) \tAcc: 87.50% (87.71%) \tEmb_Norm: 60.94 (62.24)\n",
      "Train Epoch: 1 [56320/251450]\tLoss: 0.0170 (0.1837) \tAcc: 96.88% (87.76%) \tEmb_Norm: 60.91 (62.23)\n",
      "Train Epoch: 1 [56960/251450]\tLoss: 0.0056 (0.1830) \tAcc: 100.00% (87.82%) \tEmb_Norm: 60.88 (62.21)\n",
      "Train Epoch: 1 [57600/251450]\tLoss: 0.0908 (0.1819) \tAcc: 90.62% (87.86%) \tEmb_Norm: 60.86 (62.20)\n",
      "Train Epoch: 1 [58240/251450]\tLoss: 0.0799 (0.1809) \tAcc: 87.50% (87.91%) \tEmb_Norm: 60.83 (62.18)\n",
      "Train Epoch: 1 [58880/251450]\tLoss: 0.0275 (0.1798) \tAcc: 96.88% (87.96%) \tEmb_Norm: 60.81 (62.17)\n",
      "Train Epoch: 1 [59520/251450]\tLoss: 0.0210 (0.1791) \tAcc: 93.75% (87.99%) \tEmb_Norm: 60.78 (62.15)\n",
      "Train Epoch: 1 [60160/251450]\tLoss: 0.1509 (0.1784) \tAcc: 84.38% (88.02%) \tEmb_Norm: 60.75 (62.14)\n",
      "Train Epoch: 1 [60800/251450]\tLoss: 0.1334 (0.1775) \tAcc: 90.62% (88.06%) \tEmb_Norm: 60.72 (62.12)\n",
      "Train Epoch: 1 [61440/251450]\tLoss: 0.0044 (0.1768) \tAcc: 100.00% (88.09%) \tEmb_Norm: 60.69 (62.11)\n",
      "Train Epoch: 1 [62080/251450]\tLoss: 0.2447 (0.1759) \tAcc: 81.25% (88.13%) \tEmb_Norm: 60.67 (62.09)\n",
      "Train Epoch: 1 [62720/251450]\tLoss: 0.0215 (0.1750) \tAcc: 96.88% (88.17%) \tEmb_Norm: 60.64 (62.08)\n",
      "Train Epoch: 1 [63360/251450]\tLoss: 0.2555 (0.1743) \tAcc: 90.62% (88.22%) \tEmb_Norm: 60.61 (62.06)\n",
      "Train Epoch: 1 [64000/251450]\tLoss: 0.0030 (0.1736) \tAcc: 100.00% (88.25%) \tEmb_Norm: 60.58 (62.05)\n",
      "Train Epoch: 1 [64640/251450]\tLoss: 0.0471 (0.1730) \tAcc: 93.75% (88.28%) \tEmb_Norm: 60.55 (62.03)\n",
      "Train Epoch: 1 [65280/251450]\tLoss: 0.0847 (0.1722) \tAcc: 93.75% (88.33%) \tEmb_Norm: 60.53 (62.02)\n",
      "Train Epoch: 1 [65920/251450]\tLoss: 0.1304 (0.1714) \tAcc: 93.75% (88.37%) \tEmb_Norm: 60.50 (62.01)\n",
      "Train Epoch: 1 [66560/251450]\tLoss: 0.0592 (0.1710) \tAcc: 90.62% (88.38%) \tEmb_Norm: 60.47 (61.99)\n",
      "Train Epoch: 1 [67200/251450]\tLoss: 0.0000 (0.1701) \tAcc: 100.00% (88.42%) \tEmb_Norm: 60.44 (61.98)\n",
      "Train Epoch: 1 [67840/251450]\tLoss: 0.1868 (0.1697) \tAcc: 84.38% (88.44%) \tEmb_Norm: 60.42 (61.96)\n",
      "Train Epoch: 1 [68480/251450]\tLoss: 0.2024 (0.1691) \tAcc: 84.38% (88.45%) \tEmb_Norm: 60.39 (61.95)\n",
      "Train Epoch: 1 [69120/251450]\tLoss: 0.0778 (0.1685) \tAcc: 93.75% (88.48%) \tEmb_Norm: 60.36 (61.93)\n",
      "Train Epoch: 1 [69760/251450]\tLoss: 0.0782 (0.1677) \tAcc: 87.50% (88.52%) \tEmb_Norm: 60.33 (61.92)\n",
      "Train Epoch: 1 [70400/251450]\tLoss: 0.0363 (0.1669) \tAcc: 93.75% (88.55%) \tEmb_Norm: 60.31 (61.90)\n",
      "Train Epoch: 1 [71040/251450]\tLoss: 0.0994 (0.1662) \tAcc: 87.50% (88.58%) \tEmb_Norm: 60.28 (61.89)\n",
      "Train Epoch: 1 [71680/251450]\tLoss: 0.1297 (0.1655) \tAcc: 93.75% (88.63%) \tEmb_Norm: 60.25 (61.87)\n",
      "Train Epoch: 1 [72320/251450]\tLoss: 0.1989 (0.1648) \tAcc: 93.75% (88.65%) \tEmb_Norm: 60.23 (61.86)\n",
      "Train Epoch: 1 [72960/251450]\tLoss: 0.1082 (0.1640) \tAcc: 87.50% (88.69%) \tEmb_Norm: 60.20 (61.85)\n",
      "Train Epoch: 1 [73600/251450]\tLoss: 0.0000 (0.1635) \tAcc: 100.00% (88.71%) \tEmb_Norm: 60.17 (61.83)\n",
      "Train Epoch: 1 [74240/251450]\tLoss: 0.0125 (0.1628) \tAcc: 96.88% (88.74%) \tEmb_Norm: 60.15 (61.82)\n",
      "Train Epoch: 1 [74880/251450]\tLoss: 0.0829 (0.1622) \tAcc: 93.75% (88.77%) \tEmb_Norm: 60.12 (61.80)\n",
      "Train Epoch: 1 [75520/251450]\tLoss: 0.0047 (0.1617) \tAcc: 100.00% (88.79%) \tEmb_Norm: 60.09 (61.79)\n",
      "Train Epoch: 1 [76160/251450]\tLoss: 0.0225 (0.1613) \tAcc: 96.88% (88.81%) \tEmb_Norm: 60.06 (61.77)\n",
      "Train Epoch: 1 [76800/251450]\tLoss: 0.0456 (0.1608) \tAcc: 90.62% (88.85%) \tEmb_Norm: 60.04 (61.76)\n",
      "Train Epoch: 1 [77440/251450]\tLoss: 0.1083 (0.1601) \tAcc: 90.62% (88.88%) \tEmb_Norm: 60.01 (61.74)\n",
      "Train Epoch: 1 [78080/251450]\tLoss: 0.0166 (0.1593) \tAcc: 96.88% (88.92%) \tEmb_Norm: 59.99 (61.73)\n",
      "Train Epoch: 1 [78720/251450]\tLoss: 0.1571 (0.1586) \tAcc: 84.38% (88.95%) \tEmb_Norm: 59.96 (61.72)\n",
      "Train Epoch: 1 [79360/251450]\tLoss: 0.0100 (0.1581) \tAcc: 96.88% (88.99%) \tEmb_Norm: 59.93 (61.70)\n",
      "Train Epoch: 1 [80000/251450]\tLoss: 0.0000 (0.1573) \tAcc: 100.00% (89.04%) \tEmb_Norm: 59.91 (61.69)\n",
      "Train Epoch: 1 [80640/251450]\tLoss: 0.1543 (0.1566) \tAcc: 81.25% (89.06%) \tEmb_Norm: 59.88 (61.67)\n",
      "Train Epoch: 1 [81280/251450]\tLoss: 0.0000 (0.1561) \tAcc: 100.00% (89.08%) \tEmb_Norm: 59.86 (61.66)\n",
      "Train Epoch: 1 [81920/251450]\tLoss: 0.1980 (0.1557) \tAcc: 84.38% (89.10%) \tEmb_Norm: 59.83 (61.64)\n",
      "Train Epoch: 1 [82560/251450]\tLoss: 0.0000 (0.1550) \tAcc: 100.00% (89.13%) \tEmb_Norm: 59.80 (61.63)\n",
      "Train Epoch: 1 [83200/251450]\tLoss: 0.0000 (0.1547) \tAcc: 100.00% (89.15%) \tEmb_Norm: 59.78 (61.62)\n",
      "Train Epoch: 1 [83840/251450]\tLoss: 0.0846 (0.1541) \tAcc: 96.88% (89.18%) \tEmb_Norm: 59.75 (61.60)\n",
      "Train Epoch: 1 [84480/251450]\tLoss: 0.3601 (0.1537) \tAcc: 78.12% (89.20%) \tEmb_Norm: 59.72 (61.59)\n",
      "Train Epoch: 1 [85120/251450]\tLoss: 0.0800 (0.1530) \tAcc: 90.62% (89.23%) \tEmb_Norm: 59.70 (61.57)\n",
      "Train Epoch: 1 [85760/251450]\tLoss: 0.2058 (0.1525) \tAcc: 81.25% (89.25%) \tEmb_Norm: 59.67 (61.56)\n",
      "Train Epoch: 1 [86400/251450]\tLoss: 0.2321 (0.1521) \tAcc: 84.38% (89.27%) \tEmb_Norm: 59.65 (61.55)\n",
      "Train Epoch: 1 [87040/251450]\tLoss: 0.0393 (0.1515) \tAcc: 93.75% (89.30%) \tEmb_Norm: 59.62 (61.53)\n",
      "Train Epoch: 1 [87680/251450]\tLoss: 0.0108 (0.1511) \tAcc: 96.88% (89.32%) \tEmb_Norm: 59.59 (61.52)\n",
      "Train Epoch: 1 [88320/251450]\tLoss: 0.0320 (0.1506) \tAcc: 93.75% (89.34%) \tEmb_Norm: 59.57 (61.50)\n",
      "Train Epoch: 1 [88960/251450]\tLoss: 0.1675 (0.1503) \tAcc: 84.38% (89.36%) \tEmb_Norm: 59.54 (61.49)\n",
      "Train Epoch: 1 [89600/251450]\tLoss: 0.0851 (0.1499) \tAcc: 87.50% (89.38%) \tEmb_Norm: 59.51 (61.48)\n",
      "Train Epoch: 1 [90240/251450]\tLoss: 0.0611 (0.1493) \tAcc: 93.75% (89.41%) \tEmb_Norm: 59.49 (61.46)\n",
      "Train Epoch: 1 [90880/251450]\tLoss: 0.0403 (0.1489) \tAcc: 90.62% (89.43%) \tEmb_Norm: 59.46 (61.45)\n",
      "Train Epoch: 1 [91520/251450]\tLoss: 0.0729 (0.1483) \tAcc: 96.88% (89.45%) \tEmb_Norm: 59.43 (61.43)\n",
      "Train Epoch: 1 [92160/251450]\tLoss: 0.1480 (0.1478) \tAcc: 84.38% (89.48%) \tEmb_Norm: 59.41 (61.42)\n",
      "Train Epoch: 1 [92800/251450]\tLoss: 0.2904 (0.1474) \tAcc: 84.38% (89.50%) \tEmb_Norm: 59.38 (61.41)\n",
      "Train Epoch: 1 [93440/251450]\tLoss: 0.1445 (0.1470) \tAcc: 84.38% (89.52%) \tEmb_Norm: 59.35 (61.39)\n",
      "Train Epoch: 1 [94080/251450]\tLoss: 0.1419 (0.1467) \tAcc: 90.62% (89.53%) \tEmb_Norm: 59.33 (61.38)\n",
      "Train Epoch: 1 [94720/251450]\tLoss: 0.1450 (0.1462) \tAcc: 81.25% (89.55%) \tEmb_Norm: 59.30 (61.36)\n",
      "Train Epoch: 1 [95360/251450]\tLoss: 0.1359 (0.1459) \tAcc: 87.50% (89.57%) \tEmb_Norm: 59.27 (61.35)\n",
      "Train Epoch: 1 [96000/251450]\tLoss: 0.0178 (0.1455) \tAcc: 96.88% (89.59%) \tEmb_Norm: 59.25 (61.34)\n",
      "Train Epoch: 1 [96640/251450]\tLoss: 0.0000 (0.1449) \tAcc: 100.00% (89.61%) \tEmb_Norm: 59.22 (61.32)\n",
      "Train Epoch: 1 [97280/251450]\tLoss: 0.0970 (0.1446) \tAcc: 90.62% (89.63%) \tEmb_Norm: 59.20 (61.31)\n",
      "Train Epoch: 1 [97920/251450]\tLoss: 0.0331 (0.1440) \tAcc: 93.75% (89.66%) \tEmb_Norm: 59.17 (61.29)\n",
      "Train Epoch: 1 [98560/251450]\tLoss: 0.0563 (0.1435) \tAcc: 93.75% (89.68%) \tEmb_Norm: 59.15 (61.28)\n",
      "Train Epoch: 1 [99200/251450]\tLoss: 0.1893 (0.1431) \tAcc: 84.38% (89.70%) \tEmb_Norm: 59.12 (61.27)\n",
      "Train Epoch: 1 [99840/251450]\tLoss: 0.0939 (0.1427) \tAcc: 87.50% (89.72%) \tEmb_Norm: 59.10 (61.25)\n",
      "Train Epoch: 1 [100480/251450]\tLoss: 0.1492 (0.1422) \tAcc: 87.50% (89.75%) \tEmb_Norm: 59.07 (61.24)\n",
      "Train Epoch: 1 [101120/251450]\tLoss: 0.0462 (0.1417) \tAcc: 90.62% (89.77%) \tEmb_Norm: 59.05 (61.23)\n",
      "Train Epoch: 1 [101760/251450]\tLoss: 0.1569 (0.1413) \tAcc: 87.50% (89.79%) \tEmb_Norm: 59.02 (61.21)\n",
      "Train Epoch: 1 [102400/251450]\tLoss: 0.2048 (0.1410) \tAcc: 87.50% (89.80%) \tEmb_Norm: 58.99 (61.20)\n",
      "Train Epoch: 1 [103040/251450]\tLoss: 0.0529 (0.1407) \tAcc: 96.88% (89.82%) \tEmb_Norm: 58.97 (61.18)\n",
      "Train Epoch: 1 [103680/251450]\tLoss: 0.0058 (0.1405) \tAcc: 100.00% (89.84%) \tEmb_Norm: 58.94 (61.17)\n",
      "Train Epoch: 1 [104320/251450]\tLoss: 0.1002 (0.1400) \tAcc: 96.88% (89.86%) \tEmb_Norm: 58.91 (61.16)\n",
      "Train Epoch: 1 [104960/251450]\tLoss: 0.1006 (0.1396) \tAcc: 87.50% (89.87%) \tEmb_Norm: 58.89 (61.14)\n",
      "Train Epoch: 1 [105600/251450]\tLoss: 0.1107 (0.1393) \tAcc: 90.62% (89.89%) \tEmb_Norm: 58.86 (61.13)\n",
      "Train Epoch: 1 [106240/251450]\tLoss: 0.1597 (0.1389) \tAcc: 90.62% (89.90%) \tEmb_Norm: 58.84 (61.12)\n",
      "Train Epoch: 1 [106880/251450]\tLoss: 0.0874 (0.1386) \tAcc: 93.75% (89.92%) \tEmb_Norm: 58.81 (61.10)\n",
      "Train Epoch: 1 [107520/251450]\tLoss: 0.0169 (0.1384) \tAcc: 96.88% (89.93%) \tEmb_Norm: 58.78 (61.09)\n",
      "Train Epoch: 1 [108160/251450]\tLoss: 0.0000 (0.1382) \tAcc: 100.00% (89.94%) \tEmb_Norm: 58.75 (61.07)\n",
      "Train Epoch: 1 [108800/251450]\tLoss: 0.0331 (0.1378) \tAcc: 96.88% (89.96%) \tEmb_Norm: 58.73 (61.06)\n",
      "Train Epoch: 1 [109440/251450]\tLoss: 0.0485 (0.1375) \tAcc: 90.62% (89.98%) \tEmb_Norm: 58.70 (61.05)\n",
      "Train Epoch: 1 [110080/251450]\tLoss: 0.1822 (0.1372) \tAcc: 81.25% (89.99%) \tEmb_Norm: 58.68 (61.03)\n",
      "Train Epoch: 1 [110720/251450]\tLoss: 0.0457 (0.1367) \tAcc: 93.75% (90.02%) \tEmb_Norm: 58.65 (61.02)\n",
      "Train Epoch: 1 [111360/251450]\tLoss: 0.0891 (0.1363) \tAcc: 90.62% (90.04%) \tEmb_Norm: 58.63 (61.01)\n",
      "Train Epoch: 1 [112000/251450]\tLoss: 0.0609 (0.1359) \tAcc: 90.62% (90.06%) \tEmb_Norm: 58.60 (60.99)\n",
      "Train Epoch: 1 [112640/251450]\tLoss: 0.0401 (0.1354) \tAcc: 93.75% (90.09%) \tEmb_Norm: 58.58 (60.98)\n",
      "Train Epoch: 1 [113280/251450]\tLoss: 0.0134 (0.1351) \tAcc: 96.88% (90.11%) \tEmb_Norm: 58.55 (60.96)\n",
      "Train Epoch: 1 [113920/251450]\tLoss: 0.0881 (0.1347) \tAcc: 87.50% (90.12%) \tEmb_Norm: 58.53 (60.95)\n",
      "Train Epoch: 1 [114560/251450]\tLoss: 0.1557 (0.1344) \tAcc: 81.25% (90.13%) \tEmb_Norm: 58.50 (60.94)\n",
      "Train Epoch: 1 [115200/251450]\tLoss: 0.0876 (0.1341) \tAcc: 87.50% (90.14%) \tEmb_Norm: 58.48 (60.92)\n",
      "Train Epoch: 1 [115840/251450]\tLoss: 0.0335 (0.1337) \tAcc: 90.62% (90.16%) \tEmb_Norm: 58.45 (60.91)\n",
      "Train Epoch: 1 [116480/251450]\tLoss: 0.0832 (0.1333) \tAcc: 90.62% (90.19%) \tEmb_Norm: 58.43 (60.90)\n",
      "Train Epoch: 1 [117120/251450]\tLoss: 0.0444 (0.1328) \tAcc: 93.75% (90.21%) \tEmb_Norm: 58.41 (60.88)\n",
      "Train Epoch: 1 [117760/251450]\tLoss: 0.2035 (0.1326) \tAcc: 90.62% (90.22%) \tEmb_Norm: 58.38 (60.87)\n",
      "Train Epoch: 1 [118400/251450]\tLoss: 0.1009 (0.1322) \tAcc: 93.75% (90.25%) \tEmb_Norm: 58.36 (60.86)\n",
      "Train Epoch: 1 [119040/251450]\tLoss: 0.0021 (0.1318) \tAcc: 100.00% (90.26%) \tEmb_Norm: 58.33 (60.84)\n",
      "Train Epoch: 1 [119680/251450]\tLoss: 0.0194 (0.1316) \tAcc: 96.88% (90.27%) \tEmb_Norm: 58.30 (60.83)\n",
      "Train Epoch: 1 [120320/251450]\tLoss: 0.0504 (0.1313) \tAcc: 93.75% (90.29%) \tEmb_Norm: 58.28 (60.82)\n",
      "Train Epoch: 1 [120960/251450]\tLoss: 0.0595 (0.1309) \tAcc: 96.88% (90.31%) \tEmb_Norm: 58.25 (60.80)\n",
      "Train Epoch: 1 [121600/251450]\tLoss: 0.1364 (0.1307) \tAcc: 84.38% (90.32%) \tEmb_Norm: 58.23 (60.79)\n",
      "Train Epoch: 1 [122240/251450]\tLoss: 0.0107 (0.1304) \tAcc: 96.88% (90.34%) \tEmb_Norm: 58.20 (60.77)\n",
      "Train Epoch: 1 [122880/251450]\tLoss: 0.1218 (0.1300) \tAcc: 81.25% (90.36%) \tEmb_Norm: 58.18 (60.76)\n",
      "Train Epoch: 1 [123520/251450]\tLoss: 0.0810 (0.1297) \tAcc: 87.50% (90.38%) \tEmb_Norm: 58.15 (60.75)\n",
      "Train Epoch: 1 [124160/251450]\tLoss: 0.0299 (0.1295) \tAcc: 96.88% (90.39%) \tEmb_Norm: 58.13 (60.73)\n",
      "Train Epoch: 1 [124800/251450]\tLoss: 0.0734 (0.1292) \tAcc: 93.75% (90.40%) \tEmb_Norm: 58.10 (60.72)\n",
      "Train Epoch: 1 [125440/251450]\tLoss: 0.1820 (0.1290) \tAcc: 84.38% (90.41%) \tEmb_Norm: 58.07 (60.71)\n",
      "Train Epoch: 1 [126080/251450]\tLoss: 0.0823 (0.1287) \tAcc: 93.75% (90.42%) \tEmb_Norm: 58.05 (60.69)\n",
      "Train Epoch: 1 [126720/251450]\tLoss: 0.0644 (0.1284) \tAcc: 90.62% (90.43%) \tEmb_Norm: 58.02 (60.68)\n",
      "Train Epoch: 1 [127360/251450]\tLoss: 0.0691 (0.1282) \tAcc: 93.75% (90.44%) \tEmb_Norm: 58.00 (60.67)\n",
      "Train Epoch: 1 [128000/251450]\tLoss: 0.0505 (0.1278) \tAcc: 93.75% (90.46%) \tEmb_Norm: 57.97 (60.65)\n",
      "Train Epoch: 1 [128640/251450]\tLoss: 0.1411 (0.1275) \tAcc: 90.62% (90.48%) \tEmb_Norm: 57.95 (60.64)\n",
      "Train Epoch: 1 [129280/251450]\tLoss: 0.0927 (0.1272) \tAcc: 87.50% (90.50%) \tEmb_Norm: 57.92 (60.63)\n",
      "Train Epoch: 1 [129920/251450]\tLoss: 0.0613 (0.1269) \tAcc: 93.75% (90.51%) \tEmb_Norm: 57.90 (60.61)\n",
      "Train Epoch: 1 [130560/251450]\tLoss: 0.1011 (0.1269) \tAcc: 90.62% (90.51%) \tEmb_Norm: 57.87 (60.60)\n",
      "Train Epoch: 1 [131200/251450]\tLoss: 0.1326 (0.1265) \tAcc: 93.75% (90.53%) \tEmb_Norm: 57.84 (60.59)\n",
      "Train Epoch: 1 [131840/251450]\tLoss: 0.0606 (0.1263) \tAcc: 96.88% (90.54%) \tEmb_Norm: 57.82 (60.57)\n",
      "Train Epoch: 1 [132480/251450]\tLoss: 0.0132 (0.1260) \tAcc: 96.88% (90.55%) \tEmb_Norm: 57.79 (60.56)\n",
      "Train Epoch: 1 [133120/251450]\tLoss: 0.0331 (0.1258) \tAcc: 96.88% (90.56%) \tEmb_Norm: 57.77 (60.55)\n",
      "Train Epoch: 1 [133760/251450]\tLoss: 0.2418 (0.1256) \tAcc: 84.38% (90.57%) \tEmb_Norm: 57.74 (60.53)\n",
      "Train Epoch: 1 [134400/251450]\tLoss: 0.1077 (0.1253) \tAcc: 93.75% (90.59%) \tEmb_Norm: 57.72 (60.52)\n",
      "Train Epoch: 1 [135040/251450]\tLoss: 0.0706 (0.1252) \tAcc: 93.75% (90.59%) \tEmb_Norm: 57.69 (60.51)\n",
      "Train Epoch: 1 [135680/251450]\tLoss: 0.1191 (0.1250) \tAcc: 87.50% (90.61%) \tEmb_Norm: 57.66 (60.49)\n",
      "Train Epoch: 1 [136320/251450]\tLoss: 0.0268 (0.1246) \tAcc: 93.75% (90.63%) \tEmb_Norm: 57.64 (60.48)\n",
      "Train Epoch: 1 [136960/251450]\tLoss: 0.1473 (0.1243) \tAcc: 93.75% (90.64%) \tEmb_Norm: 57.61 (60.47)\n",
      "Train Epoch: 1 [137600/251450]\tLoss: 0.0369 (0.1240) \tAcc: 96.88% (90.65%) \tEmb_Norm: 57.59 (60.45)\n",
      "Train Epoch: 1 [138240/251450]\tLoss: 0.0000 (0.1236) \tAcc: 100.00% (90.67%) \tEmb_Norm: 57.57 (60.44)\n",
      "Train Epoch: 1 [138880/251450]\tLoss: 0.1146 (0.1233) \tAcc: 87.50% (90.69%) \tEmb_Norm: 57.54 (60.43)\n",
      "Train Epoch: 1 [139520/251450]\tLoss: 0.0371 (0.1230) \tAcc: 93.75% (90.70%) \tEmb_Norm: 57.52 (60.41)\n",
      "Train Epoch: 1 [140160/251450]\tLoss: 0.2168 (0.1228) \tAcc: 84.38% (90.70%) \tEmb_Norm: 57.50 (60.40)\n",
      "Train Epoch: 1 [140800/251450]\tLoss: 0.3052 (0.1226) \tAcc: 90.62% (90.72%) \tEmb_Norm: 57.47 (60.39)\n",
      "Train Epoch: 1 [141440/251450]\tLoss: 0.0258 (0.1224) \tAcc: 96.88% (90.73%) \tEmb_Norm: 57.44 (60.37)\n",
      "Train Epoch: 1 [142080/251450]\tLoss: 0.0000 (0.1222) \tAcc: 100.00% (90.74%) \tEmb_Norm: 57.42 (60.36)\n",
      "Train Epoch: 1 [142720/251450]\tLoss: 0.0000 (0.1219) \tAcc: 100.00% (90.76%) \tEmb_Norm: 57.39 (60.35)\n",
      "Train Epoch: 1 [143360/251450]\tLoss: 0.0469 (0.1217) \tAcc: 93.75% (90.76%) \tEmb_Norm: 57.37 (60.33)\n",
      "Train Epoch: 1 [144000/251450]\tLoss: 0.0308 (0.1214) \tAcc: 96.88% (90.78%) \tEmb_Norm: 57.34 (60.32)\n",
      "Train Epoch: 1 [144640/251450]\tLoss: 0.0228 (0.1213) \tAcc: 96.88% (90.79%) \tEmb_Norm: 57.32 (60.31)\n",
      "Train Epoch: 1 [145280/251450]\tLoss: 0.0053 (0.1210) \tAcc: 100.00% (90.79%) \tEmb_Norm: 57.29 (60.29)\n",
      "Train Epoch: 1 [145920/251450]\tLoss: 0.0761 (0.1209) \tAcc: 93.75% (90.80%) \tEmb_Norm: 57.26 (60.28)\n",
      "Train Epoch: 1 [146560/251450]\tLoss: 0.1384 (0.1206) \tAcc: 81.25% (90.82%) \tEmb_Norm: 57.24 (60.27)\n",
      "Train Epoch: 1 [147200/251450]\tLoss: 0.0861 (0.1204) \tAcc: 87.50% (90.83%) \tEmb_Norm: 57.21 (60.25)\n",
      "Train Epoch: 1 [147840/251450]\tLoss: 0.0766 (0.1202) \tAcc: 93.75% (90.83%) \tEmb_Norm: 57.19 (60.24)\n",
      "Train Epoch: 1 [148480/251450]\tLoss: 0.0000 (0.1201) \tAcc: 100.00% (90.84%) \tEmb_Norm: 57.16 (60.23)\n",
      "Train Epoch: 1 [149120/251450]\tLoss: 0.0970 (0.1198) \tAcc: 93.75% (90.85%) \tEmb_Norm: 57.14 (60.21)\n",
      "Train Epoch: 1 [149760/251450]\tLoss: 0.0977 (0.1196) \tAcc: 93.75% (90.86%) \tEmb_Norm: 57.11 (60.20)\n",
      "Train Epoch: 1 [150400/251450]\tLoss: 0.0989 (0.1193) \tAcc: 87.50% (90.88%) \tEmb_Norm: 57.09 (60.19)\n",
      "Train Epoch: 1 [151040/251450]\tLoss: 0.1390 (0.1192) \tAcc: 90.62% (90.89%) \tEmb_Norm: 57.06 (60.18)\n",
      "Train Epoch: 1 [151680/251450]\tLoss: 0.1616 (0.1189) \tAcc: 90.62% (90.90%) \tEmb_Norm: 57.04 (60.16)\n",
      "Train Epoch: 1 [152320/251450]\tLoss: 0.1405 (0.1187) \tAcc: 87.50% (90.92%) \tEmb_Norm: 57.01 (60.15)\n",
      "Train Epoch: 1 [152960/251450]\tLoss: 0.0639 (0.1185) \tAcc: 93.75% (90.93%) \tEmb_Norm: 56.99 (60.14)\n",
      "Train Epoch: 1 [153600/251450]\tLoss: 0.0375 (0.1183) \tAcc: 96.88% (90.94%) \tEmb_Norm: 56.96 (60.12)\n",
      "Train Epoch: 1 [154240/251450]\tLoss: 0.0061 (0.1181) \tAcc: 100.00% (90.95%) \tEmb_Norm: 56.94 (60.11)\n",
      "Train Epoch: 1 [154880/251450]\tLoss: 0.0458 (0.1179) \tAcc: 87.50% (90.96%) \tEmb_Norm: 56.91 (60.10)\n",
      "Train Epoch: 1 [155520/251450]\tLoss: 0.1681 (0.1177) \tAcc: 87.50% (90.97%) \tEmb_Norm: 56.89 (60.08)\n",
      "Train Epoch: 1 [156160/251450]\tLoss: 0.0677 (0.1175) \tAcc: 93.75% (90.98%) \tEmb_Norm: 56.86 (60.07)\n",
      "Train Epoch: 1 [156800/251450]\tLoss: 0.0109 (0.1174) \tAcc: 96.88% (90.99%) \tEmb_Norm: 56.83 (60.06)\n",
      "Train Epoch: 1 [157440/251450]\tLoss: 0.0782 (0.1172) \tAcc: 87.50% (91.00%) \tEmb_Norm: 56.81 (60.04)\n",
      "Train Epoch: 1 [158080/251450]\tLoss: 0.0757 (0.1170) \tAcc: 93.75% (91.00%) \tEmb_Norm: 56.78 (60.03)\n",
      "Train Epoch: 1 [158720/251450]\tLoss: 0.0287 (0.1169) \tAcc: 96.88% (91.01%) \tEmb_Norm: 56.76 (60.02)\n",
      "Train Epoch: 1 [159360/251450]\tLoss: 0.0198 (0.1166) \tAcc: 93.75% (91.03%) \tEmb_Norm: 56.73 (60.00)\n",
      "Train Epoch: 1 [160000/251450]\tLoss: 0.0395 (0.1165) \tAcc: 93.75% (91.04%) \tEmb_Norm: 56.71 (59.99)\n",
      "Train Epoch: 1 [160640/251450]\tLoss: 0.1037 (0.1163) \tAcc: 87.50% (91.05%) \tEmb_Norm: 56.68 (59.98)\n",
      "Train Epoch: 1 [161280/251450]\tLoss: 0.0470 (0.1160) \tAcc: 93.75% (91.06%) \tEmb_Norm: 56.66 (59.96)\n",
      "Train Epoch: 1 [161920/251450]\tLoss: 0.1082 (0.1158) \tAcc: 93.75% (91.06%) \tEmb_Norm: 56.63 (59.95)\n",
      "Train Epoch: 1 [162560/251450]\tLoss: 0.0078 (0.1156) \tAcc: 96.88% (91.07%) \tEmb_Norm: 56.61 (59.94)\n",
      "Train Epoch: 1 [163200/251450]\tLoss: 0.0396 (0.1153) \tAcc: 96.88% (91.09%) \tEmb_Norm: 56.59 (59.93)\n",
      "Train Epoch: 1 [163840/251450]\tLoss: 0.1145 (0.1152) \tAcc: 93.75% (91.09%) \tEmb_Norm: 56.56 (59.91)\n",
      "Train Epoch: 1 [164480/251450]\tLoss: 0.0037 (0.1150) \tAcc: 100.00% (91.10%) \tEmb_Norm: 56.54 (59.90)\n",
      "Train Epoch: 1 [165120/251450]\tLoss: 0.0879 (0.1149) \tAcc: 87.50% (91.10%) \tEmb_Norm: 56.51 (59.89)\n",
      "Train Epoch: 1 [165760/251450]\tLoss: 0.0266 (0.1147) \tAcc: 96.88% (91.11%) \tEmb_Norm: 56.48 (59.87)\n",
      "Train Epoch: 1 [166400/251450]\tLoss: 0.0763 (0.1145) \tAcc: 90.62% (91.12%) \tEmb_Norm: 56.46 (59.86)\n",
      "Train Epoch: 1 [167040/251450]\tLoss: 0.1394 (0.1144) \tAcc: 87.50% (91.13%) \tEmb_Norm: 56.43 (59.85)\n",
      "Train Epoch: 1 [167680/251450]\tLoss: 0.1233 (0.1143) \tAcc: 93.75% (91.14%) \tEmb_Norm: 56.41 (59.83)\n",
      "Train Epoch: 1 [168320/251450]\tLoss: 0.0000 (0.1140) \tAcc: 100.00% (91.15%) \tEmb_Norm: 56.38 (59.82)\n",
      "Train Epoch: 1 [168960/251450]\tLoss: 0.1398 (0.1138) \tAcc: 93.75% (91.16%) \tEmb_Norm: 56.36 (59.81)\n",
      "Train Epoch: 1 [169600/251450]\tLoss: 0.0128 (0.1138) \tAcc: 96.88% (91.17%) \tEmb_Norm: 56.33 (59.79)\n",
      "Train Epoch: 1 [170240/251450]\tLoss: 0.1187 (0.1136) \tAcc: 87.50% (91.18%) \tEmb_Norm: 56.31 (59.78)\n",
      "Train Epoch: 1 [170880/251450]\tLoss: 0.0709 (0.1134) \tAcc: 90.62% (91.19%) \tEmb_Norm: 56.28 (59.77)\n",
      "Train Epoch: 1 [171520/251450]\tLoss: 0.2904 (0.1133) \tAcc: 78.12% (91.20%) \tEmb_Norm: 56.26 (59.76)\n",
      "Train Epoch: 1 [172160/251450]\tLoss: 0.0611 (0.1132) \tAcc: 93.75% (91.20%) \tEmb_Norm: 56.23 (59.74)\n",
      "Train Epoch: 1 [172800/251450]\tLoss: 0.0854 (0.1130) \tAcc: 90.62% (91.21%) \tEmb_Norm: 56.20 (59.73)\n",
      "Train Epoch: 1 [173440/251450]\tLoss: 0.1413 (0.1128) \tAcc: 93.75% (91.22%) \tEmb_Norm: 56.18 (59.72)\n",
      "Train Epoch: 1 [174080/251450]\tLoss: 0.0417 (0.1126) \tAcc: 93.75% (91.24%) \tEmb_Norm: 56.16 (59.70)\n",
      "Train Epoch: 1 [174720/251450]\tLoss: 0.0985 (0.1124) \tAcc: 90.62% (91.25%) \tEmb_Norm: 56.13 (59.69)\n",
      "Train Epoch: 1 [175360/251450]\tLoss: 0.0108 (0.1121) \tAcc: 96.88% (91.27%) \tEmb_Norm: 56.11 (59.68)\n",
      "Train Epoch: 1 [176000/251450]\tLoss: 0.0156 (0.1119) \tAcc: 96.88% (91.28%) \tEmb_Norm: 56.08 (59.66)\n",
      "Train Epoch: 1 [176640/251450]\tLoss: 0.0620 (0.1118) \tAcc: 93.75% (91.29%) \tEmb_Norm: 56.06 (59.65)\n",
      "Train Epoch: 1 [177280/251450]\tLoss: 0.0000 (0.1117) \tAcc: 100.00% (91.30%) \tEmb_Norm: 56.03 (59.64)\n",
      "Train Epoch: 1 [177920/251450]\tLoss: 0.0195 (0.1115) \tAcc: 96.88% (91.30%) \tEmb_Norm: 56.01 (59.63)\n",
      "Train Epoch: 1 [178560/251450]\tLoss: 0.0138 (0.1114) \tAcc: 96.88% (91.31%) \tEmb_Norm: 55.98 (59.61)\n",
      "Train Epoch: 1 [179200/251450]\tLoss: 0.0165 (0.1112) \tAcc: 96.88% (91.32%) \tEmb_Norm: 55.95 (59.60)\n",
      "Train Epoch: 1 [179840/251450]\tLoss: 0.0714 (0.1111) \tAcc: 93.75% (91.33%) \tEmb_Norm: 55.93 (59.59)\n",
      "Train Epoch: 1 [180480/251450]\tLoss: 0.0375 (0.1108) \tAcc: 96.88% (91.34%) \tEmb_Norm: 55.91 (59.57)\n",
      "Train Epoch: 1 [181120/251450]\tLoss: 0.1742 (0.1107) \tAcc: 93.75% (91.35%) \tEmb_Norm: 55.88 (59.56)\n",
      "Train Epoch: 1 [181760/251450]\tLoss: 0.0277 (0.1106) \tAcc: 93.75% (91.35%) \tEmb_Norm: 55.86 (59.55)\n",
      "Train Epoch: 1 [182400/251450]\tLoss: 0.0364 (0.1104) \tAcc: 96.88% (91.36%) \tEmb_Norm: 55.83 (59.53)\n",
      "Train Epoch: 1 [183040/251450]\tLoss: 0.0299 (0.1102) \tAcc: 93.75% (91.37%) \tEmb_Norm: 55.81 (59.52)\n",
      "Train Epoch: 1 [183680/251450]\tLoss: 0.0228 (0.1101) \tAcc: 96.88% (91.38%) \tEmb_Norm: 55.78 (59.51)\n",
      "Train Epoch: 1 [184320/251450]\tLoss: 0.1025 (0.1099) \tAcc: 87.50% (91.39%) \tEmb_Norm: 55.76 (59.50)\n",
      "Train Epoch: 1 [184960/251450]\tLoss: 0.0919 (0.1097) \tAcc: 90.62% (91.39%) \tEmb_Norm: 55.73 (59.48)\n",
      "Train Epoch: 1 [185600/251450]\tLoss: 0.0360 (0.1095) \tAcc: 96.88% (91.40%) \tEmb_Norm: 55.71 (59.47)\n",
      "Train Epoch: 1 [186240/251450]\tLoss: 0.0658 (0.1094) \tAcc: 90.62% (91.41%) \tEmb_Norm: 55.68 (59.46)\n",
      "Train Epoch: 1 [186880/251450]\tLoss: 0.1190 (0.1092) \tAcc: 93.75% (91.42%) \tEmb_Norm: 55.66 (59.44)\n",
      "Train Epoch: 1 [187520/251450]\tLoss: 0.0365 (0.1089) \tAcc: 93.75% (91.43%) \tEmb_Norm: 55.64 (59.43)\n",
      "Train Epoch: 1 [188160/251450]\tLoss: 0.0728 (0.1087) \tAcc: 93.75% (91.44%) \tEmb_Norm: 55.62 (59.42)\n",
      "Train Epoch: 1 [188800/251450]\tLoss: 0.0415 (0.1086) \tAcc: 93.75% (91.46%) \tEmb_Norm: 55.59 (59.40)\n",
      "Train Epoch: 1 [189440/251450]\tLoss: 0.0788 (0.1084) \tAcc: 93.75% (91.46%) \tEmb_Norm: 55.57 (59.39)\n",
      "Train Epoch: 1 [190080/251450]\tLoss: 0.0374 (0.1082) \tAcc: 93.75% (91.47%) \tEmb_Norm: 55.54 (59.38)\n",
      "Train Epoch: 1 [190720/251450]\tLoss: 0.0629 (0.1080) \tAcc: 90.62% (91.48%) \tEmb_Norm: 55.52 (59.37)\n",
      "Train Epoch: 1 [191360/251450]\tLoss: 0.0665 (0.1079) \tAcc: 93.75% (91.49%) \tEmb_Norm: 55.49 (59.35)\n",
      "Train Epoch: 1 [192000/251450]\tLoss: 0.0291 (0.1078) \tAcc: 96.88% (91.50%) \tEmb_Norm: 55.47 (59.34)\n",
      "Train Epoch: 1 [192640/251450]\tLoss: 0.0035 (0.1076) \tAcc: 100.00% (91.51%) \tEmb_Norm: 55.44 (59.33)\n",
      "Train Epoch: 1 [193280/251450]\tLoss: 0.1342 (0.1076) \tAcc: 84.38% (91.51%) \tEmb_Norm: 55.42 (59.31)\n",
      "Train Epoch: 1 [193920/251450]\tLoss: 0.0539 (0.1074) \tAcc: 93.75% (91.52%) \tEmb_Norm: 55.39 (59.30)\n",
      "Train Epoch: 1 [194560/251450]\tLoss: 0.0494 (0.1072) \tAcc: 90.62% (91.52%) \tEmb_Norm: 55.37 (59.29)\n",
      "Train Epoch: 1 [195200/251450]\tLoss: 0.0313 (0.1071) \tAcc: 96.88% (91.52%) \tEmb_Norm: 55.34 (59.28)\n",
      "Train Epoch: 1 [195840/251450]\tLoss: 0.0222 (0.1070) \tAcc: 93.75% (91.53%) \tEmb_Norm: 55.32 (59.26)\n",
      "Train Epoch: 1 [196480/251450]\tLoss: 0.0205 (0.1069) \tAcc: 96.88% (91.54%) \tEmb_Norm: 55.29 (59.25)\n",
      "Train Epoch: 1 [197120/251450]\tLoss: 0.0934 (0.1067) \tAcc: 90.62% (91.55%) \tEmb_Norm: 55.27 (59.24)\n",
      "Train Epoch: 1 [197760/251450]\tLoss: 0.0211 (0.1065) \tAcc: 93.75% (91.56%) \tEmb_Norm: 55.24 (59.22)\n",
      "Train Epoch: 1 [198400/251450]\tLoss: 0.0420 (0.1064) \tAcc: 93.75% (91.56%) \tEmb_Norm: 55.22 (59.21)\n",
      "Train Epoch: 1 [199040/251450]\tLoss: 0.0442 (0.1063) \tAcc: 93.75% (91.57%) \tEmb_Norm: 55.19 (59.20)\n",
      "Train Epoch: 1 [199680/251450]\tLoss: 0.0869 (0.1061) \tAcc: 90.62% (91.57%) \tEmb_Norm: 55.17 (59.19)\n",
      "Train Epoch: 1 [200320/251450]\tLoss: 0.0924 (0.1059) \tAcc: 90.62% (91.58%) \tEmb_Norm: 55.15 (59.17)\n",
      "Train Epoch: 1 [200960/251450]\tLoss: 0.0172 (0.1058) \tAcc: 96.88% (91.59%) \tEmb_Norm: 55.12 (59.16)\n",
      "Train Epoch: 1 [201600/251450]\tLoss: 0.0240 (0.1056) \tAcc: 96.88% (91.60%) \tEmb_Norm: 55.10 (59.15)\n",
      "=============== TEST epoch 1 ===============\n",
      "\n",
      "Test set: Average loss: 0.0638, Accuracy: 93.45%\n",
      "\n",
      "=============== checkpoint epoch 1 ===============\n",
      "=============== TRAIN epoch 2 ===============\n",
      "Train Epoch: 2 [0/251450]\tLoss: 0.0153 (0.0153) \tAcc: 96.88% (96.88%) \tEmb_Norm: 55.08 (55.08)\n",
      "Train Epoch: 2 [640/251450]\tLoss: 0.0964 (0.0684) \tAcc: 93.75% (93.45%) \tEmb_Norm: 55.06 (55.07)\n",
      "Train Epoch: 2 [1280/251450]\tLoss: 0.0129 (0.0619) \tAcc: 96.88% (94.13%) \tEmb_Norm: 55.03 (55.05)\n",
      "Train Epoch: 2 [1920/251450]\tLoss: 0.0494 (0.0578) \tAcc: 87.50% (94.16%) \tEmb_Norm: 55.01 (55.04)\n",
      "Train Epoch: 2 [2560/251450]\tLoss: 0.1361 (0.0594) \tAcc: 81.25% (93.75%) \tEmb_Norm: 54.98 (55.03)\n",
      "Train Epoch: 2 [3200/251450]\tLoss: 0.0323 (0.0605) \tAcc: 93.75% (93.97%) \tEmb_Norm: 54.96 (55.02)\n",
      "Train Epoch: 2 [3840/251450]\tLoss: 0.0235 (0.0601) \tAcc: 96.88% (94.14%) \tEmb_Norm: 54.93 (55.01)\n",
      "Train Epoch: 2 [4480/251450]\tLoss: 0.0471 (0.0593) \tAcc: 93.75% (94.17%) \tEmb_Norm: 54.91 (54.99)\n",
      "Train Epoch: 2 [5120/251450]\tLoss: 0.0506 (0.0607) \tAcc: 93.75% (94.16%) \tEmb_Norm: 54.88 (54.98)\n",
      "Train Epoch: 2 [5760/251450]\tLoss: 0.0000 (0.0584) \tAcc: 100.00% (94.30%) \tEmb_Norm: 54.86 (54.97)\n",
      "Train Epoch: 2 [6400/251450]\tLoss: 0.0138 (0.0567) \tAcc: 96.88% (94.42%) \tEmb_Norm: 54.84 (54.96)\n",
      "Train Epoch: 2 [7040/251450]\tLoss: 0.0209 (0.0564) \tAcc: 93.75% (94.46%) \tEmb_Norm: 54.81 (54.95)\n",
      "Train Epoch: 2 [7680/251450]\tLoss: 0.0069 (0.0557) \tAcc: 96.88% (94.41%) \tEmb_Norm: 54.79 (54.93)\n",
      "Train Epoch: 2 [8320/251450]\tLoss: 0.0134 (0.0576) \tAcc: 96.88% (94.31%) \tEmb_Norm: 54.77 (54.92)\n",
      "Train Epoch: 2 [8960/251450]\tLoss: 0.0344 (0.0581) \tAcc: 93.75% (94.34%) \tEmb_Norm: 54.74 (54.91)\n",
      "Train Epoch: 2 [9600/251450]\tLoss: 0.0575 (0.0587) \tAcc: 93.75% (94.28%) \tEmb_Norm: 54.72 (54.90)\n",
      "Train Epoch: 2 [10240/251450]\tLoss: 0.0337 (0.0594) \tAcc: 96.88% (94.28%) \tEmb_Norm: 54.69 (54.89)\n",
      "Train Epoch: 2 [10880/251450]\tLoss: 0.0000 (0.0597) \tAcc: 100.00% (94.25%) \tEmb_Norm: 54.66 (54.87)\n",
      "Train Epoch: 2 [11520/251450]\tLoss: 0.0507 (0.0597) \tAcc: 96.88% (94.24%) \tEmb_Norm: 54.64 (54.86)\n",
      "Train Epoch: 2 [12160/251450]\tLoss: 0.0736 (0.0602) \tAcc: 93.75% (94.22%) \tEmb_Norm: 54.61 (54.85)\n",
      "Train Epoch: 2 [12800/251450]\tLoss: 0.0093 (0.0602) \tAcc: 96.88% (94.21%) \tEmb_Norm: 54.59 (54.84)\n",
      "Train Epoch: 2 [13440/251450]\tLoss: 0.0974 (0.0600) \tAcc: 93.75% (94.24%) \tEmb_Norm: 54.57 (54.82)\n",
      "Train Epoch: 2 [14080/251450]\tLoss: 0.0439 (0.0610) \tAcc: 96.88% (94.12%) \tEmb_Norm: 54.54 (54.81)\n",
      "Train Epoch: 2 [14720/251450]\tLoss: 0.0542 (0.0599) \tAcc: 90.62% (94.21%) \tEmb_Norm: 54.52 (54.80)\n",
      "Train Epoch: 2 [15360/251450]\tLoss: 0.0622 (0.0593) \tAcc: 93.75% (94.23%) \tEmb_Norm: 54.49 (54.79)\n",
      "Train Epoch: 2 [16000/251450]\tLoss: 0.0135 (0.0588) \tAcc: 96.88% (94.27%) \tEmb_Norm: 54.47 (54.77)\n",
      "Train Epoch: 2 [16640/251450]\tLoss: 0.0000 (0.0597) \tAcc: 100.00% (94.19%) \tEmb_Norm: 54.44 (54.76)\n",
      "Train Epoch: 2 [17280/251450]\tLoss: 0.0162 (0.0593) \tAcc: 96.88% (94.19%) \tEmb_Norm: 54.42 (54.75)\n",
      "Train Epoch: 2 [17920/251450]\tLoss: 0.1060 (0.0597) \tAcc: 87.50% (94.17%) \tEmb_Norm: 54.40 (54.74)\n",
      "Train Epoch: 2 [18560/251450]\tLoss: 0.0911 (0.0593) \tAcc: 96.88% (94.21%) \tEmb_Norm: 54.37 (54.73)\n",
      "Train Epoch: 2 [19200/251450]\tLoss: 0.0000 (0.0598) \tAcc: 100.00% (94.21%) \tEmb_Norm: 54.35 (54.71)\n",
      "Train Epoch: 2 [19840/251450]\tLoss: 0.0608 (0.0602) \tAcc: 96.88% (94.15%) \tEmb_Norm: 54.32 (54.70)\n",
      "Train Epoch: 2 [20480/251450]\tLoss: 0.0842 (0.0602) \tAcc: 93.75% (94.13%) \tEmb_Norm: 54.30 (54.69)\n",
      "Train Epoch: 2 [21120/251450]\tLoss: 0.0418 (0.0595) \tAcc: 93.75% (94.16%) \tEmb_Norm: 54.27 (54.68)\n",
      "Train Epoch: 2 [21760/251450]\tLoss: 0.0485 (0.0593) \tAcc: 90.62% (94.16%) \tEmb_Norm: 54.25 (54.66)\n",
      "Train Epoch: 2 [22400/251450]\tLoss: 0.0459 (0.0590) \tAcc: 90.62% (94.14%) \tEmb_Norm: 54.23 (54.65)\n",
      "Train Epoch: 2 [23040/251450]\tLoss: 0.0877 (0.0592) \tAcc: 93.75% (94.14%) \tEmb_Norm: 54.20 (54.64)\n",
      "Train Epoch: 2 [23680/251450]\tLoss: 0.0885 (0.0595) \tAcc: 90.62% (94.13%) \tEmb_Norm: 54.18 (54.63)\n",
      "Train Epoch: 2 [24320/251450]\tLoss: 0.0799 (0.0594) \tAcc: 93.75% (94.14%) \tEmb_Norm: 54.15 (54.62)\n",
      "Train Epoch: 2 [24960/251450]\tLoss: 0.0166 (0.0590) \tAcc: 96.88% (94.17%) \tEmb_Norm: 54.13 (54.60)\n",
      "Train Epoch: 2 [25600/251450]\tLoss: 0.0556 (0.0589) \tAcc: 90.62% (94.16%) \tEmb_Norm: 54.10 (54.59)\n",
      "Train Epoch: 2 [26240/251450]\tLoss: 0.0515 (0.0593) \tAcc: 93.75% (94.15%) \tEmb_Norm: 54.08 (54.58)\n",
      "Train Epoch: 2 [26880/251450]\tLoss: 0.0984 (0.0600) \tAcc: 90.62% (94.08%) \tEmb_Norm: 54.05 (54.57)\n",
      "Train Epoch: 2 [27520/251450]\tLoss: 0.0951 (0.0600) \tAcc: 90.62% (94.09%) \tEmb_Norm: 54.03 (54.55)\n",
      "Train Epoch: 2 [28160/251450]\tLoss: 0.1399 (0.0600) \tAcc: 87.50% (94.09%) \tEmb_Norm: 54.00 (54.54)\n",
      "Train Epoch: 2 [28800/251450]\tLoss: 0.0162 (0.0596) \tAcc: 96.88% (94.10%) \tEmb_Norm: 53.98 (54.53)\n",
      "Train Epoch: 2 [29440/251450]\tLoss: 0.0878 (0.0594) \tAcc: 90.62% (94.09%) \tEmb_Norm: 53.96 (54.52)\n",
      "Train Epoch: 2 [30080/251450]\tLoss: 0.0000 (0.0591) \tAcc: 100.00% (94.09%) \tEmb_Norm: 53.93 (54.51)\n",
      "Train Epoch: 2 [30720/251450]\tLoss: 0.0290 (0.0589) \tAcc: 96.88% (94.13%) \tEmb_Norm: 53.91 (54.49)\n",
      "Train Epoch: 2 [31360/251450]\tLoss: 0.0122 (0.0586) \tAcc: 96.88% (94.17%) \tEmb_Norm: 53.89 (54.48)\n",
      "Train Epoch: 2 [32000/251450]\tLoss: 0.0262 (0.0589) \tAcc: 93.75% (94.14%) \tEmb_Norm: 53.86 (54.47)\n",
      "Train Epoch: 2 [32640/251450]\tLoss: 0.0708 (0.0592) \tAcc: 90.62% (94.12%) \tEmb_Norm: 53.83 (54.46)\n",
      "Train Epoch: 2 [33280/251450]\tLoss: 0.0033 (0.0592) \tAcc: 100.00% (94.13%) \tEmb_Norm: 53.81 (54.44)\n",
      "Train Epoch: 2 [33920/251450]\tLoss: 0.0106 (0.0592) \tAcc: 96.88% (94.14%) \tEmb_Norm: 53.79 (54.43)\n",
      "Train Epoch: 2 [34560/251450]\tLoss: 0.1343 (0.0597) \tAcc: 93.75% (94.12%) \tEmb_Norm: 53.76 (54.42)\n",
      "Train Epoch: 2 [35200/251450]\tLoss: 0.0041 (0.0596) \tAcc: 100.00% (94.13%) \tEmb_Norm: 53.73 (54.41)\n",
      "Train Epoch: 2 [35840/251450]\tLoss: 0.0213 (0.0590) \tAcc: 93.75% (94.16%) \tEmb_Norm: 53.71 (54.40)\n",
      "Train Epoch: 2 [36480/251450]\tLoss: 0.0377 (0.0587) \tAcc: 90.62% (94.18%) \tEmb_Norm: 53.69 (54.38)\n",
      "Train Epoch: 2 [37120/251450]\tLoss: 0.0765 (0.0586) \tAcc: 90.62% (94.18%) \tEmb_Norm: 53.67 (54.37)\n",
      "Train Epoch: 2 [37760/251450]\tLoss: 0.0159 (0.0581) \tAcc: 96.88% (94.22%) \tEmb_Norm: 53.64 (54.36)\n",
      "Train Epoch: 2 [38400/251450]\tLoss: 0.0088 (0.0580) \tAcc: 96.88% (94.22%) \tEmb_Norm: 53.62 (54.35)\n",
      "Train Epoch: 2 [39040/251450]\tLoss: 0.0761 (0.0581) \tAcc: 93.75% (94.21%) \tEmb_Norm: 53.60 (54.34)\n",
      "Train Epoch: 2 [39680/251450]\tLoss: 0.0000 (0.0585) \tAcc: 100.00% (94.18%) \tEmb_Norm: 53.57 (54.32)\n",
      "Train Epoch: 2 [40320/251450]\tLoss: 0.0324 (0.0583) \tAcc: 96.88% (94.19%) \tEmb_Norm: 53.55 (54.31)\n",
      "Train Epoch: 2 [40960/251450]\tLoss: 0.0000 (0.0580) \tAcc: 100.00% (94.20%) \tEmb_Norm: 53.52 (54.30)\n",
      "Train Epoch: 2 [41600/251450]\tLoss: 0.0647 (0.0580) \tAcc: 90.62% (94.19%) \tEmb_Norm: 53.50 (54.29)\n",
      "Train Epoch: 2 [42240/251450]\tLoss: 0.1482 (0.0581) \tAcc: 87.50% (94.18%) \tEmb_Norm: 53.48 (54.27)\n",
      "Train Epoch: 2 [42880/251450]\tLoss: 0.0477 (0.0579) \tAcc: 93.75% (94.21%) \tEmb_Norm: 53.45 (54.26)\n",
      "Train Epoch: 2 [43520/251450]\tLoss: 0.1186 (0.0580) \tAcc: 90.62% (94.19%) \tEmb_Norm: 53.43 (54.25)\n",
      "Train Epoch: 2 [44160/251450]\tLoss: 0.1211 (0.0582) \tAcc: 84.38% (94.15%) \tEmb_Norm: 53.40 (54.24)\n",
      "Train Epoch: 2 [44800/251450]\tLoss: 0.1106 (0.0580) \tAcc: 90.62% (94.16%) \tEmb_Norm: 53.38 (54.23)\n",
      "Train Epoch: 2 [45440/251450]\tLoss: 0.0323 (0.0580) \tAcc: 93.75% (94.15%) \tEmb_Norm: 53.35 (54.21)\n",
      "Train Epoch: 2 [46080/251450]\tLoss: 0.0736 (0.0577) \tAcc: 87.50% (94.16%) \tEmb_Norm: 53.33 (54.20)\n",
      "Train Epoch: 2 [46720/251450]\tLoss: 0.1172 (0.0577) \tAcc: 93.75% (94.18%) \tEmb_Norm: 53.31 (54.19)\n",
      "Train Epoch: 2 [47360/251450]\tLoss: 0.0289 (0.0577) \tAcc: 93.75% (94.17%) \tEmb_Norm: 53.28 (54.18)\n",
      "Train Epoch: 2 [48000/251450]\tLoss: 0.1231 (0.0578) \tAcc: 90.62% (94.16%) \tEmb_Norm: 53.26 (54.17)\n",
      "Train Epoch: 2 [48640/251450]\tLoss: 0.1540 (0.0578) \tAcc: 90.62% (94.16%) \tEmb_Norm: 53.23 (54.15)\n",
      "Train Epoch: 2 [49280/251450]\tLoss: 0.0152 (0.0581) \tAcc: 96.88% (94.15%) \tEmb_Norm: 53.21 (54.14)\n",
      "Train Epoch: 2 [49920/251450]\tLoss: 0.0197 (0.0581) \tAcc: 96.88% (94.14%) \tEmb_Norm: 53.18 (54.13)\n",
      "Train Epoch: 2 [50560/251450]\tLoss: 0.1004 (0.0580) \tAcc: 93.75% (94.14%) \tEmb_Norm: 53.16 (54.12)\n",
      "Train Epoch: 2 [51200/251450]\tLoss: 0.0142 (0.0580) \tAcc: 96.88% (94.16%) \tEmb_Norm: 53.13 (54.11)\n",
      "Train Epoch: 2 [51840/251450]\tLoss: 0.0000 (0.0578) \tAcc: 100.00% (94.17%) \tEmb_Norm: 53.11 (54.09)\n",
      "Train Epoch: 2 [52480/251450]\tLoss: 0.0597 (0.0577) \tAcc: 96.88% (94.19%) \tEmb_Norm: 53.09 (54.08)\n",
      "Train Epoch: 2 [53120/251450]\tLoss: 0.0829 (0.0580) \tAcc: 93.75% (94.18%) \tEmb_Norm: 53.06 (54.07)\n",
      "Train Epoch: 2 [53760/251450]\tLoss: 0.1577 (0.0580) \tAcc: 81.25% (94.17%) \tEmb_Norm: 53.04 (54.06)\n",
      "Train Epoch: 2 [54400/251450]\tLoss: 0.0079 (0.0581) \tAcc: 100.00% (94.14%) \tEmb_Norm: 53.01 (54.04)\n",
      "Train Epoch: 2 [55040/251450]\tLoss: 0.0755 (0.0581) \tAcc: 96.88% (94.14%) \tEmb_Norm: 52.99 (54.03)\n",
      "Train Epoch: 2 [55680/251450]\tLoss: 0.1030 (0.0583) \tAcc: 93.75% (94.15%) \tEmb_Norm: 52.96 (54.02)\n",
      "Train Epoch: 2 [56320/251450]\tLoss: 0.0000 (0.0581) \tAcc: 100.00% (94.15%) \tEmb_Norm: 52.94 (54.01)\n",
      "Train Epoch: 2 [56960/251450]\tLoss: 0.0163 (0.0582) \tAcc: 96.88% (94.13%) \tEmb_Norm: 52.91 (54.00)\n",
      "Train Epoch: 2 [57600/251450]\tLoss: 0.0000 (0.0582) \tAcc: 100.00% (94.14%) \tEmb_Norm: 52.89 (53.98)\n",
      "Train Epoch: 2 [58240/251450]\tLoss: 0.0152 (0.0583) \tAcc: 96.88% (94.13%) \tEmb_Norm: 52.86 (53.97)\n",
      "Train Epoch: 2 [58880/251450]\tLoss: 0.0304 (0.0581) \tAcc: 90.62% (94.14%) \tEmb_Norm: 52.84 (53.96)\n",
      "Train Epoch: 2 [59520/251450]\tLoss: 0.0720 (0.0581) \tAcc: 93.75% (94.13%) \tEmb_Norm: 52.82 (53.95)\n",
      "Train Epoch: 2 [60160/251450]\tLoss: 0.0367 (0.0579) \tAcc: 96.88% (94.16%) \tEmb_Norm: 52.79 (53.93)\n",
      "Train Epoch: 2 [60800/251450]\tLoss: 0.0580 (0.0577) \tAcc: 90.62% (94.17%) \tEmb_Norm: 52.77 (53.92)\n",
      "Train Epoch: 2 [61440/251450]\tLoss: 0.0170 (0.0576) \tAcc: 96.88% (94.17%) \tEmb_Norm: 52.75 (53.91)\n",
      "Train Epoch: 2 [62080/251450]\tLoss: 0.0049 (0.0576) \tAcc: 100.00% (94.17%) \tEmb_Norm: 52.72 (53.90)\n",
      "Train Epoch: 2 [62720/251450]\tLoss: 0.0839 (0.0576) \tAcc: 93.75% (94.19%) \tEmb_Norm: 52.70 (53.89)\n",
      "Train Epoch: 2 [63360/251450]\tLoss: 0.2162 (0.0576) \tAcc: 84.38% (94.19%) \tEmb_Norm: 52.68 (53.87)\n",
      "Train Epoch: 2 [64000/251450]\tLoss: 0.0837 (0.0574) \tAcc: 93.75% (94.19%) \tEmb_Norm: 52.65 (53.86)\n",
      "Train Epoch: 2 [64640/251450]\tLoss: 0.0737 (0.0573) \tAcc: 93.75% (94.20%) \tEmb_Norm: 52.63 (53.85)\n",
      "Train Epoch: 2 [65280/251450]\tLoss: 0.0386 (0.0573) \tAcc: 93.75% (94.20%) \tEmb_Norm: 52.60 (53.84)\n",
      "Train Epoch: 2 [65920/251450]\tLoss: 0.0030 (0.0572) \tAcc: 100.00% (94.22%) \tEmb_Norm: 52.58 (53.83)\n",
      "Train Epoch: 2 [66560/251450]\tLoss: 0.0259 (0.0572) \tAcc: 96.88% (94.21%) \tEmb_Norm: 52.56 (53.81)\n",
      "Train Epoch: 2 [67200/251450]\tLoss: 0.0602 (0.0570) \tAcc: 90.62% (94.22%) \tEmb_Norm: 52.53 (53.80)\n",
      "Train Epoch: 2 [67840/251450]\tLoss: 0.0000 (0.0569) \tAcc: 100.00% (94.23%) \tEmb_Norm: 52.51 (53.79)\n",
      "Train Epoch: 2 [68480/251450]\tLoss: 0.0140 (0.0568) \tAcc: 96.88% (94.25%) \tEmb_Norm: 52.49 (53.78)\n",
      "Train Epoch: 2 [69120/251450]\tLoss: 0.0275 (0.0566) \tAcc: 96.88% (94.26%) \tEmb_Norm: 52.47 (53.77)\n",
      "Train Epoch: 2 [69760/251450]\tLoss: 0.1423 (0.0568) \tAcc: 84.38% (94.24%) \tEmb_Norm: 52.44 (53.75)\n",
      "Train Epoch: 2 [70400/251450]\tLoss: 0.0980 (0.0567) \tAcc: 87.50% (94.23%) \tEmb_Norm: 52.42 (53.74)\n",
      "Train Epoch: 2 [71040/251450]\tLoss: 0.0479 (0.0566) \tAcc: 90.62% (94.24%) \tEmb_Norm: 52.39 (53.73)\n",
      "Train Epoch: 2 [71680/251450]\tLoss: 0.1835 (0.0567) \tAcc: 84.38% (94.23%) \tEmb_Norm: 52.37 (53.72)\n",
      "Train Epoch: 2 [72320/251450]\tLoss: 0.0301 (0.0565) \tAcc: 93.75% (94.24%) \tEmb_Norm: 52.35 (53.71)\n",
      "Train Epoch: 2 [72960/251450]\tLoss: 0.0651 (0.0564) \tAcc: 93.75% (94.24%) \tEmb_Norm: 52.32 (53.69)\n",
      "Train Epoch: 2 [73600/251450]\tLoss: 0.0277 (0.0563) \tAcc: 96.88% (94.25%) \tEmb_Norm: 52.30 (53.68)\n",
      "Train Epoch: 2 [74240/251450]\tLoss: 0.0455 (0.0562) \tAcc: 93.75% (94.25%) \tEmb_Norm: 52.28 (53.67)\n",
      "Train Epoch: 2 [74880/251450]\tLoss: 0.0057 (0.0564) \tAcc: 100.00% (94.23%) \tEmb_Norm: 52.25 (53.66)\n",
      "Train Epoch: 2 [75520/251450]\tLoss: 0.0000 (0.0563) \tAcc: 100.00% (94.24%) \tEmb_Norm: 52.23 (53.65)\n",
      "Train Epoch: 2 [76160/251450]\tLoss: 0.0521 (0.0564) \tAcc: 96.88% (94.24%) \tEmb_Norm: 52.20 (53.63)\n",
      "Train Epoch: 2 [76800/251450]\tLoss: 0.0552 (0.0564) \tAcc: 93.75% (94.22%) \tEmb_Norm: 52.18 (53.62)\n",
      "Train Epoch: 2 [77440/251450]\tLoss: 0.0227 (0.0562) \tAcc: 96.88% (94.23%) \tEmb_Norm: 52.16 (53.61)\n",
      "Train Epoch: 2 [78080/251450]\tLoss: 0.1852 (0.0565) \tAcc: 87.50% (94.21%) \tEmb_Norm: 52.13 (53.60)\n",
      "Train Epoch: 2 [78720/251450]\tLoss: 0.0257 (0.0565) \tAcc: 93.75% (94.21%) \tEmb_Norm: 52.10 (53.59)\n",
      "Train Epoch: 2 [79360/251450]\tLoss: 0.0081 (0.0565) \tAcc: 96.88% (94.20%) \tEmb_Norm: 52.08 (53.57)\n",
      "Train Epoch: 2 [80000/251450]\tLoss: 0.0405 (0.0564) \tAcc: 93.75% (94.21%) \tEmb_Norm: 52.06 (53.56)\n",
      "Train Epoch: 2 [80640/251450]\tLoss: 0.0625 (0.0564) \tAcc: 93.75% (94.21%) \tEmb_Norm: 52.03 (53.55)\n",
      "Train Epoch: 2 [81280/251450]\tLoss: 0.0728 (0.0563) \tAcc: 93.75% (94.23%) \tEmb_Norm: 52.01 (53.54)\n",
      "Train Epoch: 2 [81920/251450]\tLoss: 0.0000 (0.0562) \tAcc: 100.00% (94.24%) \tEmb_Norm: 51.99 (53.52)\n",
      "Train Epoch: 2 [82560/251450]\tLoss: 0.0373 (0.0564) \tAcc: 96.88% (94.23%) \tEmb_Norm: 51.96 (53.51)\n",
      "Train Epoch: 2 [83200/251450]\tLoss: 0.0777 (0.0564) \tAcc: 93.75% (94.23%) \tEmb_Norm: 51.93 (53.50)\n",
      "Train Epoch: 2 [83840/251450]\tLoss: 0.0073 (0.0564) \tAcc: 100.00% (94.23%) \tEmb_Norm: 51.91 (53.49)\n",
      "Train Epoch: 2 [84480/251450]\tLoss: 0.1076 (0.0565) \tAcc: 90.62% (94.22%) \tEmb_Norm: 51.89 (53.48)\n",
      "Train Epoch: 2 [85120/251450]\tLoss: 0.0492 (0.0564) \tAcc: 93.75% (94.24%) \tEmb_Norm: 51.86 (53.46)\n",
      "Train Epoch: 2 [85760/251450]\tLoss: 0.0249 (0.0564) \tAcc: 93.75% (94.24%) \tEmb_Norm: 51.84 (53.45)\n",
      "Train Epoch: 2 [86400/251450]\tLoss: 0.0132 (0.0563) \tAcc: 96.88% (94.24%) \tEmb_Norm: 51.81 (53.44)\n",
      "Train Epoch: 2 [87040/251450]\tLoss: 0.0000 (0.0563) \tAcc: 100.00% (94.25%) \tEmb_Norm: 51.79 (53.43)\n",
      "Train Epoch: 2 [87680/251450]\tLoss: 0.0116 (0.0562) \tAcc: 96.88% (94.26%) \tEmb_Norm: 51.77 (53.42)\n",
      "Train Epoch: 2 [88320/251450]\tLoss: 0.0000 (0.0561) \tAcc: 100.00% (94.27%) \tEmb_Norm: 51.74 (53.40)\n",
      "Train Epoch: 2 [88960/251450]\tLoss: 0.0042 (0.0561) \tAcc: 100.00% (94.27%) \tEmb_Norm: 51.72 (53.39)\n",
      "Train Epoch: 2 [89600/251450]\tLoss: 0.0185 (0.0560) \tAcc: 93.75% (94.28%) \tEmb_Norm: 51.70 (53.38)\n",
      "Train Epoch: 2 [90240/251450]\tLoss: 0.0495 (0.0559) \tAcc: 96.88% (94.28%) \tEmb_Norm: 51.67 (53.37)\n",
      "Train Epoch: 2 [90880/251450]\tLoss: 0.0000 (0.0559) \tAcc: 100.00% (94.27%) \tEmb_Norm: 51.65 (53.36)\n",
      "Train Epoch: 2 [91520/251450]\tLoss: 0.0170 (0.0558) \tAcc: 93.75% (94.27%) \tEmb_Norm: 51.63 (53.34)\n",
      "Train Epoch: 2 [92160/251450]\tLoss: 0.1537 (0.0559) \tAcc: 93.75% (94.27%) \tEmb_Norm: 51.60 (53.33)\n",
      "Train Epoch: 2 [92800/251450]\tLoss: 0.0027 (0.0558) \tAcc: 100.00% (94.27%) \tEmb_Norm: 51.58 (53.32)\n",
      "Train Epoch: 2 [93440/251450]\tLoss: 0.0858 (0.0560) \tAcc: 90.62% (94.27%) \tEmb_Norm: 51.55 (53.31)\n",
      "Train Epoch: 2 [94080/251450]\tLoss: 0.0050 (0.0559) \tAcc: 100.00% (94.27%) \tEmb_Norm: 51.53 (53.30)\n",
      "Train Epoch: 2 [94720/251450]\tLoss: 0.0771 (0.0559) \tAcc: 93.75% (94.27%) \tEmb_Norm: 51.50 (53.28)\n",
      "Train Epoch: 2 [95360/251450]\tLoss: 0.0531 (0.0560) \tAcc: 93.75% (94.27%) \tEmb_Norm: 51.48 (53.27)\n",
      "Train Epoch: 2 [96000/251450]\tLoss: 0.0239 (0.0559) \tAcc: 96.88% (94.26%) \tEmb_Norm: 51.46 (53.26)\n",
      "Train Epoch: 2 [96640/251450]\tLoss: 0.0065 (0.0559) \tAcc: 96.88% (94.26%) \tEmb_Norm: 51.43 (53.25)\n",
      "Train Epoch: 2 [97280/251450]\tLoss: 0.0128 (0.0558) \tAcc: 96.88% (94.26%) \tEmb_Norm: 51.41 (53.24)\n",
      "Train Epoch: 2 [97920/251450]\tLoss: 0.0625 (0.0556) \tAcc: 90.62% (94.28%) \tEmb_Norm: 51.39 (53.22)\n",
      "Train Epoch: 2 [98560/251450]\tLoss: 0.0000 (0.0555) \tAcc: 100.00% (94.29%) \tEmb_Norm: 51.36 (53.21)\n",
      "Train Epoch: 2 [99200/251450]\tLoss: 0.1203 (0.0555) \tAcc: 93.75% (94.30%) \tEmb_Norm: 51.34 (53.20)\n",
      "Train Epoch: 2 [99840/251450]\tLoss: 0.0715 (0.0555) \tAcc: 90.62% (94.30%) \tEmb_Norm: 51.32 (53.19)\n",
      "Train Epoch: 2 [100480/251450]\tLoss: 0.4141 (0.0556) \tAcc: 81.25% (94.30%) \tEmb_Norm: 51.29 (53.18)\n",
      "Train Epoch: 2 [101120/251450]\tLoss: 0.0867 (0.0554) \tAcc: 90.62% (94.31%) \tEmb_Norm: 51.27 (53.16)\n",
      "Train Epoch: 2 [101760/251450]\tLoss: 0.0134 (0.0554) \tAcc: 96.88% (94.31%) \tEmb_Norm: 51.24 (53.15)\n",
      "Train Epoch: 2 [102400/251450]\tLoss: 0.0000 (0.0554) \tAcc: 100.00% (94.32%) \tEmb_Norm: 51.22 (53.14)\n",
      "Train Epoch: 2 [103040/251450]\tLoss: 0.0000 (0.0553) \tAcc: 100.00% (94.32%) \tEmb_Norm: 51.20 (53.13)\n",
      "Train Epoch: 2 [103680/251450]\tLoss: 0.0159 (0.0553) \tAcc: 96.88% (94.32%) \tEmb_Norm: 51.17 (53.12)\n",
      "Train Epoch: 2 [104320/251450]\tLoss: 0.0142 (0.0551) \tAcc: 96.88% (94.33%) \tEmb_Norm: 51.15 (53.10)\n",
      "Train Epoch: 2 [104960/251450]\tLoss: 0.0101 (0.0550) \tAcc: 100.00% (94.33%) \tEmb_Norm: 51.13 (53.09)\n",
      "Train Epoch: 2 [105600/251450]\tLoss: 0.1646 (0.0552) \tAcc: 81.25% (94.33%) \tEmb_Norm: 51.10 (53.08)\n",
      "Train Epoch: 2 [106240/251450]\tLoss: 0.0000 (0.0551) \tAcc: 100.00% (94.34%) \tEmb_Norm: 51.08 (53.07)\n",
      "Train Epoch: 2 [106880/251450]\tLoss: 0.0173 (0.0550) \tAcc: 96.88% (94.34%) \tEmb_Norm: 51.06 (53.06)\n",
      "Train Epoch: 2 [107520/251450]\tLoss: 0.0968 (0.0550) \tAcc: 93.75% (94.35%) \tEmb_Norm: 51.03 (53.04)\n",
      "Train Epoch: 2 [108160/251450]\tLoss: 0.0000 (0.0550) \tAcc: 100.00% (94.34%) \tEmb_Norm: 51.01 (53.03)\n",
      "Train Epoch: 2 [108800/251450]\tLoss: 0.2567 (0.0550) \tAcc: 84.38% (94.34%) \tEmb_Norm: 50.98 (53.02)\n",
      "Train Epoch: 2 [109440/251450]\tLoss: 0.0167 (0.0550) \tAcc: 96.88% (94.33%) \tEmb_Norm: 50.96 (53.01)\n",
      "Train Epoch: 2 [110080/251450]\tLoss: 0.0112 (0.0550) \tAcc: 96.88% (94.33%) \tEmb_Norm: 50.94 (53.00)\n",
      "Train Epoch: 2 [110720/251450]\tLoss: 0.0471 (0.0550) \tAcc: 93.75% (94.33%) \tEmb_Norm: 50.91 (52.98)\n",
      "Train Epoch: 2 [111360/251450]\tLoss: 0.0377 (0.0550) \tAcc: 96.88% (94.32%) \tEmb_Norm: 50.89 (52.97)\n",
      "Train Epoch: 2 [112000/251450]\tLoss: 0.0311 (0.0550) \tAcc: 93.75% (94.32%) \tEmb_Norm: 50.86 (52.96)\n",
      "Train Epoch: 2 [112640/251450]\tLoss: 0.0031 (0.0549) \tAcc: 100.00% (94.32%) \tEmb_Norm: 50.84 (52.95)\n",
      "Train Epoch: 2 [113280/251450]\tLoss: 0.0769 (0.0549) \tAcc: 90.62% (94.32%) \tEmb_Norm: 50.82 (52.94)\n",
      "Train Epoch: 2 [113920/251450]\tLoss: 0.0621 (0.0549) \tAcc: 90.62% (94.32%) \tEmb_Norm: 50.79 (52.92)\n",
      "Train Epoch: 2 [114560/251450]\tLoss: 0.0634 (0.0549) \tAcc: 90.62% (94.31%) \tEmb_Norm: 50.77 (52.91)\n",
      "Train Epoch: 2 [115200/251450]\tLoss: 0.0511 (0.0549) \tAcc: 93.75% (94.32%) \tEmb_Norm: 50.74 (52.90)\n",
      "Train Epoch: 2 [115840/251450]\tLoss: 0.0285 (0.0549) \tAcc: 96.88% (94.32%) \tEmb_Norm: 50.72 (52.89)\n",
      "Train Epoch: 2 [116480/251450]\tLoss: 0.0964 (0.0549) \tAcc: 90.62% (94.32%) \tEmb_Norm: 50.70 (52.88)\n",
      "Train Epoch: 2 [117120/251450]\tLoss: 0.0090 (0.0548) \tAcc: 96.88% (94.33%) \tEmb_Norm: 50.67 (52.86)\n",
      "Train Epoch: 2 [117760/251450]\tLoss: 0.0912 (0.0548) \tAcc: 93.75% (94.33%) \tEmb_Norm: 50.65 (52.85)\n",
      "Train Epoch: 2 [118400/251450]\tLoss: 0.1066 (0.0548) \tAcc: 90.62% (94.33%) \tEmb_Norm: 50.62 (52.84)\n",
      "Train Epoch: 2 [119040/251450]\tLoss: 0.0000 (0.0548) \tAcc: 100.00% (94.33%) \tEmb_Norm: 50.60 (52.83)\n",
      "Train Epoch: 2 [119680/251450]\tLoss: 0.0000 (0.0547) \tAcc: 100.00% (94.34%) \tEmb_Norm: 50.58 (52.82)\n",
      "Train Epoch: 2 [120320/251450]\tLoss: 0.0689 (0.0548) \tAcc: 96.88% (94.34%) \tEmb_Norm: 50.55 (52.80)\n",
      "Train Epoch: 2 [120960/251450]\tLoss: 0.0265 (0.0548) \tAcc: 96.88% (94.34%) \tEmb_Norm: 50.53 (52.79)\n",
      "Train Epoch: 2 [121600/251450]\tLoss: 0.0140 (0.0548) \tAcc: 96.88% (94.34%) \tEmb_Norm: 50.50 (52.78)\n",
      "Train Epoch: 2 [122240/251450]\tLoss: 0.0000 (0.0547) \tAcc: 100.00% (94.34%) \tEmb_Norm: 50.48 (52.77)\n",
      "Train Epoch: 2 [122880/251450]\tLoss: 0.0338 (0.0548) \tAcc: 96.88% (94.33%) \tEmb_Norm: 50.45 (52.76)\n",
      "Train Epoch: 2 [123520/251450]\tLoss: 0.1178 (0.0549) \tAcc: 90.62% (94.33%) \tEmb_Norm: 50.43 (52.74)\n",
      "Train Epoch: 2 [124160/251450]\tLoss: 0.0107 (0.0547) \tAcc: 96.88% (94.34%) \tEmb_Norm: 50.41 (52.73)\n",
      "Train Epoch: 2 [124800/251450]\tLoss: 0.0372 (0.0547) \tAcc: 93.75% (94.34%) \tEmb_Norm: 50.38 (52.72)\n",
      "Train Epoch: 2 [125440/251450]\tLoss: 0.0720 (0.0547) \tAcc: 87.50% (94.34%) \tEmb_Norm: 50.36 (52.71)\n",
      "Train Epoch: 2 [126080/251450]\tLoss: 0.0000 (0.0546) \tAcc: 100.00% (94.35%) \tEmb_Norm: 50.34 (52.70)\n",
      "Train Epoch: 2 [126720/251450]\tLoss: 0.0000 (0.0546) \tAcc: 100.00% (94.35%) \tEmb_Norm: 50.31 (52.68)\n",
      "Train Epoch: 2 [127360/251450]\tLoss: 0.1837 (0.0547) \tAcc: 78.12% (94.35%) \tEmb_Norm: 50.28 (52.67)\n",
      "Train Epoch: 2 [128000/251450]\tLoss: 0.0525 (0.0548) \tAcc: 93.75% (94.35%) \tEmb_Norm: 50.26 (52.66)\n",
      "Train Epoch: 2 [128640/251450]\tLoss: 0.1361 (0.0549) \tAcc: 93.75% (94.34%) \tEmb_Norm: 50.23 (52.65)\n",
      "Train Epoch: 2 [129280/251450]\tLoss: 0.1147 (0.0550) \tAcc: 87.50% (94.34%) \tEmb_Norm: 50.21 (52.64)\n",
      "Train Epoch: 2 [129920/251450]\tLoss: 0.0600 (0.0549) \tAcc: 93.75% (94.34%) \tEmb_Norm: 50.18 (52.62)\n",
      "Train Epoch: 2 [130560/251450]\tLoss: 0.0186 (0.0549) \tAcc: 96.88% (94.34%) \tEmb_Norm: 50.16 (52.61)\n",
      "Train Epoch: 2 [131200/251450]\tLoss: 0.0668 (0.0548) \tAcc: 93.75% (94.34%) \tEmb_Norm: 50.14 (52.60)\n",
      "Train Epoch: 2 [131840/251450]\tLoss: 0.0385 (0.0547) \tAcc: 96.88% (94.34%) \tEmb_Norm: 50.11 (52.59)\n",
      "Train Epoch: 2 [132480/251450]\tLoss: 0.0044 (0.0548) \tAcc: 100.00% (94.34%) \tEmb_Norm: 50.09 (52.58)\n",
      "Train Epoch: 2 [133120/251450]\tLoss: 0.0453 (0.0548) \tAcc: 93.75% (94.34%) \tEmb_Norm: 50.06 (52.56)\n",
      "Train Epoch: 2 [133760/251450]\tLoss: 0.0187 (0.0547) \tAcc: 96.88% (94.35%) \tEmb_Norm: 50.04 (52.55)\n",
      "Train Epoch: 2 [134400/251450]\tLoss: 0.0064 (0.0546) \tAcc: 96.88% (94.35%) \tEmb_Norm: 50.02 (52.54)\n",
      "Train Epoch: 2 [135040/251450]\tLoss: 0.0002 (0.0546) \tAcc: 100.00% (94.35%) \tEmb_Norm: 50.00 (52.53)\n",
      "Train Epoch: 2 [135680/251450]\tLoss: 0.0833 (0.0546) \tAcc: 87.50% (94.35%) \tEmb_Norm: 49.97 (52.52)\n",
      "Train Epoch: 2 [136320/251450]\tLoss: 0.0175 (0.0546) \tAcc: 96.88% (94.35%) \tEmb_Norm: 49.95 (52.50)\n",
      "Train Epoch: 2 [136960/251450]\tLoss: 0.1267 (0.0547) \tAcc: 93.75% (94.35%) \tEmb_Norm: 49.92 (52.49)\n",
      "Train Epoch: 2 [137600/251450]\tLoss: 0.0321 (0.0546) \tAcc: 96.88% (94.35%) \tEmb_Norm: 49.90 (52.48)\n",
      "Train Epoch: 2 [138240/251450]\tLoss: 0.1046 (0.0546) \tAcc: 90.62% (94.35%) \tEmb_Norm: 49.87 (52.47)\n",
      "Train Epoch: 2 [138880/251450]\tLoss: 0.0661 (0.0546) \tAcc: 93.75% (94.35%) \tEmb_Norm: 49.85 (52.46)\n",
      "Train Epoch: 2 [139520/251450]\tLoss: 0.0300 (0.0545) \tAcc: 96.88% (94.36%) \tEmb_Norm: 49.83 (52.44)\n",
      "Train Epoch: 2 [140160/251450]\tLoss: 0.0200 (0.0545) \tAcc: 96.88% (94.36%) \tEmb_Norm: 49.80 (52.43)\n",
      "Train Epoch: 2 [140800/251450]\tLoss: 0.0937 (0.0545) \tAcc: 93.75% (94.37%) \tEmb_Norm: 49.78 (52.42)\n",
      "Train Epoch: 2 [141440/251450]\tLoss: 0.0762 (0.0545) \tAcc: 96.88% (94.36%) \tEmb_Norm: 49.75 (52.41)\n",
      "Train Epoch: 2 [142080/251450]\tLoss: 0.0852 (0.0545) \tAcc: 96.88% (94.36%) \tEmb_Norm: 49.73 (52.40)\n",
      "Train Epoch: 2 [142720/251450]\tLoss: 0.0457 (0.0545) \tAcc: 93.75% (94.37%) \tEmb_Norm: 49.71 (52.38)\n",
      "Train Epoch: 2 [143360/251450]\tLoss: 0.0231 (0.0545) \tAcc: 96.88% (94.36%) \tEmb_Norm: 49.68 (52.37)\n",
      "Train Epoch: 2 [144000/251450]\tLoss: 0.0178 (0.0544) \tAcc: 96.88% (94.37%) \tEmb_Norm: 49.66 (52.36)\n",
      "Train Epoch: 2 [144640/251450]\tLoss: 0.0243 (0.0543) \tAcc: 93.75% (94.37%) \tEmb_Norm: 49.64 (52.35)\n",
      "Train Epoch: 2 [145280/251450]\tLoss: 0.0312 (0.0543) \tAcc: 93.75% (94.38%) \tEmb_Norm: 49.61 (52.34)\n",
      "Train Epoch: 2 [145920/251450]\tLoss: 0.0000 (0.0543) \tAcc: 100.00% (94.38%) \tEmb_Norm: 49.59 (52.32)\n",
      "Train Epoch: 2 [146560/251450]\tLoss: 0.0059 (0.0543) \tAcc: 100.00% (94.38%) \tEmb_Norm: 49.56 (52.31)\n",
      "Train Epoch: 2 [147200/251450]\tLoss: 0.0656 (0.0542) \tAcc: 90.62% (94.37%) \tEmb_Norm: 49.54 (52.30)\n",
      "Train Epoch: 2 [147840/251450]\tLoss: 0.0269 (0.0541) \tAcc: 96.88% (94.38%) \tEmb_Norm: 49.52 (52.29)\n",
      "Train Epoch: 2 [148480/251450]\tLoss: 0.0050 (0.0541) \tAcc: 100.00% (94.38%) \tEmb_Norm: 49.50 (52.28)\n",
      "Train Epoch: 2 [149120/251450]\tLoss: 0.0749 (0.0541) \tAcc: 90.62% (94.38%) \tEmb_Norm: 49.47 (52.26)\n",
      "Train Epoch: 2 [149760/251450]\tLoss: 0.0395 (0.0540) \tAcc: 90.62% (94.38%) \tEmb_Norm: 49.45 (52.25)\n",
      "Train Epoch: 2 [150400/251450]\tLoss: 0.0000 (0.0540) \tAcc: 100.00% (94.38%) \tEmb_Norm: 49.43 (52.24)\n",
      "Train Epoch: 2 [151040/251450]\tLoss: 0.0074 (0.0539) \tAcc: 96.88% (94.39%) \tEmb_Norm: 49.40 (52.23)\n",
      "Train Epoch: 2 [151680/251450]\tLoss: 0.0697 (0.0539) \tAcc: 93.75% (94.39%) \tEmb_Norm: 49.38 (52.22)\n",
      "Train Epoch: 2 [152320/251450]\tLoss: 0.0539 (0.0539) \tAcc: 93.75% (94.39%) \tEmb_Norm: 49.36 (52.20)\n",
      "Train Epoch: 2 [152960/251450]\tLoss: 0.0461 (0.0539) \tAcc: 93.75% (94.40%) \tEmb_Norm: 49.33 (52.19)\n",
      "Train Epoch: 2 [153600/251450]\tLoss: 0.0088 (0.0538) \tAcc: 96.88% (94.40%) \tEmb_Norm: 49.31 (52.18)\n",
      "Train Epoch: 2 [154240/251450]\tLoss: 0.0032 (0.0538) \tAcc: 100.00% (94.40%) \tEmb_Norm: 49.28 (52.17)\n",
      "Train Epoch: 2 [154880/251450]\tLoss: 0.0299 (0.0538) \tAcc: 96.88% (94.40%) \tEmb_Norm: 49.26 (52.16)\n",
      "Train Epoch: 2 [155520/251450]\tLoss: 0.0816 (0.0537) \tAcc: 84.38% (94.40%) \tEmb_Norm: 49.24 (52.14)\n",
      "Train Epoch: 2 [156160/251450]\tLoss: 0.0073 (0.0538) \tAcc: 96.88% (94.40%) \tEmb_Norm: 49.21 (52.13)\n",
      "Train Epoch: 2 [156800/251450]\tLoss: 0.0000 (0.0537) \tAcc: 100.00% (94.40%) \tEmb_Norm: 49.19 (52.12)\n",
      "Train Epoch: 2 [157440/251450]\tLoss: 0.0383 (0.0537) \tAcc: 96.88% (94.40%) \tEmb_Norm: 49.17 (52.11)\n",
      "Train Epoch: 2 [158080/251450]\tLoss: 0.0054 (0.0537) \tAcc: 100.00% (94.40%) \tEmb_Norm: 49.14 (52.10)\n",
      "Train Epoch: 2 [158720/251450]\tLoss: 0.0794 (0.0537) \tAcc: 90.62% (94.40%) \tEmb_Norm: 49.12 (52.08)\n",
      "Train Epoch: 2 [159360/251450]\tLoss: 0.0329 (0.0537) \tAcc: 93.75% (94.40%) \tEmb_Norm: 49.09 (52.07)\n",
      "Train Epoch: 2 [160000/251450]\tLoss: 0.0000 (0.0537) \tAcc: 100.00% (94.40%) \tEmb_Norm: 49.07 (52.06)\n",
      "Train Epoch: 2 [160640/251450]\tLoss: 0.0085 (0.0537) \tAcc: 96.88% (94.40%) \tEmb_Norm: 49.04 (52.05)\n",
      "Train Epoch: 2 [161280/251450]\tLoss: 0.0111 (0.0537) \tAcc: 96.88% (94.39%) \tEmb_Norm: 49.02 (52.04)\n",
      "Train Epoch: 2 [161920/251450]\tLoss: 0.0105 (0.0537) \tAcc: 96.88% (94.40%) \tEmb_Norm: 49.00 (52.02)\n",
      "Train Epoch: 2 [162560/251450]\tLoss: 0.0712 (0.0536) \tAcc: 90.62% (94.39%) \tEmb_Norm: 48.98 (52.01)\n",
      "Train Epoch: 2 [163200/251450]\tLoss: 0.0243 (0.0536) \tAcc: 96.88% (94.40%) \tEmb_Norm: 48.95 (52.00)\n",
      "Train Epoch: 2 [163840/251450]\tLoss: 0.1436 (0.0535) \tAcc: 87.50% (94.40%) \tEmb_Norm: 48.93 (51.99)\n",
      "Train Epoch: 2 [164480/251450]\tLoss: 0.0158 (0.0534) \tAcc: 93.75% (94.41%) \tEmb_Norm: 48.91 (51.98)\n",
      "Train Epoch: 2 [165120/251450]\tLoss: 0.0383 (0.0534) \tAcc: 96.88% (94.41%) \tEmb_Norm: 48.89 (51.97)\n",
      "Train Epoch: 2 [165760/251450]\tLoss: 0.0625 (0.0534) \tAcc: 90.62% (94.41%) \tEmb_Norm: 48.86 (51.95)\n",
      "Train Epoch: 2 [166400/251450]\tLoss: 0.0308 (0.0534) \tAcc: 96.88% (94.41%) \tEmb_Norm: 48.84 (51.94)\n",
      "Train Epoch: 2 [167040/251450]\tLoss: 0.0118 (0.0533) \tAcc: 96.88% (94.41%) \tEmb_Norm: 48.81 (51.93)\n",
      "Train Epoch: 2 [167680/251450]\tLoss: 0.0311 (0.0533) \tAcc: 93.75% (94.41%) \tEmb_Norm: 48.79 (51.92)\n",
      "Train Epoch: 2 [168320/251450]\tLoss: 0.0172 (0.0533) \tAcc: 96.88% (94.41%) \tEmb_Norm: 48.77 (51.91)\n",
      "Train Epoch: 2 [168960/251450]\tLoss: 0.0458 (0.0532) \tAcc: 93.75% (94.42%) \tEmb_Norm: 48.74 (51.89)\n",
      "Train Epoch: 2 [169600/251450]\tLoss: 0.0219 (0.0531) \tAcc: 96.88% (94.42%) \tEmb_Norm: 48.72 (51.88)\n",
      "Train Epoch: 2 [170240/251450]\tLoss: 0.0216 (0.0531) \tAcc: 96.88% (94.43%) \tEmb_Norm: 48.70 (51.87)\n",
      "Train Epoch: 2 [170880/251450]\tLoss: 0.1182 (0.0531) \tAcc: 96.88% (94.43%) \tEmb_Norm: 48.67 (51.86)\n",
      "Train Epoch: 2 [171520/251450]\tLoss: 0.0657 (0.0531) \tAcc: 93.75% (94.43%) \tEmb_Norm: 48.65 (51.85)\n",
      "Train Epoch: 2 [172160/251450]\tLoss: 0.0181 (0.0530) \tAcc: 96.88% (94.44%) \tEmb_Norm: 48.63 (51.83)\n",
      "Train Epoch: 2 [172800/251450]\tLoss: 0.0187 (0.0529) \tAcc: 96.88% (94.44%) \tEmb_Norm: 48.61 (51.82)\n",
      "Train Epoch: 2 [173440/251450]\tLoss: 0.0256 (0.0529) \tAcc: 93.75% (94.44%) \tEmb_Norm: 48.58 (51.81)\n",
      "Train Epoch: 2 [174080/251450]\tLoss: 0.0000 (0.0528) \tAcc: 100.00% (94.45%) \tEmb_Norm: 48.56 (51.80)\n",
      "Train Epoch: 2 [174720/251450]\tLoss: 0.0354 (0.0528) \tAcc: 93.75% (94.45%) \tEmb_Norm: 48.54 (51.79)\n",
      "Train Epoch: 2 [175360/251450]\tLoss: 0.0000 (0.0527) \tAcc: 100.00% (94.46%) \tEmb_Norm: 48.51 (51.77)\n",
      "Train Epoch: 2 [176000/251450]\tLoss: 0.2557 (0.0527) \tAcc: 78.12% (94.46%) \tEmb_Norm: 48.49 (51.76)\n",
      "Train Epoch: 2 [176640/251450]\tLoss: 0.0000 (0.0527) \tAcc: 100.00% (94.46%) \tEmb_Norm: 48.47 (51.75)\n",
      "Train Epoch: 2 [177280/251450]\tLoss: 0.0492 (0.0526) \tAcc: 93.75% (94.46%) \tEmb_Norm: 48.45 (51.74)\n",
      "Train Epoch: 2 [177920/251450]\tLoss: 0.0563 (0.0526) \tAcc: 93.75% (94.47%) \tEmb_Norm: 48.42 (51.73)\n",
      "Train Epoch: 2 [178560/251450]\tLoss: 0.0175 (0.0526) \tAcc: 96.88% (94.47%) \tEmb_Norm: 48.40 (51.71)\n",
      "Train Epoch: 2 [179200/251450]\tLoss: 0.0573 (0.0526) \tAcc: 84.38% (94.46%) \tEmb_Norm: 48.37 (51.70)\n",
      "Train Epoch: 2 [179840/251450]\tLoss: 0.0581 (0.0525) \tAcc: 93.75% (94.47%) \tEmb_Norm: 48.35 (51.69)\n",
      "Train Epoch: 2 [180480/251450]\tLoss: 0.0126 (0.0525) \tAcc: 96.88% (94.47%) \tEmb_Norm: 48.33 (51.68)\n",
      "Train Epoch: 2 [181120/251450]\tLoss: 0.0000 (0.0525) \tAcc: 100.00% (94.47%) \tEmb_Norm: 48.30 (51.67)\n",
      "Train Epoch: 2 [181760/251450]\tLoss: 0.0090 (0.0525) \tAcc: 96.88% (94.47%) \tEmb_Norm: 48.28 (51.66)\n",
      "Train Epoch: 2 [182400/251450]\tLoss: 0.0399 (0.0525) \tAcc: 96.88% (94.47%) \tEmb_Norm: 48.26 (51.64)\n",
      "Train Epoch: 2 [183040/251450]\tLoss: 0.0000 (0.0525) \tAcc: 100.00% (94.47%) \tEmb_Norm: 48.23 (51.63)\n",
      "Train Epoch: 2 [183680/251450]\tLoss: 0.0059 (0.0524) \tAcc: 100.00% (94.47%) \tEmb_Norm: 48.21 (51.62)\n",
      "Train Epoch: 2 [184320/251450]\tLoss: 0.1322 (0.0524) \tAcc: 84.38% (94.47%) \tEmb_Norm: 48.19 (51.61)\n",
      "Train Epoch: 2 [184960/251450]\tLoss: 0.0015 (0.0524) \tAcc: 100.00% (94.48%) \tEmb_Norm: 48.16 (51.60)\n",
      "Train Epoch: 2 [185600/251450]\tLoss: 0.0315 (0.0524) \tAcc: 96.88% (94.47%) \tEmb_Norm: 48.14 (51.58)\n",
      "Train Epoch: 2 [186240/251450]\tLoss: 0.0314 (0.0524) \tAcc: 96.88% (94.47%) \tEmb_Norm: 48.11 (51.57)\n",
      "Train Epoch: 2 [186880/251450]\tLoss: 0.0000 (0.0523) \tAcc: 100.00% (94.48%) \tEmb_Norm: 48.09 (51.56)\n",
      "Train Epoch: 2 [187520/251450]\tLoss: 0.0697 (0.0523) \tAcc: 90.62% (94.47%) \tEmb_Norm: 48.07 (51.55)\n",
      "Train Epoch: 2 [188160/251450]\tLoss: 0.0000 (0.0523) \tAcc: 100.00% (94.47%) \tEmb_Norm: 48.04 (51.54)\n",
      "Train Epoch: 2 [188800/251450]\tLoss: 0.0488 (0.0523) \tAcc: 96.88% (94.47%) \tEmb_Norm: 48.02 (51.52)\n",
      "Train Epoch: 2 [189440/251450]\tLoss: 0.0848 (0.0523) \tAcc: 87.50% (94.47%) \tEmb_Norm: 48.00 (51.51)\n",
      "Train Epoch: 2 [190080/251450]\tLoss: 0.0245 (0.0522) \tAcc: 96.88% (94.48%) \tEmb_Norm: 47.98 (51.50)\n",
      "Train Epoch: 2 [190720/251450]\tLoss: 0.0492 (0.0522) \tAcc: 90.62% (94.47%) \tEmb_Norm: 47.95 (51.49)\n",
      "Train Epoch: 2 [191360/251450]\tLoss: 0.0072 (0.0522) \tAcc: 96.88% (94.47%) \tEmb_Norm: 47.93 (51.48)\n",
      "Train Epoch: 2 [192000/251450]\tLoss: 0.0633 (0.0522) \tAcc: 93.75% (94.48%) \tEmb_Norm: 47.90 (51.47)\n",
      "Train Epoch: 2 [192640/251450]\tLoss: 0.0680 (0.0521) \tAcc: 93.75% (94.48%) \tEmb_Norm: 47.88 (51.45)\n",
      "Train Epoch: 2 [193280/251450]\tLoss: 0.0278 (0.0521) \tAcc: 96.88% (94.48%) \tEmb_Norm: 47.86 (51.44)\n",
      "Train Epoch: 2 [193920/251450]\tLoss: 0.0263 (0.0520) \tAcc: 93.75% (94.48%) \tEmb_Norm: 47.84 (51.43)\n",
      "Train Epoch: 2 [194560/251450]\tLoss: 0.0106 (0.0520) \tAcc: 96.88% (94.47%) \tEmb_Norm: 47.81 (51.42)\n",
      "Train Epoch: 2 [195200/251450]\tLoss: 0.0173 (0.0520) \tAcc: 96.88% (94.48%) \tEmb_Norm: 47.79 (51.41)\n",
      "Train Epoch: 2 [195840/251450]\tLoss: 0.0754 (0.0520) \tAcc: 90.62% (94.47%) \tEmb_Norm: 47.76 (51.39)\n",
      "Train Epoch: 2 [196480/251450]\tLoss: 0.0097 (0.0520) \tAcc: 96.88% (94.48%) \tEmb_Norm: 47.74 (51.38)\n",
      "Train Epoch: 2 [197120/251450]\tLoss: 0.0469 (0.0520) \tAcc: 93.75% (94.47%) \tEmb_Norm: 47.72 (51.37)\n",
      "Train Epoch: 2 [197760/251450]\tLoss: 0.0509 (0.0519) \tAcc: 93.75% (94.48%) \tEmb_Norm: 47.70 (51.36)\n",
      "Train Epoch: 2 [198400/251450]\tLoss: 0.0000 (0.0519) \tAcc: 100.00% (94.48%) \tEmb_Norm: 47.67 (51.35)\n",
      "Train Epoch: 2 [199040/251450]\tLoss: 0.0230 (0.0518) \tAcc: 96.88% (94.48%) \tEmb_Norm: 47.65 (51.33)\n",
      "Train Epoch: 2 [199680/251450]\tLoss: 0.2034 (0.0518) \tAcc: 90.62% (94.48%) \tEmb_Norm: 47.63 (51.32)\n",
      "Train Epoch: 2 [200320/251450]\tLoss: 0.0101 (0.0518) \tAcc: 96.88% (94.49%) \tEmb_Norm: 47.60 (51.31)\n",
      "Train Epoch: 2 [200960/251450]\tLoss: 0.0136 (0.0517) \tAcc: 96.88% (94.50%) \tEmb_Norm: 47.58 (51.30)\n",
      "Train Epoch: 2 [201600/251450]\tLoss: 0.0298 (0.0516) \tAcc: 93.75% (94.50%) \tEmb_Norm: 47.56 (51.29)\n",
      "=============== TEST epoch 2 ===============\n",
      "\n",
      "Test set: Average loss: 0.0592, Accuracy: 93.84%\n",
      "\n",
      "=============== checkpoint epoch 2 ===============\n",
      "=============== TRAIN epoch 3 ===============\n",
      "Train Epoch: 3 [0/251450]\tLoss: 0.0948 (0.0948) \tAcc: 90.62% (90.62%) \tEmb_Norm: 47.54 (47.54)\n",
      "Train Epoch: 3 [640/251450]\tLoss: 0.0453 (0.0426) \tAcc: 96.88% (95.54%) \tEmb_Norm: 47.52 (47.53)\n",
      "Train Epoch: 3 [1280/251450]\tLoss: 0.0452 (0.0430) \tAcc: 93.75% (95.43%) \tEmb_Norm: 47.50 (47.52)\n",
      "Train Epoch: 3 [1920/251450]\tLoss: 0.0427 (0.0431) \tAcc: 96.88% (95.59%) \tEmb_Norm: 47.47 (47.51)\n",
      "Train Epoch: 3 [2560/251450]\tLoss: 0.0205 (0.0463) \tAcc: 96.88% (95.25%) \tEmb_Norm: 47.45 (47.50)\n",
      "Train Epoch: 3 [3200/251450]\tLoss: 0.0288 (0.0439) \tAcc: 96.88% (95.39%) \tEmb_Norm: 47.42 (47.48)\n",
      "Train Epoch: 3 [3840/251450]\tLoss: 0.0000 (0.0397) \tAcc: 100.00% (95.74%) \tEmb_Norm: 47.40 (47.47)\n",
      "Train Epoch: 3 [4480/251450]\tLoss: 0.1354 (0.0427) \tAcc: 87.50% (95.43%) \tEmb_Norm: 47.38 (47.46)\n",
      "Train Epoch: 3 [5120/251450]\tLoss: 0.0139 (0.0415) \tAcc: 96.88% (95.50%) \tEmb_Norm: 47.36 (47.45)\n",
      "Train Epoch: 3 [5760/251450]\tLoss: 0.0052 (0.0436) \tAcc: 100.00% (95.20%) \tEmb_Norm: 47.33 (47.44)\n",
      "Train Epoch: 3 [6400/251450]\tLoss: 0.0531 (0.0458) \tAcc: 93.75% (95.01%) \tEmb_Norm: 47.31 (47.43)\n",
      "Train Epoch: 3 [7040/251450]\tLoss: 0.1926 (0.0466) \tAcc: 90.62% (94.94%) \tEmb_Norm: 47.28 (47.41)\n",
      "Train Epoch: 3 [7680/251450]\tLoss: 0.0042 (0.0467) \tAcc: 100.00% (94.94%) \tEmb_Norm: 47.26 (47.40)\n",
      "Train Epoch: 3 [8320/251450]\tLoss: 0.0000 (0.0455) \tAcc: 100.00% (95.03%) \tEmb_Norm: 47.24 (47.39)\n",
      "Train Epoch: 3 [8960/251450]\tLoss: 0.0000 (0.0447) \tAcc: 100.00% (95.05%) \tEmb_Norm: 47.21 (47.38)\n",
      "Train Epoch: 3 [9600/251450]\tLoss: 0.0000 (0.0446) \tAcc: 100.00% (95.07%) \tEmb_Norm: 47.19 (47.37)\n",
      "Train Epoch: 3 [10240/251450]\tLoss: 0.0000 (0.0454) \tAcc: 100.00% (95.04%) \tEmb_Norm: 47.17 (47.35)\n",
      "Train Epoch: 3 [10880/251450]\tLoss: 0.0897 (0.0449) \tAcc: 90.62% (95.06%) \tEmb_Norm: 47.14 (47.34)\n",
      "Train Epoch: 3 [11520/251450]\tLoss: 0.0000 (0.0455) \tAcc: 100.00% (94.98%) \tEmb_Norm: 47.12 (47.33)\n",
      "Train Epoch: 3 [12160/251450]\tLoss: 0.0455 (0.0461) \tAcc: 93.75% (94.95%) \tEmb_Norm: 47.09 (47.32)\n",
      "Train Epoch: 3 [12800/251450]\tLoss: 0.0170 (0.0461) \tAcc: 96.88% (94.94%) \tEmb_Norm: 47.07 (47.31)\n",
      "Train Epoch: 3 [13440/251450]\tLoss: 0.0176 (0.0456) \tAcc: 96.88% (95.00%) \tEmb_Norm: 47.05 (47.30)\n",
      "Train Epoch: 3 [14080/251450]\tLoss: 0.0802 (0.0447) \tAcc: 93.75% (95.08%) \tEmb_Norm: 47.03 (47.28)\n",
      "Train Epoch: 3 [14720/251450]\tLoss: 0.0689 (0.0443) \tAcc: 93.75% (95.07%) \tEmb_Norm: 47.00 (47.27)\n",
      "Train Epoch: 3 [15360/251450]\tLoss: 0.0212 (0.0440) \tAcc: 93.75% (95.06%) \tEmb_Norm: 46.98 (47.26)\n",
      "Train Epoch: 3 [16000/251450]\tLoss: 0.0683 (0.0442) \tAcc: 93.75% (95.02%) \tEmb_Norm: 46.96 (47.25)\n",
      "Train Epoch: 3 [16640/251450]\tLoss: 0.0454 (0.0448) \tAcc: 93.75% (94.96%) \tEmb_Norm: 46.93 (47.24)\n",
      "Train Epoch: 3 [17280/251450]\tLoss: 0.0707 (0.0445) \tAcc: 93.75% (94.99%) \tEmb_Norm: 46.91 (47.23)\n",
      "Train Epoch: 3 [17920/251450]\tLoss: 0.0478 (0.0440) \tAcc: 96.88% (95.04%) \tEmb_Norm: 46.89 (47.21)\n",
      "Train Epoch: 3 [18560/251450]\tLoss: 0.0636 (0.0445) \tAcc: 93.75% (94.98%) \tEmb_Norm: 46.86 (47.20)\n",
      "Train Epoch: 3 [19200/251450]\tLoss: 0.0385 (0.0443) \tAcc: 96.88% (95.02%) \tEmb_Norm: 46.84 (47.19)\n",
      "Train Epoch: 3 [19840/251450]\tLoss: 0.0000 (0.0439) \tAcc: 100.00% (95.05%) \tEmb_Norm: 46.82 (47.18)\n",
      "Train Epoch: 3 [20480/251450]\tLoss: 0.1443 (0.0435) \tAcc: 96.88% (95.11%) \tEmb_Norm: 46.80 (47.17)\n",
      "Train Epoch: 3 [21120/251450]\tLoss: 0.0857 (0.0433) \tAcc: 90.62% (95.13%) \tEmb_Norm: 46.77 (47.16)\n",
      "Train Epoch: 3 [21760/251450]\tLoss: 0.0036 (0.0431) \tAcc: 100.00% (95.16%) \tEmb_Norm: 46.75 (47.14)\n",
      "Train Epoch: 3 [22400/251450]\tLoss: 0.0353 (0.0431) \tAcc: 93.75% (95.13%) \tEmb_Norm: 46.73 (47.13)\n",
      "Train Epoch: 3 [23040/251450]\tLoss: 0.0055 (0.0431) \tAcc: 100.00% (95.13%) \tEmb_Norm: 46.70 (47.12)\n",
      "Train Epoch: 3 [23680/251450]\tLoss: 0.0577 (0.0435) \tAcc: 93.75% (95.10%) \tEmb_Norm: 46.68 (47.11)\n",
      "Train Epoch: 3 [24320/251450]\tLoss: 0.0000 (0.0434) \tAcc: 100.00% (95.09%) \tEmb_Norm: 46.66 (47.10)\n",
      "Train Epoch: 3 [24960/251450]\tLoss: 0.0000 (0.0429) \tAcc: 100.00% (95.13%) \tEmb_Norm: 46.63 (47.09)\n",
      "Train Epoch: 3 [25600/251450]\tLoss: 0.0588 (0.0426) \tAcc: 93.75% (95.13%) \tEmb_Norm: 46.61 (47.07)\n",
      "Train Epoch: 3 [26240/251450]\tLoss: 0.0917 (0.0425) \tAcc: 93.75% (95.15%) \tEmb_Norm: 46.59 (47.06)\n",
      "Train Epoch: 3 [26880/251450]\tLoss: 0.0062 (0.0426) \tAcc: 100.00% (95.13%) \tEmb_Norm: 46.57 (47.05)\n",
      "Train Epoch: 3 [27520/251450]\tLoss: 0.0227 (0.0425) \tAcc: 96.88% (95.17%) \tEmb_Norm: 46.54 (47.04)\n",
      "Train Epoch: 3 [28160/251450]\tLoss: 0.0520 (0.0423) \tAcc: 93.75% (95.18%) \tEmb_Norm: 46.52 (47.03)\n",
      "Train Epoch: 3 [28800/251450]\tLoss: 0.0805 (0.0420) \tAcc: 90.62% (95.20%) \tEmb_Norm: 46.50 (47.02)\n",
      "Train Epoch: 3 [29440/251450]\tLoss: 0.0597 (0.0418) \tAcc: 93.75% (95.22%) \tEmb_Norm: 46.48 (47.00)\n",
      "Train Epoch: 3 [30080/251450]\tLoss: 0.0025 (0.0416) \tAcc: 100.00% (95.25%) \tEmb_Norm: 46.46 (46.99)\n",
      "Train Epoch: 3 [30720/251450]\tLoss: 0.0000 (0.0417) \tAcc: 100.00% (95.25%) \tEmb_Norm: 46.43 (46.98)\n",
      "Train Epoch: 3 [31360/251450]\tLoss: 0.0000 (0.0414) \tAcc: 100.00% (95.27%) \tEmb_Norm: 46.41 (46.97)\n",
      "Train Epoch: 3 [32000/251450]\tLoss: 0.0258 (0.0417) \tAcc: 93.75% (95.23%) \tEmb_Norm: 46.38 (46.96)\n",
      "Train Epoch: 3 [32640/251450]\tLoss: 0.0062 (0.0418) \tAcc: 100.00% (95.22%) \tEmb_Norm: 46.36 (46.95)\n",
      "Train Epoch: 3 [33280/251450]\tLoss: 0.0809 (0.0417) \tAcc: 90.62% (95.23%) \tEmb_Norm: 46.34 (46.94)\n",
      "Train Epoch: 3 [33920/251450]\tLoss: 0.1452 (0.0416) \tAcc: 90.62% (95.23%) \tEmb_Norm: 46.32 (46.92)\n",
      "Train Epoch: 3 [34560/251450]\tLoss: 0.0358 (0.0417) \tAcc: 96.88% (95.22%) \tEmb_Norm: 46.29 (46.91)\n",
      "Train Epoch: 3 [35200/251450]\tLoss: 0.0432 (0.0418) \tAcc: 93.75% (95.21%) \tEmb_Norm: 46.27 (46.90)\n",
      "Train Epoch: 3 [35840/251450]\tLoss: 0.0765 (0.0418) \tAcc: 93.75% (95.22%) \tEmb_Norm: 46.24 (46.89)\n",
      "Train Epoch: 3 [36480/251450]\tLoss: 0.0167 (0.0415) \tAcc: 96.88% (95.25%) \tEmb_Norm: 46.22 (46.88)\n",
      "Train Epoch: 3 [37120/251450]\tLoss: 0.0482 (0.0416) \tAcc: 90.62% (95.24%) \tEmb_Norm: 46.20 (46.87)\n",
      "Train Epoch: 3 [37760/251450]\tLoss: 0.0000 (0.0413) \tAcc: 100.00% (95.25%) \tEmb_Norm: 46.18 (46.86)\n",
      "Train Epoch: 3 [38400/251450]\tLoss: 0.0494 (0.0413) \tAcc: 96.88% (95.26%) \tEmb_Norm: 46.16 (46.84)\n",
      "Train Epoch: 3 [39040/251450]\tLoss: 0.0089 (0.0413) \tAcc: 96.88% (95.27%) \tEmb_Norm: 46.13 (46.83)\n",
      "Train Epoch: 3 [39680/251450]\tLoss: 0.0569 (0.0412) \tAcc: 93.75% (95.29%) \tEmb_Norm: 46.11 (46.82)\n",
      "Train Epoch: 3 [40320/251450]\tLoss: 0.0000 (0.0411) \tAcc: 100.00% (95.29%) \tEmb_Norm: 46.09 (46.81)\n",
      "Train Epoch: 3 [40960/251450]\tLoss: 0.0234 (0.0410) \tAcc: 93.75% (95.29%) \tEmb_Norm: 46.06 (46.80)\n",
      "Train Epoch: 3 [41600/251450]\tLoss: 0.0452 (0.0409) \tAcc: 96.88% (95.30%) \tEmb_Norm: 46.04 (46.79)\n",
      "Train Epoch: 3 [42240/251450]\tLoss: 0.0580 (0.0408) \tAcc: 90.62% (95.31%) \tEmb_Norm: 46.02 (46.78)\n",
      "Train Epoch: 3 [42880/251450]\tLoss: 0.0000 (0.0407) \tAcc: 100.00% (95.31%) \tEmb_Norm: 46.00 (46.76)\n",
      "Train Epoch: 3 [43520/251450]\tLoss: 0.0320 (0.0405) \tAcc: 96.88% (95.32%) \tEmb_Norm: 45.98 (46.75)\n",
      "Train Epoch: 3 [44160/251450]\tLoss: 0.0000 (0.0406) \tAcc: 100.00% (95.32%) \tEmb_Norm: 45.95 (46.74)\n",
      "Train Epoch: 3 [44800/251450]\tLoss: 0.0121 (0.0406) \tAcc: 96.88% (95.32%) \tEmb_Norm: 45.93 (46.73)\n",
      "Train Epoch: 3 [45440/251450]\tLoss: 0.0278 (0.0405) \tAcc: 93.75% (95.31%) \tEmb_Norm: 45.91 (46.72)\n",
      "Train Epoch: 3 [46080/251450]\tLoss: 0.0055 (0.0405) \tAcc: 100.00% (95.30%) \tEmb_Norm: 45.88 (46.71)\n",
      "Train Epoch: 3 [46720/251450]\tLoss: 0.0067 (0.0404) \tAcc: 96.88% (95.31%) \tEmb_Norm: 45.86 (46.70)\n",
      "Train Epoch: 3 [47360/251450]\tLoss: 0.0215 (0.0402) \tAcc: 96.88% (95.32%) \tEmb_Norm: 45.84 (46.68)\n",
      "Train Epoch: 3 [48000/251450]\tLoss: 0.0350 (0.0401) \tAcc: 93.75% (95.32%) \tEmb_Norm: 45.82 (46.67)\n",
      "Train Epoch: 3 [48640/251450]\tLoss: 0.0170 (0.0402) \tAcc: 96.88% (95.32%) \tEmb_Norm: 45.79 (46.66)\n",
      "Train Epoch: 3 [49280/251450]\tLoss: 0.0746 (0.0402) \tAcc: 93.75% (95.32%) \tEmb_Norm: 45.77 (46.65)\n",
      "Train Epoch: 3 [49920/251450]\tLoss: 0.0913 (0.0402) \tAcc: 93.75% (95.32%) \tEmb_Norm: 45.75 (46.64)\n",
      "Train Epoch: 3 [50560/251450]\tLoss: 0.0202 (0.0401) \tAcc: 96.88% (95.33%) \tEmb_Norm: 45.73 (46.63)\n",
      "Train Epoch: 3 [51200/251450]\tLoss: 0.0930 (0.0400) \tAcc: 90.62% (95.32%) \tEmb_Norm: 45.70 (46.62)\n",
      "Train Epoch: 3 [51840/251450]\tLoss: 0.0948 (0.0401) \tAcc: 87.50% (95.31%) \tEmb_Norm: 45.68 (46.60)\n",
      "Train Epoch: 3 [52480/251450]\tLoss: 0.0051 (0.0402) \tAcc: 100.00% (95.30%) \tEmb_Norm: 45.66 (46.59)\n",
      "Train Epoch: 3 [53120/251450]\tLoss: 0.0599 (0.0401) \tAcc: 90.62% (95.30%) \tEmb_Norm: 45.63 (46.58)\n",
      "Train Epoch: 3 [53760/251450]\tLoss: 0.0609 (0.0401) \tAcc: 90.62% (95.29%) \tEmb_Norm: 45.61 (46.57)\n",
      "Train Epoch: 3 [54400/251450]\tLoss: 0.1228 (0.0401) \tAcc: 87.50% (95.29%) \tEmb_Norm: 45.59 (46.56)\n",
      "Train Epoch: 3 [55040/251450]\tLoss: 0.0375 (0.0401) \tAcc: 96.88% (95.28%) \tEmb_Norm: 45.57 (46.55)\n",
      "Train Epoch: 3 [55680/251450]\tLoss: 0.0058 (0.0399) \tAcc: 100.00% (95.31%) \tEmb_Norm: 45.54 (46.54)\n",
      "Train Epoch: 3 [56320/251450]\tLoss: 0.0093 (0.0396) \tAcc: 96.88% (95.33%) \tEmb_Norm: 45.52 (46.52)\n",
      "Train Epoch: 3 [56960/251450]\tLoss: 0.0600 (0.0396) \tAcc: 93.75% (95.32%) \tEmb_Norm: 45.50 (46.51)\n",
      "Train Epoch: 3 [57600/251450]\tLoss: 0.0570 (0.0397) \tAcc: 93.75% (95.32%) \tEmb_Norm: 45.48 (46.50)\n",
      "Train Epoch: 3 [58240/251450]\tLoss: 0.0027 (0.0396) \tAcc: 100.00% (95.32%) \tEmb_Norm: 45.46 (46.49)\n",
      "Train Epoch: 3 [58880/251450]\tLoss: 0.0410 (0.0395) \tAcc: 96.88% (95.33%) \tEmb_Norm: 45.43 (46.48)\n",
      "Train Epoch: 3 [59520/251450]\tLoss: 0.0000 (0.0395) \tAcc: 100.00% (95.32%) \tEmb_Norm: 45.41 (46.47)\n",
      "Train Epoch: 3 [60160/251450]\tLoss: 0.0271 (0.0394) \tAcc: 93.75% (95.32%) \tEmb_Norm: 45.39 (46.46)\n",
      "Train Epoch: 3 [60800/251450]\tLoss: 0.0035 (0.0394) \tAcc: 100.00% (95.33%) \tEmb_Norm: 45.37 (46.44)\n",
      "Train Epoch: 3 [61440/251450]\tLoss: 0.0524 (0.0393) \tAcc: 90.62% (95.34%) \tEmb_Norm: 45.34 (46.43)\n",
      "Train Epoch: 3 [62080/251450]\tLoss: 0.0516 (0.0392) \tAcc: 93.75% (95.34%) \tEmb_Norm: 45.32 (46.42)\n",
      "Train Epoch: 3 [62720/251450]\tLoss: 0.0375 (0.0393) \tAcc: 93.75% (95.34%) \tEmb_Norm: 45.30 (46.41)\n",
      "Train Epoch: 3 [63360/251450]\tLoss: 0.0432 (0.0392) \tAcc: 93.75% (95.34%) \tEmb_Norm: 45.28 (46.40)\n",
      "Train Epoch: 3 [64000/251450]\tLoss: 0.0613 (0.0392) \tAcc: 93.75% (95.34%) \tEmb_Norm: 45.25 (46.39)\n",
      "Train Epoch: 3 [64640/251450]\tLoss: 0.0441 (0.0392) \tAcc: 96.88% (95.33%) \tEmb_Norm: 45.23 (46.38)\n",
      "Train Epoch: 3 [65280/251450]\tLoss: 0.0274 (0.0394) \tAcc: 93.75% (95.33%) \tEmb_Norm: 45.21 (46.37)\n",
      "Train Epoch: 3 [65920/251450]\tLoss: 0.0143 (0.0394) \tAcc: 96.88% (95.32%) \tEmb_Norm: 45.18 (46.35)\n",
      "Train Epoch: 3 [66560/251450]\tLoss: 0.0495 (0.0394) \tAcc: 93.75% (95.32%) \tEmb_Norm: 45.16 (46.34)\n",
      "Train Epoch: 3 [67200/251450]\tLoss: 0.0366 (0.0393) \tAcc: 90.62% (95.32%) \tEmb_Norm: 45.14 (46.33)\n",
      "Train Epoch: 3 [67840/251450]\tLoss: 0.0721 (0.0393) \tAcc: 90.62% (95.31%) \tEmb_Norm: 45.12 (46.32)\n",
      "Train Epoch: 3 [68480/251450]\tLoss: 0.0112 (0.0394) \tAcc: 96.88% (95.30%) \tEmb_Norm: 45.09 (46.31)\n",
      "Train Epoch: 3 [69120/251450]\tLoss: 0.0247 (0.0393) \tAcc: 96.88% (95.29%) \tEmb_Norm: 45.07 (46.30)\n",
      "Train Epoch: 3 [69760/251450]\tLoss: 0.0191 (0.0392) \tAcc: 93.75% (95.29%) \tEmb_Norm: 45.05 (46.29)\n",
      "Train Epoch: 3 [70400/251450]\tLoss: 0.0770 (0.0392) \tAcc: 90.62% (95.29%) \tEmb_Norm: 45.03 (46.27)\n",
      "Train Epoch: 3 [71040/251450]\tLoss: 0.0163 (0.0392) \tAcc: 96.88% (95.29%) \tEmb_Norm: 45.00 (46.26)\n",
      "Train Epoch: 3 [71680/251450]\tLoss: 0.0025 (0.0390) \tAcc: 100.00% (95.31%) \tEmb_Norm: 44.98 (46.25)\n",
      "Train Epoch: 3 [72320/251450]\tLoss: 0.0013 (0.0391) \tAcc: 100.00% (95.31%) \tEmb_Norm: 44.96 (46.24)\n",
      "Train Epoch: 3 [72960/251450]\tLoss: 0.0324 (0.0389) \tAcc: 96.88% (95.33%) \tEmb_Norm: 44.94 (46.23)\n",
      "Train Epoch: 3 [73600/251450]\tLoss: 0.0545 (0.0390) \tAcc: 93.75% (95.31%) \tEmb_Norm: 44.91 (46.22)\n",
      "Train Epoch: 3 [74240/251450]\tLoss: 0.0000 (0.0389) \tAcc: 100.00% (95.32%) \tEmb_Norm: 44.89 (46.21)\n",
      "Train Epoch: 3 [74880/251450]\tLoss: 0.0015 (0.0388) \tAcc: 100.00% (95.32%) \tEmb_Norm: 44.87 (46.19)\n",
      "Train Epoch: 3 [75520/251450]\tLoss: 0.1288 (0.0389) \tAcc: 90.62% (95.32%) \tEmb_Norm: 44.85 (46.18)\n",
      "Train Epoch: 3 [76160/251450]\tLoss: 0.0000 (0.0388) \tAcc: 100.00% (95.32%) \tEmb_Norm: 44.82 (46.17)\n",
      "Train Epoch: 3 [76800/251450]\tLoss: 0.0169 (0.0388) \tAcc: 96.88% (95.32%) \tEmb_Norm: 44.80 (46.16)\n",
      "Train Epoch: 3 [77440/251450]\tLoss: 0.0311 (0.0387) \tAcc: 93.75% (95.33%) \tEmb_Norm: 44.78 (46.15)\n",
      "Train Epoch: 3 [78080/251450]\tLoss: 0.0143 (0.0386) \tAcc: 96.88% (95.34%) \tEmb_Norm: 44.76 (46.14)\n",
      "Train Epoch: 3 [78720/251450]\tLoss: 0.0212 (0.0387) \tAcc: 93.75% (95.32%) \tEmb_Norm: 44.73 (46.13)\n",
      "Train Epoch: 3 [79360/251450]\tLoss: 0.0508 (0.0388) \tAcc: 93.75% (95.32%) \tEmb_Norm: 44.71 (46.12)\n",
      "Train Epoch: 3 [80000/251450]\tLoss: 0.0070 (0.0387) \tAcc: 100.00% (95.33%) \tEmb_Norm: 44.69 (46.10)\n",
      "Train Epoch: 3 [80640/251450]\tLoss: 0.0007 (0.0387) \tAcc: 100.00% (95.33%) \tEmb_Norm: 44.66 (46.09)\n",
      "Train Epoch: 3 [81280/251450]\tLoss: 0.1412 (0.0388) \tAcc: 87.50% (95.32%) \tEmb_Norm: 44.64 (46.08)\n",
      "Train Epoch: 3 [81920/251450]\tLoss: 0.0347 (0.0388) \tAcc: 96.88% (95.32%) \tEmb_Norm: 44.62 (46.07)\n",
      "Train Epoch: 3 [82560/251450]\tLoss: 0.0499 (0.0387) \tAcc: 93.75% (95.33%) \tEmb_Norm: 44.60 (46.06)\n",
      "Train Epoch: 3 [83200/251450]\tLoss: 0.0153 (0.0387) \tAcc: 96.88% (95.33%) \tEmb_Norm: 44.57 (46.05)\n",
      "Train Epoch: 3 [83840/251450]\tLoss: 0.0075 (0.0387) \tAcc: 96.88% (95.33%) \tEmb_Norm: 44.55 (46.04)\n",
      "Train Epoch: 3 [84480/251450]\tLoss: 0.0379 (0.0388) \tAcc: 93.75% (95.33%) \tEmb_Norm: 44.53 (46.03)\n",
      "Train Epoch: 3 [85120/251450]\tLoss: 0.0342 (0.0388) \tAcc: 93.75% (95.34%) \tEmb_Norm: 44.50 (46.01)\n",
      "Train Epoch: 3 [85760/251450]\tLoss: 0.0239 (0.0387) \tAcc: 96.88% (95.34%) \tEmb_Norm: 44.48 (46.00)\n",
      "Train Epoch: 3 [86400/251450]\tLoss: 0.0341 (0.0387) \tAcc: 93.75% (95.35%) \tEmb_Norm: 44.46 (45.99)\n",
      "Train Epoch: 3 [87040/251450]\tLoss: 0.0000 (0.0386) \tAcc: 100.00% (95.35%) \tEmb_Norm: 44.44 (45.98)\n",
      "Train Epoch: 3 [87680/251450]\tLoss: 0.1059 (0.0385) \tAcc: 90.62% (95.36%) \tEmb_Norm: 44.42 (45.97)\n",
      "Train Epoch: 3 [88320/251450]\tLoss: 0.0353 (0.0385) \tAcc: 93.75% (95.37%) \tEmb_Norm: 44.39 (45.96)\n",
      "Train Epoch: 3 [88960/251450]\tLoss: 0.0394 (0.0384) \tAcc: 96.88% (95.37%) \tEmb_Norm: 44.37 (45.95)\n",
      "Train Epoch: 3 [89600/251450]\tLoss: 0.0749 (0.0384) \tAcc: 87.50% (95.37%) \tEmb_Norm: 44.35 (45.93)\n",
      "Train Epoch: 3 [90240/251450]\tLoss: 0.0862 (0.0385) \tAcc: 90.62% (95.36%) \tEmb_Norm: 44.32 (45.92)\n",
      "Train Epoch: 3 [90880/251450]\tLoss: 0.0155 (0.0385) \tAcc: 96.88% (95.37%) \tEmb_Norm: 44.30 (45.91)\n",
      "Train Epoch: 3 [91520/251450]\tLoss: 0.1834 (0.0386) \tAcc: 84.38% (95.36%) \tEmb_Norm: 44.28 (45.90)\n",
      "Train Epoch: 3 [92160/251450]\tLoss: 0.0813 (0.0385) \tAcc: 93.75% (95.37%) \tEmb_Norm: 44.25 (45.89)\n",
      "Train Epoch: 3 [92800/251450]\tLoss: 0.1004 (0.0385) \tAcc: 90.62% (95.37%) \tEmb_Norm: 44.23 (45.88)\n",
      "Train Epoch: 3 [93440/251450]\tLoss: 0.0000 (0.0384) \tAcc: 100.00% (95.38%) \tEmb_Norm: 44.21 (45.87)\n",
      "Train Epoch: 3 [94080/251450]\tLoss: 0.0439 (0.0384) \tAcc: 93.75% (95.38%) \tEmb_Norm: 44.19 (45.85)\n",
      "Train Epoch: 3 [94720/251450]\tLoss: 0.0194 (0.0385) \tAcc: 96.88% (95.38%) \tEmb_Norm: 44.16 (45.84)\n",
      "Train Epoch: 3 [95360/251450]\tLoss: 0.0467 (0.0385) \tAcc: 93.75% (95.38%) \tEmb_Norm: 44.14 (45.83)\n",
      "Train Epoch: 3 [96000/251450]\tLoss: 0.1425 (0.0385) \tAcc: 87.50% (95.38%) \tEmb_Norm: 44.12 (45.82)\n",
      "Train Epoch: 3 [96640/251450]\tLoss: 0.0233 (0.0387) \tAcc: 96.88% (95.37%) \tEmb_Norm: 44.09 (45.81)\n",
      "Train Epoch: 3 [97280/251450]\tLoss: 0.0786 (0.0388) \tAcc: 90.62% (95.35%) \tEmb_Norm: 44.07 (45.80)\n",
      "Train Epoch: 3 [97920/251450]\tLoss: 0.0363 (0.0387) \tAcc: 96.88% (95.35%) \tEmb_Norm: 44.04 (45.79)\n",
      "Train Epoch: 3 [98560/251450]\tLoss: 0.0387 (0.0387) \tAcc: 96.88% (95.35%) \tEmb_Norm: 44.02 (45.78)\n",
      "Train Epoch: 3 [99200/251450]\tLoss: 0.0103 (0.0387) \tAcc: 96.88% (95.36%) \tEmb_Norm: 44.00 (45.76)\n",
      "Train Epoch: 3 [99840/251450]\tLoss: 0.0319 (0.0387) \tAcc: 93.75% (95.35%) \tEmb_Norm: 43.98 (45.75)\n",
      "Train Epoch: 3 [100480/251450]\tLoss: 0.1005 (0.0386) \tAcc: 87.50% (95.37%) \tEmb_Norm: 43.95 (45.74)\n",
      "Train Epoch: 3 [101120/251450]\tLoss: 0.0291 (0.0386) \tAcc: 93.75% (95.37%) \tEmb_Norm: 43.93 (45.73)\n",
      "Train Epoch: 3 [101760/251450]\tLoss: 0.0353 (0.0385) \tAcc: 96.88% (95.37%) \tEmb_Norm: 43.91 (45.72)\n",
      "Train Epoch: 3 [102400/251450]\tLoss: 0.0175 (0.0386) \tAcc: 96.88% (95.36%) \tEmb_Norm: 43.89 (45.71)\n",
      "Train Epoch: 3 [103040/251450]\tLoss: 0.1115 (0.0386) \tAcc: 87.50% (95.36%) \tEmb_Norm: 43.86 (45.70)\n",
      "Train Epoch: 3 [103680/251450]\tLoss: 0.0105 (0.0386) \tAcc: 96.88% (95.36%) \tEmb_Norm: 43.84 (45.68)\n",
      "Train Epoch: 3 [104320/251450]\tLoss: 0.0094 (0.0385) \tAcc: 96.88% (95.37%) \tEmb_Norm: 43.82 (45.67)\n",
      "Train Epoch: 3 [104960/251450]\tLoss: 0.0407 (0.0385) \tAcc: 96.88% (95.37%) \tEmb_Norm: 43.80 (45.66)\n",
      "Train Epoch: 3 [105600/251450]\tLoss: 0.0269 (0.0385) \tAcc: 96.88% (95.36%) \tEmb_Norm: 43.77 (45.65)\n",
      "Train Epoch: 3 [106240/251450]\tLoss: 0.0289 (0.0385) \tAcc: 93.75% (95.35%) \tEmb_Norm: 43.75 (45.64)\n",
      "Train Epoch: 3 [106880/251450]\tLoss: 0.0769 (0.0386) \tAcc: 93.75% (95.34%) \tEmb_Norm: 43.72 (45.63)\n",
      "Train Epoch: 3 [107520/251450]\tLoss: 0.0000 (0.0386) \tAcc: 100.00% (95.35%) \tEmb_Norm: 43.70 (45.62)\n",
      "Train Epoch: 3 [108160/251450]\tLoss: 0.0182 (0.0387) \tAcc: 96.88% (95.34%) \tEmb_Norm: 43.68 (45.60)\n",
      "Train Epoch: 3 [108800/251450]\tLoss: 0.0715 (0.0387) \tAcc: 90.62% (95.33%) \tEmb_Norm: 43.66 (45.59)\n",
      "Train Epoch: 3 [109440/251450]\tLoss: 0.0117 (0.0387) \tAcc: 96.88% (95.33%) \tEmb_Norm: 43.63 (45.58)\n",
      "Train Epoch: 3 [110080/251450]\tLoss: 0.0052 (0.0387) \tAcc: 100.00% (95.33%) \tEmb_Norm: 43.61 (45.57)\n",
      "Train Epoch: 3 [110720/251450]\tLoss: 0.0543 (0.0387) \tAcc: 96.88% (95.33%) \tEmb_Norm: 43.59 (45.56)\n",
      "Train Epoch: 3 [111360/251450]\tLoss: 0.0263 (0.0387) \tAcc: 96.88% (95.33%) \tEmb_Norm: 43.56 (45.55)\n",
      "Train Epoch: 3 [112000/251450]\tLoss: 0.0325 (0.0386) \tAcc: 93.75% (95.34%) \tEmb_Norm: 43.54 (45.54)\n",
      "Train Epoch: 3 [112640/251450]\tLoss: 0.0594 (0.0386) \tAcc: 93.75% (95.34%) \tEmb_Norm: 43.52 (45.52)\n",
      "Train Epoch: 3 [113280/251450]\tLoss: 0.0065 (0.0386) \tAcc: 96.88% (95.34%) \tEmb_Norm: 43.50 (45.51)\n",
      "Train Epoch: 3 [113920/251450]\tLoss: 0.0783 (0.0386) \tAcc: 90.62% (95.34%) \tEmb_Norm: 43.47 (45.50)\n",
      "Train Epoch: 3 [114560/251450]\tLoss: 0.0109 (0.0386) \tAcc: 100.00% (95.34%) \tEmb_Norm: 43.45 (45.49)\n",
      "Train Epoch: 3 [115200/251450]\tLoss: 0.1094 (0.0386) \tAcc: 90.62% (95.34%) \tEmb_Norm: 43.43 (45.48)\n",
      "Train Epoch: 3 [115840/251450]\tLoss: 0.0457 (0.0385) \tAcc: 96.88% (95.35%) \tEmb_Norm: 43.41 (45.47)\n",
      "Train Epoch: 3 [116480/251450]\tLoss: 0.0866 (0.0385) \tAcc: 90.62% (95.35%) \tEmb_Norm: 43.38 (45.46)\n",
      "Train Epoch: 3 [117120/251450]\tLoss: 0.1084 (0.0385) \tAcc: 90.62% (95.34%) \tEmb_Norm: 43.36 (45.44)\n",
      "Train Epoch: 3 [117760/251450]\tLoss: 0.0068 (0.0385) \tAcc: 100.00% (95.35%) \tEmb_Norm: 43.34 (45.43)\n",
      "Train Epoch: 3 [118400/251450]\tLoss: 0.0599 (0.0385) \tAcc: 93.75% (95.35%) \tEmb_Norm: 43.31 (45.42)\n",
      "Train Epoch: 3 [119040/251450]\tLoss: 0.0427 (0.0385) \tAcc: 96.88% (95.35%) \tEmb_Norm: 43.29 (45.41)\n",
      "Train Epoch: 3 [119680/251450]\tLoss: 0.0045 (0.0384) \tAcc: 100.00% (95.35%) \tEmb_Norm: 43.27 (45.40)\n",
      "Train Epoch: 3 [120320/251450]\tLoss: 0.0088 (0.0384) \tAcc: 96.88% (95.36%) \tEmb_Norm: 43.25 (45.39)\n",
      "Train Epoch: 3 [120960/251450]\tLoss: 0.0313 (0.0385) \tAcc: 93.75% (95.35%) \tEmb_Norm: 43.22 (45.38)\n",
      "Train Epoch: 3 [121600/251450]\tLoss: 0.0245 (0.0384) \tAcc: 96.88% (95.36%) \tEmb_Norm: 43.20 (45.37)\n",
      "Train Epoch: 3 [122240/251450]\tLoss: 0.0000 (0.0384) \tAcc: 100.00% (95.36%) \tEmb_Norm: 43.18 (45.35)\n",
      "Train Epoch: 3 [122880/251450]\tLoss: 0.0000 (0.0383) \tAcc: 100.00% (95.37%) \tEmb_Norm: 43.16 (45.34)\n",
      "Train Epoch: 3 [123520/251450]\tLoss: 0.0287 (0.0383) \tAcc: 93.75% (95.37%) \tEmb_Norm: 43.13 (45.33)\n",
      "Train Epoch: 3 [124160/251450]\tLoss: 0.0129 (0.0383) \tAcc: 96.88% (95.37%) \tEmb_Norm: 43.11 (45.32)\n",
      "Train Epoch: 3 [124800/251450]\tLoss: 0.0000 (0.0383) \tAcc: 100.00% (95.37%) \tEmb_Norm: 43.09 (45.31)\n",
      "Train Epoch: 3 [125440/251450]\tLoss: 0.0147 (0.0382) \tAcc: 96.88% (95.38%) \tEmb_Norm: 43.07 (45.30)\n",
      "Train Epoch: 3 [126080/251450]\tLoss: 0.0059 (0.0381) \tAcc: 100.00% (95.38%) \tEmb_Norm: 43.05 (45.29)\n",
      "Train Epoch: 3 [126720/251450]\tLoss: 0.0752 (0.0382) \tAcc: 90.62% (95.38%) \tEmb_Norm: 43.02 (45.27)\n",
      "Train Epoch: 3 [127360/251450]\tLoss: 0.0055 (0.0381) \tAcc: 100.00% (95.39%) \tEmb_Norm: 43.00 (45.26)\n",
      "Train Epoch: 3 [128000/251450]\tLoss: 0.0257 (0.0381) \tAcc: 96.88% (95.39%) \tEmb_Norm: 42.98 (45.25)\n",
      "Train Epoch: 3 [128640/251450]\tLoss: 0.0447 (0.0381) \tAcc: 93.75% (95.38%) \tEmb_Norm: 42.95 (45.24)\n",
      "Train Epoch: 3 [129280/251450]\tLoss: 0.0314 (0.0381) \tAcc: 93.75% (95.39%) \tEmb_Norm: 42.93 (45.23)\n",
      "Train Epoch: 3 [129920/251450]\tLoss: 0.0100 (0.0381) \tAcc: 96.88% (95.38%) \tEmb_Norm: 42.91 (45.22)\n",
      "Train Epoch: 3 [130560/251450]\tLoss: 0.0000 (0.0381) \tAcc: 100.00% (95.37%) \tEmb_Norm: 42.89 (45.21)\n",
      "Train Epoch: 3 [131200/251450]\tLoss: 0.0041 (0.0381) \tAcc: 100.00% (95.37%) \tEmb_Norm: 42.86 (45.19)\n",
      "Train Epoch: 3 [131840/251450]\tLoss: 0.0420 (0.0381) \tAcc: 93.75% (95.37%) \tEmb_Norm: 42.84 (45.18)\n",
      "Train Epoch: 3 [132480/251450]\tLoss: 0.0375 (0.0381) \tAcc: 93.75% (95.37%) \tEmb_Norm: 42.82 (45.17)\n",
      "Train Epoch: 3 [133120/251450]\tLoss: 0.0255 (0.0381) \tAcc: 96.88% (95.37%) \tEmb_Norm: 42.79 (45.16)\n",
      "Train Epoch: 3 [133760/251450]\tLoss: 0.0024 (0.0381) \tAcc: 100.00% (95.37%) \tEmb_Norm: 42.77 (45.15)\n",
      "Train Epoch: 3 [134400/251450]\tLoss: 0.0000 (0.0381) \tAcc: 100.00% (95.37%) \tEmb_Norm: 42.75 (45.14)\n",
      "Train Epoch: 3 [135040/251450]\tLoss: 0.0107 (0.0381) \tAcc: 96.88% (95.37%) \tEmb_Norm: 42.73 (45.13)\n",
      "Train Epoch: 3 [135680/251450]\tLoss: 0.0952 (0.0381) \tAcc: 93.75% (95.38%) \tEmb_Norm: 42.70 (45.11)\n",
      "Train Epoch: 3 [136320/251450]\tLoss: 0.0139 (0.0381) \tAcc: 96.88% (95.38%) \tEmb_Norm: 42.68 (45.10)\n",
      "Train Epoch: 3 [136960/251450]\tLoss: 0.0054 (0.0381) \tAcc: 100.00% (95.38%) \tEmb_Norm: 42.66 (45.09)\n",
      "Train Epoch: 3 [137600/251450]\tLoss: 0.0652 (0.0381) \tAcc: 87.50% (95.38%) \tEmb_Norm: 42.63 (45.08)\n",
      "Train Epoch: 3 [138240/251450]\tLoss: 0.0545 (0.0381) \tAcc: 90.62% (95.37%) \tEmb_Norm: 42.61 (45.07)\n",
      "Train Epoch: 3 [138880/251450]\tLoss: 0.0073 (0.0381) \tAcc: 96.88% (95.38%) \tEmb_Norm: 42.59 (45.06)\n",
      "Train Epoch: 3 [139520/251450]\tLoss: 0.0000 (0.0380) \tAcc: 100.00% (95.38%) \tEmb_Norm: 42.57 (45.05)\n",
      "Train Epoch: 3 [140160/251450]\tLoss: 0.0166 (0.0380) \tAcc: 96.88% (95.38%) \tEmb_Norm: 42.54 (45.04)\n",
      "Train Epoch: 3 [140800/251450]\tLoss: 0.0071 (0.0381) \tAcc: 96.88% (95.37%) \tEmb_Norm: 42.52 (45.02)\n",
      "Train Epoch: 3 [141440/251450]\tLoss: 0.0000 (0.0380) \tAcc: 100.00% (95.38%) \tEmb_Norm: 42.50 (45.01)\n",
      "Train Epoch: 3 [142080/251450]\tLoss: 0.0872 (0.0380) \tAcc: 90.62% (95.38%) \tEmb_Norm: 42.48 (45.00)\n",
      "Train Epoch: 3 [142720/251450]\tLoss: 0.0000 (0.0379) \tAcc: 100.00% (95.39%) \tEmb_Norm: 42.45 (44.99)\n",
      "Train Epoch: 3 [143360/251450]\tLoss: 0.0052 (0.0379) \tAcc: 100.00% (95.39%) \tEmb_Norm: 42.43 (44.98)\n",
      "Train Epoch: 3 [144000/251450]\tLoss: 0.0044 (0.0378) \tAcc: 100.00% (95.40%) \tEmb_Norm: 42.41 (44.97)\n",
      "Train Epoch: 3 [144640/251450]\tLoss: 0.0000 (0.0378) \tAcc: 100.00% (95.40%) \tEmb_Norm: 42.39 (44.96)\n",
      "Train Epoch: 3 [145280/251450]\tLoss: 0.0499 (0.0377) \tAcc: 90.62% (95.40%) \tEmb_Norm: 42.37 (44.94)\n",
      "Train Epoch: 3 [145920/251450]\tLoss: 0.0041 (0.0377) \tAcc: 100.00% (95.41%) \tEmb_Norm: 42.35 (44.93)\n",
      "Train Epoch: 3 [146560/251450]\tLoss: 0.0000 (0.0376) \tAcc: 100.00% (95.41%) \tEmb_Norm: 42.32 (44.92)\n",
      "Train Epoch: 3 [147200/251450]\tLoss: 0.0000 (0.0376) \tAcc: 100.00% (95.41%) \tEmb_Norm: 42.30 (44.91)\n",
      "Train Epoch: 3 [147840/251450]\tLoss: 0.0444 (0.0376) \tAcc: 93.75% (95.41%) \tEmb_Norm: 42.28 (44.90)\n",
      "Train Epoch: 3 [148480/251450]\tLoss: 0.0000 (0.0376) \tAcc: 100.00% (95.41%) \tEmb_Norm: 42.26 (44.89)\n",
      "Train Epoch: 3 [149120/251450]\tLoss: 0.0284 (0.0375) \tAcc: 96.88% (95.41%) \tEmb_Norm: 42.24 (44.88)\n",
      "Train Epoch: 3 [149760/251450]\tLoss: 0.0391 (0.0375) \tAcc: 90.62% (95.41%) \tEmb_Norm: 42.21 (44.86)\n",
      "Train Epoch: 3 [150400/251450]\tLoss: 0.0638 (0.0375) \tAcc: 90.62% (95.41%) \tEmb_Norm: 42.19 (44.85)\n",
      "Train Epoch: 3 [151040/251450]\tLoss: 0.0214 (0.0375) \tAcc: 93.75% (95.41%) \tEmb_Norm: 42.17 (44.84)\n",
      "Train Epoch: 3 [151680/251450]\tLoss: 0.0212 (0.0375) \tAcc: 96.88% (95.41%) \tEmb_Norm: 42.14 (44.83)\n",
      "Train Epoch: 3 [152320/251450]\tLoss: 0.0134 (0.0375) \tAcc: 96.88% (95.41%) \tEmb_Norm: 42.12 (44.82)\n",
      "Train Epoch: 3 [152960/251450]\tLoss: 0.0000 (0.0375) \tAcc: 100.00% (95.41%) \tEmb_Norm: 42.10 (44.81)\n",
      "Train Epoch: 3 [153600/251450]\tLoss: 0.0890 (0.0375) \tAcc: 84.38% (95.41%) \tEmb_Norm: 42.08 (44.80)\n",
      "Train Epoch: 3 [154240/251450]\tLoss: 0.0373 (0.0375) \tAcc: 93.75% (95.41%) \tEmb_Norm: 42.05 (44.79)\n",
      "Train Epoch: 3 [154880/251450]\tLoss: 0.0057 (0.0375) \tAcc: 100.00% (95.41%) \tEmb_Norm: 42.03 (44.77)\n",
      "Train Epoch: 3 [155520/251450]\tLoss: 0.0000 (0.0375) \tAcc: 100.00% (95.41%) \tEmb_Norm: 42.01 (44.76)\n",
      "Train Epoch: 3 [156160/251450]\tLoss: 0.0970 (0.0375) \tAcc: 87.50% (95.40%) \tEmb_Norm: 41.98 (44.75)\n",
      "Train Epoch: 3 [156800/251450]\tLoss: 0.0511 (0.0376) \tAcc: 93.75% (95.40%) \tEmb_Norm: 41.96 (44.74)\n",
      "Train Epoch: 3 [157440/251450]\tLoss: 0.0511 (0.0376) \tAcc: 93.75% (95.39%) \tEmb_Norm: 41.94 (44.73)\n",
      "Train Epoch: 3 [158080/251450]\tLoss: 0.1157 (0.0375) \tAcc: 84.38% (95.40%) \tEmb_Norm: 41.91 (44.72)\n",
      "Train Epoch: 3 [158720/251450]\tLoss: 0.0193 (0.0375) \tAcc: 93.75% (95.39%) \tEmb_Norm: 41.89 (44.71)\n",
      "Train Epoch: 3 [159360/251450]\tLoss: 0.0926 (0.0375) \tAcc: 96.88% (95.39%) \tEmb_Norm: 41.87 (44.69)\n",
      "Train Epoch: 3 [160000/251450]\tLoss: 0.0833 (0.0376) \tAcc: 87.50% (95.38%) \tEmb_Norm: 41.84 (44.68)\n",
      "Train Epoch: 3 [160640/251450]\tLoss: 0.0148 (0.0376) \tAcc: 96.88% (95.38%) \tEmb_Norm: 41.82 (44.67)\n",
      "Train Epoch: 3 [161280/251450]\tLoss: 0.0078 (0.0376) \tAcc: 96.88% (95.38%) \tEmb_Norm: 41.80 (44.66)\n",
      "Train Epoch: 3 [161920/251450]\tLoss: 0.0444 (0.0376) \tAcc: 93.75% (95.39%) \tEmb_Norm: 41.78 (44.65)\n",
      "Train Epoch: 3 [162560/251450]\tLoss: 0.0000 (0.0375) \tAcc: 100.00% (95.39%) \tEmb_Norm: 41.76 (44.64)\n",
      "Train Epoch: 3 [163200/251450]\tLoss: 0.0150 (0.0375) \tAcc: 93.75% (95.39%) \tEmb_Norm: 41.73 (44.63)\n",
      "Train Epoch: 3 [163840/251450]\tLoss: 0.0649 (0.0375) \tAcc: 93.75% (95.38%) \tEmb_Norm: 41.71 (44.62)\n",
      "Train Epoch: 3 [164480/251450]\tLoss: 0.0000 (0.0375) \tAcc: 100.00% (95.39%) \tEmb_Norm: 41.69 (44.60)\n",
      "Train Epoch: 3 [165120/251450]\tLoss: 0.0000 (0.0375) \tAcc: 100.00% (95.38%) \tEmb_Norm: 41.67 (44.59)\n",
      "Train Epoch: 3 [165760/251450]\tLoss: 0.1141 (0.0375) \tAcc: 87.50% (95.38%) \tEmb_Norm: 41.64 (44.58)\n",
      "Train Epoch: 3 [166400/251450]\tLoss: 0.1131 (0.0375) \tAcc: 93.75% (95.37%) \tEmb_Norm: 41.62 (44.57)\n",
      "Train Epoch: 3 [167040/251450]\tLoss: 0.0359 (0.0375) \tAcc: 93.75% (95.38%) \tEmb_Norm: 41.60 (44.56)\n",
      "Train Epoch: 3 [167680/251450]\tLoss: 0.1036 (0.0374) \tAcc: 84.38% (95.38%) \tEmb_Norm: 41.57 (44.55)\n",
      "Train Epoch: 3 [168320/251450]\tLoss: 0.0980 (0.0375) \tAcc: 90.62% (95.38%) \tEmb_Norm: 41.55 (44.54)\n",
      "Train Epoch: 3 [168960/251450]\tLoss: 0.0000 (0.0374) \tAcc: 100.00% (95.38%) \tEmb_Norm: 41.53 (44.52)\n",
      "Train Epoch: 3 [169600/251450]\tLoss: 0.0616 (0.0375) \tAcc: 96.88% (95.38%) \tEmb_Norm: 41.51 (44.51)\n",
      "Train Epoch: 3 [170240/251450]\tLoss: 0.0641 (0.0374) \tAcc: 90.62% (95.38%) \tEmb_Norm: 41.48 (44.50)\n",
      "Train Epoch: 3 [170880/251450]\tLoss: 0.0071 (0.0374) \tAcc: 100.00% (95.38%) \tEmb_Norm: 41.46 (44.49)\n",
      "Train Epoch: 3 [171520/251450]\tLoss: 0.0260 (0.0375) \tAcc: 96.88% (95.37%) \tEmb_Norm: 41.43 (44.48)\n",
      "Train Epoch: 3 [172160/251450]\tLoss: 0.0218 (0.0375) \tAcc: 96.88% (95.37%) \tEmb_Norm: 41.41 (44.47)\n",
      "Train Epoch: 3 [172800/251450]\tLoss: 0.0000 (0.0374) \tAcc: 100.00% (95.38%) \tEmb_Norm: 41.39 (44.46)\n",
      "Train Epoch: 3 [173440/251450]\tLoss: 0.0379 (0.0374) \tAcc: 93.75% (95.38%) \tEmb_Norm: 41.37 (44.45)\n",
      "Train Epoch: 3 [174080/251450]\tLoss: 0.0713 (0.0374) \tAcc: 90.62% (95.37%) \tEmb_Norm: 41.34 (44.43)\n",
      "Train Epoch: 3 [174720/251450]\tLoss: 0.0000 (0.0374) \tAcc: 100.00% (95.38%) \tEmb_Norm: 41.32 (44.42)\n",
      "Train Epoch: 3 [175360/251450]\tLoss: 0.0411 (0.0374) \tAcc: 96.88% (95.37%) \tEmb_Norm: 41.30 (44.41)\n",
      "Train Epoch: 3 [176000/251450]\tLoss: 0.0016 (0.0374) \tAcc: 100.00% (95.38%) \tEmb_Norm: 41.28 (44.40)\n",
      "Train Epoch: 3 [176640/251450]\tLoss: 0.1413 (0.0373) \tAcc: 90.62% (95.38%) \tEmb_Norm: 41.26 (44.39)\n",
      "Train Epoch: 3 [177280/251450]\tLoss: 0.0726 (0.0373) \tAcc: 96.88% (95.38%) \tEmb_Norm: 41.23 (44.38)\n",
      "Train Epoch: 3 [177920/251450]\tLoss: 0.0000 (0.0373) \tAcc: 100.00% (95.38%) \tEmb_Norm: 41.21 (44.37)\n",
      "Train Epoch: 3 [178560/251450]\tLoss: 0.0230 (0.0372) \tAcc: 93.75% (95.39%) \tEmb_Norm: 41.19 (44.35)\n",
      "Train Epoch: 3 [179200/251450]\tLoss: 0.0832 (0.0373) \tAcc: 96.88% (95.38%) \tEmb_Norm: 41.17 (44.34)\n",
      "Train Epoch: 3 [179840/251450]\tLoss: 0.0046 (0.0373) \tAcc: 100.00% (95.39%) \tEmb_Norm: 41.14 (44.33)\n",
      "Train Epoch: 3 [180480/251450]\tLoss: 0.0187 (0.0373) \tAcc: 96.88% (95.38%) \tEmb_Norm: 41.12 (44.32)\n",
      "Train Epoch: 3 [181120/251450]\tLoss: 0.0257 (0.0373) \tAcc: 96.88% (95.39%) \tEmb_Norm: 41.10 (44.31)\n",
      "Train Epoch: 3 [181760/251450]\tLoss: 0.0273 (0.0373) \tAcc: 93.75% (95.39%) \tEmb_Norm: 41.08 (44.30)\n",
      "Train Epoch: 3 [182400/251450]\tLoss: 0.0000 (0.0372) \tAcc: 100.00% (95.39%) \tEmb_Norm: 41.05 (44.29)\n",
      "Train Epoch: 3 [183040/251450]\tLoss: 0.0203 (0.0372) \tAcc: 96.88% (95.39%) \tEmb_Norm: 41.03 (44.27)\n",
      "Train Epoch: 3 [183680/251450]\tLoss: 0.0455 (0.0372) \tAcc: 96.88% (95.39%) \tEmb_Norm: 41.01 (44.26)\n",
      "Train Epoch: 3 [184320/251450]\tLoss: 0.0343 (0.0372) \tAcc: 93.75% (95.38%) \tEmb_Norm: 40.98 (44.25)\n",
      "Train Epoch: 3 [184960/251450]\tLoss: 0.0000 (0.0372) \tAcc: 100.00% (95.39%) \tEmb_Norm: 40.96 (44.24)\n",
      "Train Epoch: 3 [185600/251450]\tLoss: 0.0830 (0.0372) \tAcc: 90.62% (95.39%) \tEmb_Norm: 40.94 (44.23)\n",
      "Train Epoch: 3 [186240/251450]\tLoss: 0.0000 (0.0372) \tAcc: 100.00% (95.39%) \tEmb_Norm: 40.92 (44.22)\n",
      "Train Epoch: 3 [186880/251450]\tLoss: 0.0231 (0.0372) \tAcc: 96.88% (95.40%) \tEmb_Norm: 40.89 (44.21)\n",
      "Train Epoch: 3 [187520/251450]\tLoss: 0.0236 (0.0371) \tAcc: 96.88% (95.39%) \tEmb_Norm: 40.87 (44.20)\n",
      "Train Epoch: 3 [188160/251450]\tLoss: 0.0000 (0.0371) \tAcc: 100.00% (95.40%) \tEmb_Norm: 40.85 (44.18)\n",
      "Train Epoch: 3 [188800/251450]\tLoss: 0.0078 (0.0371) \tAcc: 100.00% (95.40%) \tEmb_Norm: 40.83 (44.17)\n",
      "Train Epoch: 3 [189440/251450]\tLoss: 0.0287 (0.0371) \tAcc: 93.75% (95.40%) \tEmb_Norm: 40.80 (44.16)\n",
      "Train Epoch: 3 [190080/251450]\tLoss: 0.0650 (0.0371) \tAcc: 87.50% (95.40%) \tEmb_Norm: 40.78 (44.15)\n",
      "Train Epoch: 3 [190720/251450]\tLoss: 0.0110 (0.0371) \tAcc: 96.88% (95.40%) \tEmb_Norm: 40.76 (44.14)\n",
      "Train Epoch: 3 [191360/251450]\tLoss: 0.0731 (0.0371) \tAcc: 93.75% (95.40%) \tEmb_Norm: 40.74 (44.13)\n",
      "Train Epoch: 3 [192000/251450]\tLoss: 0.0320 (0.0371) \tAcc: 93.75% (95.40%) \tEmb_Norm: 40.71 (44.12)\n",
      "Train Epoch: 3 [192640/251450]\tLoss: 0.0170 (0.0371) \tAcc: 96.88% (95.40%) \tEmb_Norm: 40.69 (44.10)\n",
      "Train Epoch: 3 [193280/251450]\tLoss: 0.0439 (0.0371) \tAcc: 96.88% (95.40%) \tEmb_Norm: 40.67 (44.09)\n",
      "Train Epoch: 3 [193920/251450]\tLoss: 0.0322 (0.0370) \tAcc: 93.75% (95.40%) \tEmb_Norm: 40.65 (44.08)\n",
      "Train Epoch: 3 [194560/251450]\tLoss: 0.0000 (0.0370) \tAcc: 100.00% (95.41%) \tEmb_Norm: 40.63 (44.07)\n",
      "Train Epoch: 3 [195200/251450]\tLoss: 0.0829 (0.0370) \tAcc: 93.75% (95.40%) \tEmb_Norm: 40.60 (44.06)\n",
      "Train Epoch: 3 [195840/251450]\tLoss: 0.0010 (0.0370) \tAcc: 100.00% (95.41%) \tEmb_Norm: 40.58 (44.05)\n",
      "Train Epoch: 3 [196480/251450]\tLoss: 0.0412 (0.0369) \tAcc: 93.75% (95.41%) \tEmb_Norm: 40.56 (44.04)\n",
      "Train Epoch: 3 [197120/251450]\tLoss: 0.0116 (0.0369) \tAcc: 100.00% (95.41%) \tEmb_Norm: 40.54 (44.03)\n",
      "Train Epoch: 3 [197760/251450]\tLoss: 0.0033 (0.0369) \tAcc: 100.00% (95.41%) \tEmb_Norm: 40.51 (44.01)\n",
      "Train Epoch: 3 [198400/251450]\tLoss: 0.0074 (0.0369) \tAcc: 100.00% (95.41%) \tEmb_Norm: 40.49 (44.00)\n",
      "Train Epoch: 3 [199040/251450]\tLoss: 0.0864 (0.0369) \tAcc: 87.50% (95.41%) \tEmb_Norm: 40.47 (43.99)\n",
      "Train Epoch: 3 [199680/251450]\tLoss: 0.0341 (0.0369) \tAcc: 96.88% (95.41%) \tEmb_Norm: 40.44 (43.98)\n",
      "Train Epoch: 3 [200320/251450]\tLoss: 0.0077 (0.0369) \tAcc: 100.00% (95.41%) \tEmb_Norm: 40.42 (43.97)\n",
      "Train Epoch: 3 [200960/251450]\tLoss: 0.0727 (0.0368) \tAcc: 90.62% (95.41%) \tEmb_Norm: 40.40 (43.96)\n",
      "Train Epoch: 3 [201600/251450]\tLoss: 0.0075 (0.0368) \tAcc: 100.00% (95.41%) \tEmb_Norm: 40.38 (43.95)\n",
      "=============== TEST epoch 3 ===============\n",
      "\n",
      "Test set: Average loss: 0.0376, Accuracy: 94.98%\n",
      "\n",
      "=============== checkpoint epoch 3 ===============\n",
      "=============== TRAIN epoch 4 ===============\n",
      "Train Epoch: 4 [0/251450]\tLoss: 0.0589 (0.0589) \tAcc: 90.62% (90.62%) \tEmb_Norm: 40.36 (40.36)\n",
      "Train Epoch: 4 [640/251450]\tLoss: 0.0071 (0.0327) \tAcc: 96.88% (95.39%) \tEmb_Norm: 40.34 (40.35)\n",
      "Train Epoch: 4 [1280/251450]\tLoss: 0.0012 (0.0346) \tAcc: 100.00% (95.05%) \tEmb_Norm: 40.32 (40.34)\n",
      "Train Epoch: 4 [1920/251450]\tLoss: 0.0200 (0.0328) \tAcc: 96.88% (95.39%) \tEmb_Norm: 40.29 (40.33)\n",
      "Train Epoch: 4 [2560/251450]\tLoss: 0.0134 (0.0344) \tAcc: 96.88% (95.10%) \tEmb_Norm: 40.27 (40.32)\n",
      "Train Epoch: 4 [3200/251450]\tLoss: 0.0523 (0.0347) \tAcc: 90.62% (95.11%) \tEmb_Norm: 40.25 (40.31)\n",
      "Train Epoch: 4 [3840/251450]\tLoss: 0.0334 (0.0341) \tAcc: 96.88% (95.17%) \tEmb_Norm: 40.23 (40.29)\n",
      "Train Epoch: 4 [4480/251450]\tLoss: 0.0000 (0.0335) \tAcc: 100.00% (95.35%) \tEmb_Norm: 40.20 (40.28)\n",
      "Train Epoch: 4 [5120/251450]\tLoss: 0.0006 (0.0326) \tAcc: 100.00% (95.38%) \tEmb_Norm: 40.18 (40.27)\n",
      "Train Epoch: 4 [5760/251450]\tLoss: 0.0086 (0.0321) \tAcc: 96.88% (95.48%) \tEmb_Norm: 40.16 (40.26)\n",
      "Train Epoch: 4 [6400/251450]\tLoss: 0.0563 (0.0318) \tAcc: 90.62% (95.48%) \tEmb_Norm: 40.14 (40.25)\n",
      "Train Epoch: 4 [7040/251450]\tLoss: 0.0353 (0.0313) \tAcc: 93.75% (95.57%) \tEmb_Norm: 40.12 (40.24)\n",
      "Train Epoch: 4 [7680/251450]\tLoss: 0.0240 (0.0303) \tAcc: 93.75% (95.63%) \tEmb_Norm: 40.10 (40.23)\n",
      "Train Epoch: 4 [8320/251450]\tLoss: 0.0747 (0.0308) \tAcc: 96.88% (95.65%) \tEmb_Norm: 40.07 (40.22)\n",
      "Train Epoch: 4 [8960/251450]\tLoss: 0.0000 (0.0311) \tAcc: 100.00% (95.64%) \tEmb_Norm: 40.05 (40.21)\n",
      "Train Epoch: 4 [9600/251450]\tLoss: 0.0000 (0.0306) \tAcc: 100.00% (95.67%) \tEmb_Norm: 40.03 (40.19)\n",
      "Train Epoch: 4 [10240/251450]\tLoss: 0.0341 (0.0307) \tAcc: 93.75% (95.65%) \tEmb_Norm: 40.01 (40.18)\n",
      "Train Epoch: 4 [10880/251450]\tLoss: 0.0172 (0.0315) \tAcc: 96.88% (95.59%) \tEmb_Norm: 39.98 (40.17)\n",
      "Train Epoch: 4 [11520/251450]\tLoss: 0.0095 (0.0312) \tAcc: 96.88% (95.59%) \tEmb_Norm: 39.96 (40.16)\n",
      "Train Epoch: 4 [12160/251450]\tLoss: 0.0000 (0.0314) \tAcc: 100.00% (95.60%) \tEmb_Norm: 39.94 (40.15)\n",
      "Train Epoch: 4 [12800/251450]\tLoss: 0.0232 (0.0305) \tAcc: 93.75% (95.72%) \tEmb_Norm: 39.92 (40.14)\n",
      "Train Epoch: 4 [13440/251450]\tLoss: 0.0541 (0.0304) \tAcc: 96.88% (95.70%) \tEmb_Norm: 39.90 (40.13)\n",
      "Train Epoch: 4 [14080/251450]\tLoss: 0.0038 (0.0305) \tAcc: 100.00% (95.74%) \tEmb_Norm: 39.87 (40.12)\n",
      "Train Epoch: 4 [14720/251450]\tLoss: 0.0805 (0.0311) \tAcc: 93.75% (95.69%) \tEmb_Norm: 39.85 (40.11)\n",
      "Train Epoch: 4 [15360/251450]\tLoss: 0.0035 (0.0307) \tAcc: 100.00% (95.73%) \tEmb_Norm: 39.83 (40.09)\n",
      "Train Epoch: 4 [16000/251450]\tLoss: 0.0000 (0.0304) \tAcc: 100.00% (95.78%) \tEmb_Norm: 39.81 (40.08)\n",
      "Train Epoch: 4 [16640/251450]\tLoss: 0.0000 (0.0299) \tAcc: 100.00% (95.84%) \tEmb_Norm: 39.79 (40.07)\n",
      "Train Epoch: 4 [17280/251450]\tLoss: 0.0510 (0.0301) \tAcc: 90.62% (95.79%) \tEmb_Norm: 39.76 (40.06)\n",
      "Train Epoch: 4 [17920/251450]\tLoss: 0.0000 (0.0299) \tAcc: 100.00% (95.79%) \tEmb_Norm: 39.74 (40.05)\n",
      "Train Epoch: 4 [18560/251450]\tLoss: 0.0000 (0.0298) \tAcc: 100.00% (95.83%) \tEmb_Norm: 39.72 (40.04)\n",
      "Train Epoch: 4 [19200/251450]\tLoss: 0.0113 (0.0301) \tAcc: 96.88% (95.78%) \tEmb_Norm: 39.70 (40.03)\n",
      "Train Epoch: 4 [19840/251450]\tLoss: 0.0205 (0.0301) \tAcc: 96.88% (95.79%) \tEmb_Norm: 39.67 (40.02)\n",
      "Train Epoch: 4 [20480/251450]\tLoss: 0.0214 (0.0305) \tAcc: 96.88% (95.77%) \tEmb_Norm: 39.65 (40.01)\n",
      "Train Epoch: 4 [21120/251450]\tLoss: 0.0584 (0.0304) \tAcc: 90.62% (95.79%) \tEmb_Norm: 39.63 (39.99)\n",
      "Train Epoch: 4 [21760/251450]\tLoss: 0.0000 (0.0301) \tAcc: 100.00% (95.82%) \tEmb_Norm: 39.61 (39.98)\n",
      "Train Epoch: 4 [22400/251450]\tLoss: 0.0000 (0.0297) \tAcc: 100.00% (95.86%) \tEmb_Norm: 39.59 (39.97)\n",
      "Train Epoch: 4 [23040/251450]\tLoss: 0.0142 (0.0295) \tAcc: 96.88% (95.89%) \tEmb_Norm: 39.57 (39.96)\n",
      "Train Epoch: 4 [23680/251450]\tLoss: 0.0000 (0.0294) \tAcc: 100.00% (95.91%) \tEmb_Norm: 39.54 (39.95)\n",
      "Train Epoch: 4 [24320/251450]\tLoss: 0.0474 (0.0293) \tAcc: 93.75% (95.90%) \tEmb_Norm: 39.52 (39.94)\n",
      "Train Epoch: 4 [24960/251450]\tLoss: 0.0029 (0.0294) \tAcc: 100.00% (95.89%) \tEmb_Norm: 39.50 (39.93)\n",
      "Train Epoch: 4 [25600/251450]\tLoss: 0.0075 (0.0293) \tAcc: 96.88% (95.90%) \tEmb_Norm: 39.48 (39.92)\n",
      "Train Epoch: 4 [26240/251450]\tLoss: 0.0562 (0.0296) \tAcc: 93.75% (95.88%) \tEmb_Norm: 39.45 (39.91)\n",
      "Train Epoch: 4 [26880/251450]\tLoss: 0.0234 (0.0294) \tAcc: 93.75% (95.90%) \tEmb_Norm: 39.43 (39.90)\n",
      "Train Epoch: 4 [27520/251450]\tLoss: 0.0342 (0.0291) \tAcc: 93.75% (95.92%) \tEmb_Norm: 39.41 (39.88)\n",
      "Train Epoch: 4 [28160/251450]\tLoss: 0.0078 (0.0289) \tAcc: 96.88% (95.94%) \tEmb_Norm: 39.39 (39.87)\n",
      "Train Epoch: 4 [28800/251450]\tLoss: 0.0149 (0.0289) \tAcc: 96.88% (95.96%) \tEmb_Norm: 39.37 (39.86)\n",
      "Train Epoch: 4 [29440/251450]\tLoss: 0.0268 (0.0288) \tAcc: 93.75% (95.96%) \tEmb_Norm: 39.35 (39.85)\n",
      "Train Epoch: 4 [30080/251450]\tLoss: 0.1673 (0.0289) \tAcc: 90.62% (95.96%) \tEmb_Norm: 39.33 (39.84)\n",
      "Train Epoch: 4 [30720/251450]\tLoss: 0.0000 (0.0293) \tAcc: 100.00% (95.94%) \tEmb_Norm: 39.30 (39.83)\n",
      "Train Epoch: 4 [31360/251450]\tLoss: 0.0568 (0.0293) \tAcc: 93.75% (95.94%) \tEmb_Norm: 39.28 (39.82)\n",
      "Train Epoch: 4 [32000/251450]\tLoss: 0.0000 (0.0292) \tAcc: 100.00% (95.95%) \tEmb_Norm: 39.26 (39.81)\n",
      "Train Epoch: 4 [32640/251450]\tLoss: 0.0068 (0.0292) \tAcc: 96.88% (95.93%) \tEmb_Norm: 39.24 (39.80)\n",
      "Train Epoch: 4 [33280/251450]\tLoss: 0.0000 (0.0291) \tAcc: 100.00% (95.93%) \tEmb_Norm: 39.22 (39.79)\n",
      "Train Epoch: 4 [33920/251450]\tLoss: 0.0876 (0.0291) \tAcc: 90.62% (95.94%) \tEmb_Norm: 39.19 (39.78)\n",
      "Train Epoch: 4 [34560/251450]\tLoss: 0.0691 (0.0291) \tAcc: 87.50% (95.94%) \tEmb_Norm: 39.17 (39.76)\n",
      "Train Epoch: 4 [35200/251450]\tLoss: 0.0121 (0.0291) \tAcc: 96.88% (95.93%) \tEmb_Norm: 39.15 (39.75)\n",
      "Train Epoch: 4 [35840/251450]\tLoss: 0.0073 (0.0290) \tAcc: 96.88% (95.94%) \tEmb_Norm: 39.13 (39.74)\n",
      "Train Epoch: 4 [36480/251450]\tLoss: 0.0544 (0.0293) \tAcc: 96.88% (95.92%) \tEmb_Norm: 39.10 (39.73)\n",
      "Train Epoch: 4 [37120/251450]\tLoss: 0.0243 (0.0291) \tAcc: 96.88% (95.94%) \tEmb_Norm: 39.08 (39.72)\n",
      "Train Epoch: 4 [37760/251450]\tLoss: 0.1197 (0.0293) \tAcc: 90.62% (95.94%) \tEmb_Norm: 39.06 (39.71)\n",
      "Train Epoch: 4 [38400/251450]\tLoss: 0.0142 (0.0293) \tAcc: 96.88% (95.93%) \tEmb_Norm: 39.04 (39.70)\n",
      "Train Epoch: 4 [39040/251450]\tLoss: 0.0658 (0.0293) \tAcc: 90.62% (95.92%) \tEmb_Norm: 39.02 (39.69)\n",
      "Train Epoch: 4 [39680/251450]\tLoss: 0.0188 (0.0291) \tAcc: 96.88% (95.93%) \tEmb_Norm: 38.99 (39.68)\n",
      "Train Epoch: 4 [40320/251450]\tLoss: 0.0042 (0.0289) \tAcc: 100.00% (95.97%) \tEmb_Norm: 38.97 (39.67)\n",
      "Train Epoch: 4 [40960/251450]\tLoss: 0.0387 (0.0289) \tAcc: 96.88% (95.98%) \tEmb_Norm: 38.95 (39.65)\n",
      "Train Epoch: 4 [41600/251450]\tLoss: 0.0271 (0.0288) \tAcc: 96.88% (95.98%) \tEmb_Norm: 38.93 (39.64)\n",
      "Train Epoch: 4 [42240/251450]\tLoss: 0.0380 (0.0288) \tAcc: 93.75% (95.98%) \tEmb_Norm: 38.91 (39.63)\n",
      "Train Epoch: 4 [42880/251450]\tLoss: 0.0922 (0.0289) \tAcc: 87.50% (95.97%) \tEmb_Norm: 38.89 (39.62)\n",
      "Train Epoch: 4 [43520/251450]\tLoss: 0.0233 (0.0288) \tAcc: 93.75% (95.97%) \tEmb_Norm: 38.86 (39.61)\n",
      "Train Epoch: 4 [44160/251450]\tLoss: 0.0221 (0.0288) \tAcc: 93.75% (95.97%) \tEmb_Norm: 38.84 (39.60)\n",
      "Train Epoch: 4 [44800/251450]\tLoss: 0.0372 (0.0288) \tAcc: 96.88% (95.98%) \tEmb_Norm: 38.82 (39.59)\n",
      "Train Epoch: 4 [45440/251450]\tLoss: 0.0103 (0.0289) \tAcc: 96.88% (95.97%) \tEmb_Norm: 38.80 (39.58)\n",
      "Train Epoch: 4 [46080/251450]\tLoss: 0.0100 (0.0289) \tAcc: 96.88% (95.98%) \tEmb_Norm: 38.77 (39.57)\n",
      "Train Epoch: 4 [46720/251450]\tLoss: 0.0233 (0.0288) \tAcc: 93.75% (95.99%) \tEmb_Norm: 38.75 (39.56)\n",
      "Train Epoch: 4 [47360/251450]\tLoss: 0.0504 (0.0287) \tAcc: 93.75% (95.99%) \tEmb_Norm: 38.73 (39.54)\n",
      "Train Epoch: 4 [48000/251450]\tLoss: 0.0246 (0.0289) \tAcc: 93.75% (95.96%) \tEmb_Norm: 38.71 (39.53)\n",
      "Train Epoch: 4 [48640/251450]\tLoss: 0.0000 (0.0288) \tAcc: 100.00% (95.98%) \tEmb_Norm: 38.69 (39.52)\n",
      "Train Epoch: 4 [49280/251450]\tLoss: 0.0449 (0.0288) \tAcc: 90.62% (95.97%) \tEmb_Norm: 38.67 (39.51)\n",
      "Train Epoch: 4 [49920/251450]\tLoss: 0.0000 (0.0289) \tAcc: 100.00% (95.97%) \tEmb_Norm: 38.64 (39.50)\n",
      "Train Epoch: 4 [50560/251450]\tLoss: 0.0013 (0.0291) \tAcc: 100.00% (95.95%) \tEmb_Norm: 38.62 (39.49)\n",
      "Train Epoch: 4 [51200/251450]\tLoss: 0.0000 (0.0290) \tAcc: 100.00% (95.96%) \tEmb_Norm: 38.60 (39.48)\n",
      "Train Epoch: 4 [51840/251450]\tLoss: 0.0417 (0.0291) \tAcc: 96.88% (95.95%) \tEmb_Norm: 38.57 (39.47)\n",
      "Train Epoch: 4 [52480/251450]\tLoss: 0.0622 (0.0291) \tAcc: 90.62% (95.95%) \tEmb_Norm: 38.55 (39.46)\n",
      "Train Epoch: 4 [53120/251450]\tLoss: 0.0908 (0.0291) \tAcc: 90.62% (95.95%) \tEmb_Norm: 38.53 (39.45)\n",
      "Train Epoch: 4 [53760/251450]\tLoss: 0.0047 (0.0291) \tAcc: 100.00% (95.95%) \tEmb_Norm: 38.51 (39.43)\n",
      "Train Epoch: 4 [54400/251450]\tLoss: 0.0111 (0.0292) \tAcc: 96.88% (95.95%) \tEmb_Norm: 38.48 (39.42)\n",
      "Train Epoch: 4 [55040/251450]\tLoss: 0.0541 (0.0292) \tAcc: 93.75% (95.95%) \tEmb_Norm: 38.46 (39.41)\n",
      "Train Epoch: 4 [55680/251450]\tLoss: 0.0000 (0.0292) \tAcc: 100.00% (95.95%) \tEmb_Norm: 38.44 (39.40)\n",
      "Train Epoch: 4 [56320/251450]\tLoss: 0.0232 (0.0292) \tAcc: 96.88% (95.95%) \tEmb_Norm: 38.42 (39.39)\n",
      "Train Epoch: 4 [56960/251450]\tLoss: 0.0000 (0.0293) \tAcc: 100.00% (95.94%) \tEmb_Norm: 38.39 (39.38)\n",
      "Train Epoch: 4 [57600/251450]\tLoss: 0.0000 (0.0292) \tAcc: 100.00% (95.95%) \tEmb_Norm: 38.37 (39.37)\n",
      "Train Epoch: 4 [58240/251450]\tLoss: 0.0251 (0.0293) \tAcc: 93.75% (95.94%) \tEmb_Norm: 38.35 (39.36)\n",
      "Train Epoch: 4 [58880/251450]\tLoss: 0.0109 (0.0293) \tAcc: 96.88% (95.95%) \tEmb_Norm: 38.33 (39.35)\n",
      "Train Epoch: 4 [59520/251450]\tLoss: 0.0010 (0.0293) \tAcc: 100.00% (95.95%) \tEmb_Norm: 38.31 (39.33)\n",
      "Train Epoch: 4 [60160/251450]\tLoss: 0.0223 (0.0293) \tAcc: 96.88% (95.94%) \tEmb_Norm: 38.28 (39.32)\n",
      "Train Epoch: 4 [60800/251450]\tLoss: 0.0795 (0.0293) \tAcc: 96.88% (95.94%) \tEmb_Norm: 38.26 (39.31)\n",
      "Train Epoch: 4 [61440/251450]\tLoss: 0.0060 (0.0293) \tAcc: 100.00% (95.94%) \tEmb_Norm: 38.24 (39.30)\n",
      "Train Epoch: 4 [62080/251450]\tLoss: 0.0263 (0.0292) \tAcc: 93.75% (95.95%) \tEmb_Norm: 38.22 (39.29)\n",
      "Train Epoch: 4 [62720/251450]\tLoss: 0.0028 (0.0292) \tAcc: 100.00% (95.95%) \tEmb_Norm: 38.20 (39.28)\n",
      "Train Epoch: 4 [63360/251450]\tLoss: 0.0390 (0.0293) \tAcc: 93.75% (95.93%) \tEmb_Norm: 38.17 (39.27)\n",
      "Train Epoch: 4 [64000/251450]\tLoss: 0.0362 (0.0294) \tAcc: 93.75% (95.93%) \tEmb_Norm: 38.15 (39.26)\n",
      "Train Epoch: 4 [64640/251450]\tLoss: 0.0000 (0.0295) \tAcc: 100.00% (95.93%) \tEmb_Norm: 38.13 (39.25)\n",
      "Train Epoch: 4 [65280/251450]\tLoss: 0.0550 (0.0294) \tAcc: 93.75% (95.93%) \tEmb_Norm: 38.10 (39.24)\n",
      "Train Epoch: 4 [65920/251450]\tLoss: 0.0166 (0.0294) \tAcc: 96.88% (95.93%) \tEmb_Norm: 38.08 (39.22)\n",
      "Train Epoch: 4 [66560/251450]\tLoss: 0.0512 (0.0294) \tAcc: 93.75% (95.92%) \tEmb_Norm: 38.06 (39.21)\n",
      "Train Epoch: 4 [67200/251450]\tLoss: 0.0612 (0.0294) \tAcc: 90.62% (95.93%) \tEmb_Norm: 38.04 (39.20)\n",
      "Train Epoch: 4 [67840/251450]\tLoss: 0.0353 (0.0294) \tAcc: 96.88% (95.94%) \tEmb_Norm: 38.02 (39.19)\n",
      "Train Epoch: 4 [68480/251450]\tLoss: 0.0465 (0.0294) \tAcc: 93.75% (95.93%) \tEmb_Norm: 37.99 (39.18)\n",
      "Train Epoch: 4 [69120/251450]\tLoss: 0.0268 (0.0293) \tAcc: 96.88% (95.94%) \tEmb_Norm: 37.97 (39.17)\n",
      "Train Epoch: 4 [69760/251450]\tLoss: 0.0197 (0.0293) \tAcc: 96.88% (95.94%) \tEmb_Norm: 37.95 (39.16)\n",
      "Train Epoch: 4 [70400/251450]\tLoss: 0.0503 (0.0293) \tAcc: 93.75% (95.94%) \tEmb_Norm: 37.93 (39.15)\n",
      "Train Epoch: 4 [71040/251450]\tLoss: 0.0556 (0.0293) \tAcc: 87.50% (95.94%) \tEmb_Norm: 37.91 (39.14)\n",
      "Train Epoch: 4 [71680/251450]\tLoss: 0.0032 (0.0292) \tAcc: 100.00% (95.94%) \tEmb_Norm: 37.88 (39.12)\n",
      "Train Epoch: 4 [72320/251450]\tLoss: 0.0012 (0.0291) \tAcc: 100.00% (95.95%) \tEmb_Norm: 37.86 (39.11)\n",
      "Train Epoch: 4 [72960/251450]\tLoss: 0.0694 (0.0291) \tAcc: 87.50% (95.95%) \tEmb_Norm: 37.84 (39.10)\n",
      "Train Epoch: 4 [73600/251450]\tLoss: 0.0284 (0.0291) \tAcc: 93.75% (95.95%) \tEmb_Norm: 37.82 (39.09)\n",
      "Train Epoch: 4 [74240/251450]\tLoss: 0.0210 (0.0291) \tAcc: 96.88% (95.95%) \tEmb_Norm: 37.80 (39.08)\n",
      "Train Epoch: 4 [74880/251450]\tLoss: 0.0233 (0.0291) \tAcc: 93.75% (95.96%) \tEmb_Norm: 37.78 (39.07)\n",
      "Train Epoch: 4 [75520/251450]\tLoss: 0.0669 (0.0291) \tAcc: 93.75% (95.96%) \tEmb_Norm: 37.75 (39.06)\n",
      "Train Epoch: 4 [76160/251450]\tLoss: 0.0218 (0.0290) \tAcc: 93.75% (95.97%) \tEmb_Norm: 37.73 (39.05)\n",
      "Train Epoch: 4 [76800/251450]\tLoss: 0.0129 (0.0290) \tAcc: 96.88% (95.97%) \tEmb_Norm: 37.71 (39.04)\n",
      "Train Epoch: 4 [77440/251450]\tLoss: 0.0251 (0.0290) \tAcc: 93.75% (95.97%) \tEmb_Norm: 37.69 (39.02)\n",
      "Train Epoch: 4 [78080/251450]\tLoss: 0.0093 (0.0289) \tAcc: 100.00% (95.97%) \tEmb_Norm: 37.67 (39.01)\n",
      "Train Epoch: 4 [78720/251450]\tLoss: 0.0001 (0.0288) \tAcc: 100.00% (95.99%) \tEmb_Norm: 37.65 (39.00)\n",
      "Train Epoch: 4 [79360/251450]\tLoss: 0.0441 (0.0288) \tAcc: 93.75% (95.99%) \tEmb_Norm: 37.63 (38.99)\n",
      "Train Epoch: 4 [80000/251450]\tLoss: 0.0640 (0.0288) \tAcc: 93.75% (96.00%) \tEmb_Norm: 37.60 (38.98)\n",
      "Train Epoch: 4 [80640/251450]\tLoss: 0.1082 (0.0288) \tAcc: 90.62% (96.00%) \tEmb_Norm: 37.58 (38.97)\n",
      "Train Epoch: 4 [81280/251450]\tLoss: 0.0295 (0.0288) \tAcc: 96.88% (96.00%) \tEmb_Norm: 37.56 (38.96)\n",
      "Train Epoch: 4 [81920/251450]\tLoss: 0.0632 (0.0288) \tAcc: 93.75% (96.00%) \tEmb_Norm: 37.54 (38.95)\n",
      "Train Epoch: 4 [82560/251450]\tLoss: 0.0803 (0.0288) \tAcc: 87.50% (95.99%) \tEmb_Norm: 37.51 (38.94)\n",
      "Train Epoch: 4 [83200/251450]\tLoss: 0.0440 (0.0289) \tAcc: 93.75% (95.99%) \tEmb_Norm: 37.49 (38.93)\n",
      "Train Epoch: 4 [83840/251450]\tLoss: 0.0021 (0.0289) \tAcc: 100.00% (95.99%) \tEmb_Norm: 37.47 (38.91)\n",
      "Train Epoch: 4 [84480/251450]\tLoss: 0.0462 (0.0289) \tAcc: 90.62% (95.98%) \tEmb_Norm: 37.45 (38.90)\n",
      "Train Epoch: 4 [85120/251450]\tLoss: 0.0345 (0.0290) \tAcc: 93.75% (95.97%) \tEmb_Norm: 37.42 (38.89)\n",
      "Train Epoch: 4 [85760/251450]\tLoss: 0.1165 (0.0290) \tAcc: 84.38% (95.97%) \tEmb_Norm: 37.40 (38.88)\n",
      "Train Epoch: 4 [86400/251450]\tLoss: 0.0000 (0.0290) \tAcc: 100.00% (95.97%) \tEmb_Norm: 37.38 (38.87)\n",
      "Train Epoch: 4 [87040/251450]\tLoss: 0.0301 (0.0290) \tAcc: 93.75% (95.97%) \tEmb_Norm: 37.36 (38.86)\n",
      "Train Epoch: 4 [87680/251450]\tLoss: 0.0690 (0.0289) \tAcc: 90.62% (95.96%) \tEmb_Norm: 37.34 (38.85)\n",
      "Train Epoch: 4 [88320/251450]\tLoss: 0.0147 (0.0289) \tAcc: 96.88% (95.97%) \tEmb_Norm: 37.31 (38.84)\n",
      "Train Epoch: 4 [88960/251450]\tLoss: 0.0165 (0.0289) \tAcc: 96.88% (95.97%) \tEmb_Norm: 37.29 (38.83)\n",
      "Train Epoch: 4 [89600/251450]\tLoss: 0.0036 (0.0289) \tAcc: 100.00% (95.98%) \tEmb_Norm: 37.27 (38.82)\n",
      "Train Epoch: 4 [90240/251450]\tLoss: 0.0395 (0.0289) \tAcc: 96.88% (95.97%) \tEmb_Norm: 37.25 (38.80)\n",
      "Train Epoch: 4 [90880/251450]\tLoss: 0.0420 (0.0289) \tAcc: 90.62% (95.97%) \tEmb_Norm: 37.23 (38.79)\n",
      "Train Epoch: 4 [91520/251450]\tLoss: 0.0406 (0.0289) \tAcc: 93.75% (95.96%) \tEmb_Norm: 37.20 (38.78)\n",
      "Train Epoch: 4 [92160/251450]\tLoss: 0.0226 (0.0288) \tAcc: 93.75% (95.97%) \tEmb_Norm: 37.18 (38.77)\n",
      "Train Epoch: 4 [92800/251450]\tLoss: 0.0087 (0.0288) \tAcc: 96.88% (95.97%) \tEmb_Norm: 37.16 (38.76)\n",
      "Train Epoch: 4 [93440/251450]\tLoss: 0.0050 (0.0287) \tAcc: 100.00% (95.98%) \tEmb_Norm: 37.14 (38.75)\n",
      "Train Epoch: 4 [94080/251450]\tLoss: 0.0000 (0.0288) \tAcc: 100.00% (95.98%) \tEmb_Norm: 37.12 (38.74)\n",
      "Train Epoch: 4 [94720/251450]\tLoss: 0.0411 (0.0288) \tAcc: 96.88% (95.97%) \tEmb_Norm: 37.10 (38.73)\n",
      "Train Epoch: 4 [95360/251450]\tLoss: 0.0667 (0.0288) \tAcc: 93.75% (95.98%) \tEmb_Norm: 37.07 (38.72)\n",
      "Train Epoch: 4 [96000/251450]\tLoss: 0.0058 (0.0287) \tAcc: 100.00% (95.99%) \tEmb_Norm: 37.05 (38.71)\n",
      "Train Epoch: 4 [96640/251450]\tLoss: 0.0598 (0.0286) \tAcc: 96.88% (95.99%) \tEmb_Norm: 37.03 (38.69)\n",
      "Train Epoch: 4 [97280/251450]\tLoss: 0.0082 (0.0286) \tAcc: 96.88% (95.99%) \tEmb_Norm: 37.01 (38.68)\n",
      "Train Epoch: 4 [97920/251450]\tLoss: 0.0002 (0.0285) \tAcc: 100.00% (96.00%) \tEmb_Norm: 36.99 (38.67)\n",
      "Train Epoch: 4 [98560/251450]\tLoss: 0.0243 (0.0285) \tAcc: 93.75% (96.00%) \tEmb_Norm: 36.97 (38.66)\n",
      "Train Epoch: 4 [99200/251450]\tLoss: 0.0739 (0.0285) \tAcc: 90.62% (96.00%) \tEmb_Norm: 36.95 (38.65)\n",
      "Train Epoch: 4 [99840/251450]\tLoss: 0.0280 (0.0285) \tAcc: 96.88% (96.00%) \tEmb_Norm: 36.93 (38.64)\n",
      "Train Epoch: 4 [100480/251450]\tLoss: 0.0785 (0.0285) \tAcc: 96.88% (96.00%) \tEmb_Norm: 36.90 (38.63)\n",
      "Train Epoch: 4 [101120/251450]\tLoss: 0.0667 (0.0285) \tAcc: 93.75% (96.00%) \tEmb_Norm: 36.88 (38.62)\n",
      "Train Epoch: 4 [101760/251450]\tLoss: 0.0916 (0.0285) \tAcc: 87.50% (96.00%) \tEmb_Norm: 36.86 (38.61)\n",
      "Train Epoch: 4 [102400/251450]\tLoss: 0.0393 (0.0285) \tAcc: 93.75% (96.00%) \tEmb_Norm: 36.84 (38.60)\n",
      "Train Epoch: 4 [103040/251450]\tLoss: 0.0081 (0.0285) \tAcc: 100.00% (96.00%) \tEmb_Norm: 36.82 (38.58)\n",
      "Train Epoch: 4 [103680/251450]\tLoss: 0.0472 (0.0285) \tAcc: 90.62% (95.99%) \tEmb_Norm: 36.79 (38.57)\n",
      "Train Epoch: 4 [104320/251450]\tLoss: 0.0485 (0.0285) \tAcc: 96.88% (95.99%) \tEmb_Norm: 36.77 (38.56)\n",
      "Train Epoch: 4 [104960/251450]\tLoss: 0.0595 (0.0284) \tAcc: 90.62% (95.99%) \tEmb_Norm: 36.75 (38.55)\n",
      "Train Epoch: 4 [105600/251450]\tLoss: 0.1183 (0.0285) \tAcc: 90.62% (95.98%) \tEmb_Norm: 36.73 (38.54)\n",
      "Train Epoch: 4 [106240/251450]\tLoss: 0.0069 (0.0285) \tAcc: 96.88% (95.98%) \tEmb_Norm: 36.70 (38.53)\n",
      "Train Epoch: 4 [106880/251450]\tLoss: 0.0000 (0.0286) \tAcc: 100.00% (95.98%) \tEmb_Norm: 36.68 (38.52)\n",
      "Train Epoch: 4 [107520/251450]\tLoss: 0.0183 (0.0286) \tAcc: 93.75% (95.98%) \tEmb_Norm: 36.66 (38.51)\n",
      "Train Epoch: 4 [108160/251450]\tLoss: 0.0797 (0.0286) \tAcc: 87.50% (95.98%) \tEmb_Norm: 36.64 (38.50)\n",
      "Train Epoch: 4 [108800/251450]\tLoss: 0.0144 (0.0286) \tAcc: 96.88% (95.98%) \tEmb_Norm: 36.61 (38.49)\n",
      "Train Epoch: 4 [109440/251450]\tLoss: 0.0012 (0.0285) \tAcc: 100.00% (95.98%) \tEmb_Norm: 36.59 (38.47)\n",
      "Train Epoch: 4 [110080/251450]\tLoss: 0.0395 (0.0286) \tAcc: 93.75% (95.98%) \tEmb_Norm: 36.57 (38.46)\n",
      "Train Epoch: 4 [110720/251450]\tLoss: 0.0186 (0.0285) \tAcc: 96.88% (95.98%) \tEmb_Norm: 36.55 (38.45)\n",
      "Train Epoch: 4 [111360/251450]\tLoss: 0.0353 (0.0285) \tAcc: 93.75% (95.99%) \tEmb_Norm: 36.53 (38.44)\n",
      "Train Epoch: 4 [112000/251450]\tLoss: 0.0011 (0.0285) \tAcc: 100.00% (95.99%) \tEmb_Norm: 36.51 (38.43)\n",
      "Train Epoch: 4 [112640/251450]\tLoss: 0.0187 (0.0285) \tAcc: 96.88% (95.98%) \tEmb_Norm: 36.48 (38.42)\n",
      "Train Epoch: 4 [113280/251450]\tLoss: 0.0493 (0.0285) \tAcc: 93.75% (95.98%) \tEmb_Norm: 36.46 (38.41)\n",
      "Train Epoch: 4 [113920/251450]\tLoss: 0.0000 (0.0284) \tAcc: 100.00% (95.99%) \tEmb_Norm: 36.44 (38.40)\n",
      "Train Epoch: 4 [114560/251450]\tLoss: 0.0000 (0.0284) \tAcc: 100.00% (95.99%) \tEmb_Norm: 36.42 (38.39)\n",
      "Train Epoch: 4 [115200/251450]\tLoss: 0.0319 (0.0284) \tAcc: 96.88% (96.00%) \tEmb_Norm: 36.40 (38.38)\n",
      "Train Epoch: 4 [115840/251450]\tLoss: 0.1016 (0.0283) \tAcc: 93.75% (96.00%) \tEmb_Norm: 36.38 (38.36)\n",
      "Train Epoch: 4 [116480/251450]\tLoss: 0.0275 (0.0284) \tAcc: 93.75% (95.99%) \tEmb_Norm: 36.35 (38.35)\n",
      "Train Epoch: 4 [117120/251450]\tLoss: 0.0464 (0.0283) \tAcc: 93.75% (95.99%) \tEmb_Norm: 36.33 (38.34)\n",
      "Train Epoch: 4 [117760/251450]\tLoss: 0.0360 (0.0284) \tAcc: 93.75% (95.99%) \tEmb_Norm: 36.31 (38.33)\n",
      "Train Epoch: 4 [118400/251450]\tLoss: 0.0151 (0.0283) \tAcc: 96.88% (96.00%) \tEmb_Norm: 36.29 (38.32)\n",
      "Train Epoch: 4 [119040/251450]\tLoss: 0.0000 (0.0283) \tAcc: 100.00% (96.00%) \tEmb_Norm: 36.27 (38.31)\n",
      "Train Epoch: 4 [119680/251450]\tLoss: 0.0208 (0.0284) \tAcc: 96.88% (95.99%) \tEmb_Norm: 36.24 (38.30)\n",
      "Train Epoch: 4 [120320/251450]\tLoss: 0.0000 (0.0283) \tAcc: 100.00% (96.00%) \tEmb_Norm: 36.22 (38.29)\n",
      "Train Epoch: 4 [120960/251450]\tLoss: 0.0000 (0.0283) \tAcc: 100.00% (96.00%) \tEmb_Norm: 36.20 (38.28)\n",
      "Train Epoch: 4 [121600/251450]\tLoss: 0.0000 (0.0282) \tAcc: 100.00% (96.01%) \tEmb_Norm: 36.18 (38.27)\n",
      "Train Epoch: 4 [122240/251450]\tLoss: 0.0461 (0.0282) \tAcc: 93.75% (96.01%) \tEmb_Norm: 36.16 (38.25)\n",
      "Train Epoch: 4 [122880/251450]\tLoss: 0.0209 (0.0282) \tAcc: 96.88% (96.00%) \tEmb_Norm: 36.14 (38.24)\n",
      "Train Epoch: 4 [123520/251450]\tLoss: 0.0249 (0.0282) \tAcc: 93.75% (96.00%) \tEmb_Norm: 36.12 (38.23)\n",
      "Train Epoch: 4 [124160/251450]\tLoss: 0.0000 (0.0281) \tAcc: 100.00% (96.01%) \tEmb_Norm: 36.09 (38.22)\n",
      "Train Epoch: 4 [124800/251450]\tLoss: 0.0288 (0.0281) \tAcc: 96.88% (96.01%) \tEmb_Norm: 36.07 (38.21)\n",
      "Train Epoch: 4 [125440/251450]\tLoss: 0.0779 (0.0281) \tAcc: 93.75% (96.01%) \tEmb_Norm: 36.05 (38.20)\n",
      "Train Epoch: 4 [126080/251450]\tLoss: 0.0512 (0.0282) \tAcc: 93.75% (96.01%) \tEmb_Norm: 36.03 (38.19)\n",
      "Train Epoch: 4 [126720/251450]\tLoss: 0.0483 (0.0281) \tAcc: 96.88% (96.01%) \tEmb_Norm: 36.01 (38.18)\n",
      "Train Epoch: 4 [127360/251450]\tLoss: 0.0032 (0.0281) \tAcc: 100.00% (96.01%) \tEmb_Norm: 35.98 (38.17)\n",
      "Train Epoch: 4 [128000/251450]\tLoss: 0.0744 (0.0282) \tAcc: 93.75% (96.01%) \tEmb_Norm: 35.96 (38.16)\n",
      "Train Epoch: 4 [128640/251450]\tLoss: 0.0168 (0.0281) \tAcc: 96.88% (96.01%) \tEmb_Norm: 35.94 (38.14)\n",
      "Train Epoch: 4 [129280/251450]\tLoss: 0.0177 (0.0281) \tAcc: 96.88% (96.01%) \tEmb_Norm: 35.92 (38.13)\n",
      "Train Epoch: 4 [129920/251450]\tLoss: 0.0156 (0.0281) \tAcc: 96.88% (96.01%) \tEmb_Norm: 35.90 (38.12)\n",
      "Train Epoch: 4 [130560/251450]\tLoss: 0.0000 (0.0281) \tAcc: 100.00% (96.02%) \tEmb_Norm: 35.87 (38.11)\n",
      "Train Epoch: 4 [131200/251450]\tLoss: 0.0000 (0.0280) \tAcc: 100.00% (96.03%) \tEmb_Norm: 35.85 (38.10)\n",
      "Train Epoch: 4 [131840/251450]\tLoss: 0.0387 (0.0279) \tAcc: 96.88% (96.04%) \tEmb_Norm: 35.83 (38.09)\n",
      "Train Epoch: 4 [132480/251450]\tLoss: 0.0187 (0.0280) \tAcc: 96.88% (96.04%) \tEmb_Norm: 35.81 (38.08)\n",
      "Train Epoch: 4 [133120/251450]\tLoss: 0.0000 (0.0279) \tAcc: 100.00% (96.04%) \tEmb_Norm: 35.79 (38.07)\n",
      "Train Epoch: 4 [133760/251450]\tLoss: 0.0735 (0.0279) \tAcc: 87.50% (96.05%) \tEmb_Norm: 35.77 (38.06)\n",
      "Train Epoch: 4 [134400/251450]\tLoss: 0.0016 (0.0279) \tAcc: 100.00% (96.04%) \tEmb_Norm: 35.75 (38.05)\n",
      "Train Epoch: 4 [135040/251450]\tLoss: 0.0000 (0.0279) \tAcc: 100.00% (96.04%) \tEmb_Norm: 35.72 (38.04)\n",
      "Train Epoch: 4 [135680/251450]\tLoss: 0.0000 (0.0279) \tAcc: 100.00% (96.04%) \tEmb_Norm: 35.70 (38.02)\n",
      "Train Epoch: 4 [136320/251450]\tLoss: 0.0340 (0.0279) \tAcc: 90.62% (96.04%) \tEmb_Norm: 35.68 (38.01)\n",
      "Train Epoch: 4 [136960/251450]\tLoss: 0.0234 (0.0279) \tAcc: 96.88% (96.04%) \tEmb_Norm: 35.66 (38.00)\n",
      "Train Epoch: 4 [137600/251450]\tLoss: 0.0000 (0.0278) \tAcc: 100.00% (96.04%) \tEmb_Norm: 35.64 (37.99)\n",
      "Train Epoch: 4 [138240/251450]\tLoss: 0.0005 (0.0278) \tAcc: 100.00% (96.05%) \tEmb_Norm: 35.62 (37.98)\n",
      "Train Epoch: 4 [138880/251450]\tLoss: 0.1074 (0.0278) \tAcc: 87.50% (96.04%) \tEmb_Norm: 35.59 (37.97)\n",
      "Train Epoch: 4 [139520/251450]\tLoss: 0.0177 (0.0278) \tAcc: 96.88% (96.04%) \tEmb_Norm: 35.57 (37.96)\n",
      "Train Epoch: 4 [140160/251450]\tLoss: 0.0000 (0.0278) \tAcc: 100.00% (96.04%) \tEmb_Norm: 35.55 (37.95)\n",
      "Train Epoch: 4 [140800/251450]\tLoss: 0.0000 (0.0277) \tAcc: 100.00% (96.05%) \tEmb_Norm: 35.53 (37.94)\n",
      "Train Epoch: 4 [141440/251450]\tLoss: 0.0291 (0.0277) \tAcc: 93.75% (96.05%) \tEmb_Norm: 35.51 (37.93)\n",
      "Train Epoch: 4 [142080/251450]\tLoss: 0.0098 (0.0278) \tAcc: 96.88% (96.04%) \tEmb_Norm: 35.49 (37.91)\n",
      "Train Epoch: 4 [142720/251450]\tLoss: 0.0124 (0.0278) \tAcc: 96.88% (96.04%) \tEmb_Norm: 35.46 (37.90)\n",
      "Train Epoch: 4 [143360/251450]\tLoss: 0.0202 (0.0278) \tAcc: 93.75% (96.04%) \tEmb_Norm: 35.44 (37.89)\n",
      "Train Epoch: 4 [144000/251450]\tLoss: 0.0906 (0.0278) \tAcc: 87.50% (96.04%) \tEmb_Norm: 35.42 (37.88)\n",
      "Train Epoch: 4 [144640/251450]\tLoss: 0.0013 (0.0277) \tAcc: 100.00% (96.04%) \tEmb_Norm: 35.40 (37.87)\n",
      "Train Epoch: 4 [145280/251450]\tLoss: 0.0226 (0.0277) \tAcc: 93.75% (96.04%) \tEmb_Norm: 35.38 (37.86)\n",
      "Train Epoch: 4 [145920/251450]\tLoss: 0.0000 (0.0277) \tAcc: 100.00% (96.04%) \tEmb_Norm: 35.35 (37.85)\n",
      "Train Epoch: 4 [146560/251450]\tLoss: 0.0453 (0.0277) \tAcc: 90.62% (96.04%) \tEmb_Norm: 35.33 (37.84)\n",
      "Train Epoch: 4 [147200/251450]\tLoss: 0.0411 (0.0277) \tAcc: 93.75% (96.04%) \tEmb_Norm: 35.31 (37.83)\n",
      "Train Epoch: 4 [147840/251450]\tLoss: 0.0315 (0.0277) \tAcc: 96.88% (96.04%) \tEmb_Norm: 35.29 (37.82)\n",
      "Train Epoch: 4 [148480/251450]\tLoss: 0.0533 (0.0277) \tAcc: 90.62% (96.03%) \tEmb_Norm: 35.27 (37.81)\n",
      "Train Epoch: 4 [149120/251450]\tLoss: 0.0000 (0.0277) \tAcc: 100.00% (96.03%) \tEmb_Norm: 35.24 (37.79)\n",
      "Train Epoch: 4 [149760/251450]\tLoss: 0.0397 (0.0277) \tAcc: 93.75% (96.04%) \tEmb_Norm: 35.22 (37.78)\n",
      "Train Epoch: 4 [150400/251450]\tLoss: 0.0267 (0.0277) \tAcc: 93.75% (96.04%) \tEmb_Norm: 35.20 (37.77)\n",
      "Train Epoch: 4 [151040/251450]\tLoss: 0.1189 (0.0278) \tAcc: 90.62% (96.03%) \tEmb_Norm: 35.18 (37.76)\n",
      "Train Epoch: 4 [151680/251450]\tLoss: 0.0151 (0.0278) \tAcc: 96.88% (96.03%) \tEmb_Norm: 35.16 (37.75)\n",
      "Train Epoch: 4 [152320/251450]\tLoss: 0.0611 (0.0277) \tAcc: 90.62% (96.03%) \tEmb_Norm: 35.13 (37.74)\n",
      "Train Epoch: 4 [152960/251450]\tLoss: 0.0151 (0.0277) \tAcc: 96.88% (96.03%) \tEmb_Norm: 35.11 (37.73)\n",
      "Train Epoch: 4 [153600/251450]\tLoss: 0.0074 (0.0277) \tAcc: 96.88% (96.03%) \tEmb_Norm: 35.09 (37.72)\n",
      "Train Epoch: 4 [154240/251450]\tLoss: 0.0451 (0.0277) \tAcc: 96.88% (96.03%) \tEmb_Norm: 35.07 (37.71)\n",
      "Train Epoch: 4 [154880/251450]\tLoss: 0.0252 (0.0277) \tAcc: 96.88% (96.04%) \tEmb_Norm: 35.05 (37.70)\n",
      "Train Epoch: 4 [155520/251450]\tLoss: 0.0012 (0.0277) \tAcc: 100.00% (96.03%) \tEmb_Norm: 35.03 (37.69)\n",
      "Train Epoch: 4 [156160/251450]\tLoss: 0.0175 (0.0278) \tAcc: 96.88% (96.02%) \tEmb_Norm: 35.00 (37.67)\n",
      "Train Epoch: 4 [156800/251450]\tLoss: 0.0298 (0.0277) \tAcc: 90.62% (96.03%) \tEmb_Norm: 34.98 (37.66)\n",
      "Train Epoch: 4 [157440/251450]\tLoss: 0.0660 (0.0276) \tAcc: 93.75% (96.04%) \tEmb_Norm: 34.96 (37.65)\n",
      "Train Epoch: 4 [158080/251450]\tLoss: 0.0038 (0.0276) \tAcc: 100.00% (96.04%) \tEmb_Norm: 34.94 (37.64)\n",
      "Train Epoch: 4 [158720/251450]\tLoss: 0.0415 (0.0276) \tAcc: 96.88% (96.04%) \tEmb_Norm: 34.92 (37.63)\n",
      "Train Epoch: 4 [159360/251450]\tLoss: 0.0000 (0.0277) \tAcc: 100.00% (96.04%) \tEmb_Norm: 34.89 (37.62)\n",
      "Train Epoch: 4 [160000/251450]\tLoss: 0.0158 (0.0277) \tAcc: 96.88% (96.04%) \tEmb_Norm: 34.87 (37.61)\n",
      "Train Epoch: 4 [160640/251450]\tLoss: 0.0016 (0.0277) \tAcc: 100.00% (96.04%) \tEmb_Norm: 34.85 (37.60)\n",
      "Train Epoch: 4 [161280/251450]\tLoss: 0.0000 (0.0276) \tAcc: 100.00% (96.04%) \tEmb_Norm: 34.83 (37.59)\n",
      "Train Epoch: 4 [161920/251450]\tLoss: 0.0000 (0.0277) \tAcc: 100.00% (96.05%) \tEmb_Norm: 34.81 (37.58)\n",
      "Train Epoch: 4 [162560/251450]\tLoss: 0.0019 (0.0277) \tAcc: 100.00% (96.04%) \tEmb_Norm: 34.78 (37.56)\n",
      "Train Epoch: 4 [163200/251450]\tLoss: 0.0129 (0.0277) \tAcc: 96.88% (96.04%) \tEmb_Norm: 34.76 (37.55)\n",
      "Train Epoch: 4 [163840/251450]\tLoss: 0.0108 (0.0277) \tAcc: 96.88% (96.05%) \tEmb_Norm: 34.74 (37.54)\n",
      "Train Epoch: 4 [164480/251450]\tLoss: 0.0215 (0.0276) \tAcc: 96.88% (96.05%) \tEmb_Norm: 34.72 (37.53)\n",
      "Train Epoch: 4 [165120/251450]\tLoss: 0.0361 (0.0276) \tAcc: 93.75% (96.05%) \tEmb_Norm: 34.70 (37.52)\n",
      "Train Epoch: 4 [165760/251450]\tLoss: 0.0388 (0.0277) \tAcc: 93.75% (96.04%) \tEmb_Norm: 34.67 (37.51)\n",
      "Train Epoch: 4 [166400/251450]\tLoss: 0.0000 (0.0276) \tAcc: 100.00% (96.05%) \tEmb_Norm: 34.65 (37.50)\n",
      "Train Epoch: 4 [167040/251450]\tLoss: 0.0000 (0.0276) \tAcc: 100.00% (96.05%) \tEmb_Norm: 34.63 (37.49)\n",
      "Train Epoch: 4 [167680/251450]\tLoss: 0.0250 (0.0276) \tAcc: 93.75% (96.05%) \tEmb_Norm: 34.61 (37.48)\n",
      "Train Epoch: 4 [168320/251450]\tLoss: 0.0008 (0.0276) \tAcc: 100.00% (96.06%) \tEmb_Norm: 34.59 (37.47)\n",
      "Train Epoch: 4 [168960/251450]\tLoss: 0.0560 (0.0276) \tAcc: 90.62% (96.06%) \tEmb_Norm: 34.56 (37.46)\n",
      "Train Epoch: 4 [169600/251450]\tLoss: 0.0187 (0.0275) \tAcc: 96.88% (96.06%) \tEmb_Norm: 34.54 (37.44)\n",
      "Train Epoch: 4 [170240/251450]\tLoss: 0.0404 (0.0275) \tAcc: 90.62% (96.06%) \tEmb_Norm: 34.52 (37.43)\n",
      "Train Epoch: 4 [170880/251450]\tLoss: 0.0259 (0.0275) \tAcc: 96.88% (96.06%) \tEmb_Norm: 34.50 (37.42)\n",
      "Train Epoch: 4 [171520/251450]\tLoss: 0.0187 (0.0275) \tAcc: 96.88% (96.07%) \tEmb_Norm: 34.48 (37.41)\n",
      "Train Epoch: 4 [172160/251450]\tLoss: 0.0292 (0.0275) \tAcc: 96.88% (96.07%) \tEmb_Norm: 34.46 (37.40)\n",
      "Train Epoch: 4 [172800/251450]\tLoss: 0.0064 (0.0275) \tAcc: 100.00% (96.07%) \tEmb_Norm: 34.43 (37.39)\n",
      "Train Epoch: 4 [173440/251450]\tLoss: 0.0226 (0.0275) \tAcc: 96.88% (96.07%) \tEmb_Norm: 34.41 (37.38)\n",
      "Train Epoch: 4 [174080/251450]\tLoss: 0.0232 (0.0275) \tAcc: 96.88% (96.06%) \tEmb_Norm: 34.39 (37.37)\n",
      "Train Epoch: 4 [174720/251450]\tLoss: 0.0306 (0.0275) \tAcc: 96.88% (96.06%) \tEmb_Norm: 34.37 (37.36)\n",
      "Train Epoch: 4 [175360/251450]\tLoss: 0.0166 (0.0275) \tAcc: 93.75% (96.06%) \tEmb_Norm: 34.34 (37.35)\n",
      "Train Epoch: 4 [176000/251450]\tLoss: 0.0000 (0.0275) \tAcc: 100.00% (96.06%) \tEmb_Norm: 34.32 (37.33)\n",
      "Train Epoch: 4 [176640/251450]\tLoss: 0.0000 (0.0275) \tAcc: 100.00% (96.06%) \tEmb_Norm: 34.30 (37.32)\n",
      "Train Epoch: 4 [177280/251450]\tLoss: 0.0023 (0.0275) \tAcc: 100.00% (96.06%) \tEmb_Norm: 34.28 (37.31)\n",
      "Train Epoch: 4 [177920/251450]\tLoss: 0.0169 (0.0275) \tAcc: 96.88% (96.06%) \tEmb_Norm: 34.26 (37.30)\n",
      "Train Epoch: 4 [178560/251450]\tLoss: 0.0000 (0.0275) \tAcc: 100.00% (96.06%) \tEmb_Norm: 34.23 (37.29)\n",
      "Train Epoch: 4 [179200/251450]\tLoss: 0.0000 (0.0275) \tAcc: 100.00% (96.07%) \tEmb_Norm: 34.21 (37.28)\n",
      "Train Epoch: 4 [179840/251450]\tLoss: 0.0227 (0.0275) \tAcc: 96.88% (96.07%) \tEmb_Norm: 34.19 (37.27)\n",
      "Train Epoch: 4 [180480/251450]\tLoss: 0.0308 (0.0275) \tAcc: 96.88% (96.07%) \tEmb_Norm: 34.17 (37.26)\n",
      "Train Epoch: 4 [181120/251450]\tLoss: 0.0000 (0.0274) \tAcc: 100.00% (96.08%) \tEmb_Norm: 34.15 (37.25)\n",
      "Train Epoch: 4 [181760/251450]\tLoss: 0.0604 (0.0274) \tAcc: 96.88% (96.08%) \tEmb_Norm: 34.13 (37.24)\n",
      "Train Epoch: 4 [182400/251450]\tLoss: 0.0050 (0.0273) \tAcc: 100.00% (96.08%) \tEmb_Norm: 34.11 (37.23)\n",
      "Train Epoch: 4 [183040/251450]\tLoss: 0.0189 (0.0274) \tAcc: 93.75% (96.08%) \tEmb_Norm: 34.09 (37.21)\n",
      "Train Epoch: 4 [183680/251450]\tLoss: 0.0246 (0.0273) \tAcc: 96.88% (96.08%) \tEmb_Norm: 34.06 (37.20)\n",
      "Train Epoch: 4 [184320/251450]\tLoss: 0.0174 (0.0273) \tAcc: 96.88% (96.09%) \tEmb_Norm: 34.04 (37.19)\n",
      "Train Epoch: 4 [184960/251450]\tLoss: 0.0028 (0.0273) \tAcc: 100.00% (96.09%) \tEmb_Norm: 34.02 (37.18)\n",
      "Train Epoch: 4 [185600/251450]\tLoss: 0.0508 (0.0273) \tAcc: 93.75% (96.08%) \tEmb_Norm: 34.00 (37.17)\n",
      "Train Epoch: 4 [186240/251450]\tLoss: 0.0046 (0.0273) \tAcc: 100.00% (96.08%) \tEmb_Norm: 33.98 (37.16)\n",
      "Train Epoch: 4 [186880/251450]\tLoss: 0.0042 (0.0273) \tAcc: 100.00% (96.08%) \tEmb_Norm: 33.96 (37.15)\n",
      "Train Epoch: 4 [187520/251450]\tLoss: 0.0000 (0.0273) \tAcc: 100.00% (96.08%) \tEmb_Norm: 33.94 (37.14)\n",
      "Train Epoch: 4 [188160/251450]\tLoss: 0.0555 (0.0273) \tAcc: 90.62% (96.08%) \tEmb_Norm: 33.91 (37.13)\n",
      "Train Epoch: 4 [188800/251450]\tLoss: 0.0271 (0.0273) \tAcc: 96.88% (96.08%) \tEmb_Norm: 33.89 (37.12)\n",
      "Train Epoch: 4 [189440/251450]\tLoss: 0.0116 (0.0273) \tAcc: 96.88% (96.08%) \tEmb_Norm: 33.87 (37.10)\n",
      "Train Epoch: 4 [190080/251450]\tLoss: 0.0323 (0.0273) \tAcc: 93.75% (96.08%) \tEmb_Norm: 33.85 (37.09)\n",
      "Train Epoch: 4 [190720/251450]\tLoss: 0.0963 (0.0273) \tAcc: 84.38% (96.08%) \tEmb_Norm: 33.83 (37.08)\n",
      "Train Epoch: 4 [191360/251450]\tLoss: 0.0161 (0.0273) \tAcc: 96.88% (96.08%) \tEmb_Norm: 33.80 (37.07)\n",
      "Train Epoch: 4 [192000/251450]\tLoss: 0.0118 (0.0273) \tAcc: 96.88% (96.07%) \tEmb_Norm: 33.78 (37.06)\n",
      "Train Epoch: 4 [192640/251450]\tLoss: 0.0062 (0.0273) \tAcc: 100.00% (96.08%) \tEmb_Norm: 33.76 (37.05)\n",
      "Train Epoch: 4 [193280/251450]\tLoss: 0.0000 (0.0273) \tAcc: 100.00% (96.08%) \tEmb_Norm: 33.74 (37.04)\n",
      "Train Epoch: 4 [193920/251450]\tLoss: 0.0090 (0.0273) \tAcc: 96.88% (96.08%) \tEmb_Norm: 33.72 (37.03)\n",
      "Train Epoch: 4 [194560/251450]\tLoss: 0.0160 (0.0273) \tAcc: 96.88% (96.07%) \tEmb_Norm: 33.69 (37.02)\n",
      "Train Epoch: 4 [195200/251450]\tLoss: 0.0078 (0.0273) \tAcc: 96.88% (96.07%) \tEmb_Norm: 33.67 (37.01)\n",
      "Train Epoch: 4 [195840/251450]\tLoss: 0.0000 (0.0273) \tAcc: 100.00% (96.07%) \tEmb_Norm: 33.65 (37.00)\n",
      "Train Epoch: 4 [196480/251450]\tLoss: 0.0000 (0.0273) \tAcc: 100.00% (96.08%) \tEmb_Norm: 33.63 (36.98)\n",
      "Train Epoch: 4 [197120/251450]\tLoss: 0.0183 (0.0273) \tAcc: 96.88% (96.08%) \tEmb_Norm: 33.60 (36.97)\n",
      "Train Epoch: 4 [197760/251450]\tLoss: 0.0075 (0.0273) \tAcc: 100.00% (96.08%) \tEmb_Norm: 33.58 (36.96)\n",
      "Train Epoch: 4 [198400/251450]\tLoss: 0.0672 (0.0272) \tAcc: 90.62% (96.08%) \tEmb_Norm: 33.56 (36.95)\n",
      "Train Epoch: 4 [199040/251450]\tLoss: 0.0150 (0.0272) \tAcc: 96.88% (96.08%) \tEmb_Norm: 33.54 (36.94)\n",
      "Train Epoch: 4 [199680/251450]\tLoss: 0.0120 (0.0272) \tAcc: 96.88% (96.08%) \tEmb_Norm: 33.52 (36.93)\n",
      "Train Epoch: 4 [200320/251450]\tLoss: 0.0447 (0.0272) \tAcc: 96.88% (96.08%) \tEmb_Norm: 33.50 (36.92)\n",
      "Train Epoch: 4 [200960/251450]\tLoss: 0.0314 (0.0272) \tAcc: 93.75% (96.08%) \tEmb_Norm: 33.48 (36.91)\n",
      "Train Epoch: 4 [201600/251450]\tLoss: 0.1268 (0.0273) \tAcc: 93.75% (96.08%) \tEmb_Norm: 33.45 (36.90)\n",
      "=============== TEST epoch 4 ===============\n",
      "\n",
      "Test set: Average loss: 0.0322, Accuracy: 95.16%\n",
      "\n",
      "=============== checkpoint epoch 4 ===============\n",
      "=============== TRAIN epoch 5 ===============\n",
      "Train Epoch: 5 [0/251450]\tLoss: 0.0510 (0.0510) \tAcc: 93.75% (93.75%) \tEmb_Norm: 33.43 (33.43)\n",
      "Train Epoch: 5 [640/251450]\tLoss: 0.0183 (0.0267) \tAcc: 96.88% (95.68%) \tEmb_Norm: 33.41 (33.42)\n",
      "Train Epoch: 5 [1280/251450]\tLoss: 0.0000 (0.0236) \tAcc: 100.00% (95.88%) \tEmb_Norm: 33.39 (33.41)\n",
      "Train Epoch: 5 [1920/251450]\tLoss: 0.0253 (0.0231) \tAcc: 93.75% (96.00%) \tEmb_Norm: 33.37 (33.40)\n",
      "Train Epoch: 5 [2560/251450]\tLoss: 0.0530 (0.0244) \tAcc: 93.75% (96.06%) \tEmb_Norm: 33.35 (33.39)\n",
      "Train Epoch: 5 [3200/251450]\tLoss: 0.0179 (0.0239) \tAcc: 96.88% (96.19%) \tEmb_Norm: 33.33 (33.38)\n",
      "Train Epoch: 5 [3840/251450]\tLoss: 0.0031 (0.0220) \tAcc: 100.00% (96.51%) \tEmb_Norm: 33.30 (33.37)\n",
      "Train Epoch: 5 [4480/251450]\tLoss: 0.0919 (0.0234) \tAcc: 93.75% (96.43%) \tEmb_Norm: 33.28 (33.36)\n",
      "Train Epoch: 5 [5120/251450]\tLoss: 0.0085 (0.0232) \tAcc: 96.88% (96.37%) \tEmb_Norm: 33.26 (33.35)\n",
      "Train Epoch: 5 [5760/251450]\tLoss: 0.1185 (0.0230) \tAcc: 87.50% (96.39%) \tEmb_Norm: 33.24 (33.34)\n",
      "Train Epoch: 5 [6400/251450]\tLoss: 0.0000 (0.0229) \tAcc: 100.00% (96.42%) \tEmb_Norm: 33.22 (33.33)\n",
      "Train Epoch: 5 [7040/251450]\tLoss: 0.0006 (0.0223) \tAcc: 100.00% (96.51%) \tEmb_Norm: 33.20 (33.32)\n",
      "Train Epoch: 5 [7680/251450]\tLoss: 0.0296 (0.0224) \tAcc: 96.88% (96.49%) \tEmb_Norm: 33.18 (33.30)\n",
      "Train Epoch: 5 [8320/251450]\tLoss: 0.0144 (0.0218) \tAcc: 96.88% (96.53%) \tEmb_Norm: 33.16 (33.29)\n",
      "Train Epoch: 5 [8960/251450]\tLoss: 0.0000 (0.0223) \tAcc: 100.00% (96.46%) \tEmb_Norm: 33.13 (33.28)\n",
      "Train Epoch: 5 [9600/251450]\tLoss: 0.0091 (0.0223) \tAcc: 96.88% (96.45%) \tEmb_Norm: 33.11 (33.27)\n",
      "Train Epoch: 5 [10240/251450]\tLoss: 0.1115 (0.0224) \tAcc: 81.25% (96.46%) \tEmb_Norm: 33.09 (33.26)\n",
      "Train Epoch: 5 [10880/251450]\tLoss: 0.0437 (0.0220) \tAcc: 96.88% (96.48%) \tEmb_Norm: 33.07 (33.25)\n",
      "Train Epoch: 5 [11520/251450]\tLoss: 0.0175 (0.0218) \tAcc: 93.75% (96.51%) \tEmb_Norm: 33.05 (33.24)\n",
      "Train Epoch: 5 [12160/251450]\tLoss: 0.0201 (0.0218) \tAcc: 93.75% (96.49%) \tEmb_Norm: 33.03 (33.23)\n",
      "Train Epoch: 5 [12800/251450]\tLoss: 0.0000 (0.0217) \tAcc: 100.00% (96.52%) \tEmb_Norm: 33.01 (33.22)\n",
      "Train Epoch: 5 [13440/251450]\tLoss: 0.0000 (0.0215) \tAcc: 100.00% (96.53%) \tEmb_Norm: 32.99 (33.21)\n",
      "Train Epoch: 5 [14080/251450]\tLoss: 0.0171 (0.0211) \tAcc: 96.88% (96.60%) \tEmb_Norm: 32.97 (33.20)\n",
      "Train Epoch: 5 [14720/251450]\tLoss: 0.0374 (0.0217) \tAcc: 93.75% (96.52%) \tEmb_Norm: 32.94 (33.19)\n",
      "Train Epoch: 5 [15360/251450]\tLoss: 0.0000 (0.0216) \tAcc: 100.00% (96.54%) \tEmb_Norm: 32.92 (33.18)\n",
      "Train Epoch: 5 [16000/251450]\tLoss: 0.0134 (0.0216) \tAcc: 96.88% (96.56%) \tEmb_Norm: 32.90 (33.17)\n",
      "Train Epoch: 5 [16640/251450]\tLoss: 0.0067 (0.0221) \tAcc: 100.00% (96.52%) \tEmb_Norm: 32.88 (33.16)\n",
      "Train Epoch: 5 [17280/251450]\tLoss: 0.0000 (0.0219) \tAcc: 100.00% (96.54%) \tEmb_Norm: 32.86 (33.15)\n",
      "Train Epoch: 5 [17920/251450]\tLoss: 0.0109 (0.0221) \tAcc: 96.88% (96.54%) \tEmb_Norm: 32.84 (33.13)\n",
      "Train Epoch: 5 [18560/251450]\tLoss: 0.0332 (0.0219) \tAcc: 90.62% (96.57%) \tEmb_Norm: 32.81 (33.12)\n",
      "Train Epoch: 5 [19200/251450]\tLoss: 0.0411 (0.0218) \tAcc: 96.88% (96.57%) \tEmb_Norm: 32.79 (33.11)\n",
      "Train Epoch: 5 [19840/251450]\tLoss: 0.0065 (0.0218) \tAcc: 96.88% (96.60%) \tEmb_Norm: 32.77 (33.10)\n",
      "Train Epoch: 5 [20480/251450]\tLoss: 0.0000 (0.0217) \tAcc: 100.00% (96.63%) \tEmb_Norm: 32.75 (33.09)\n",
      "Train Epoch: 5 [21120/251450]\tLoss: 0.0078 (0.0216) \tAcc: 96.88% (96.64%) \tEmb_Norm: 32.73 (33.08)\n",
      "Train Epoch: 5 [21760/251450]\tLoss: 0.0051 (0.0217) \tAcc: 100.00% (96.61%) \tEmb_Norm: 32.71 (33.07)\n",
      "Train Epoch: 5 [22400/251450]\tLoss: 0.0000 (0.0217) \tAcc: 100.00% (96.60%) \tEmb_Norm: 32.69 (33.06)\n",
      "Train Epoch: 5 [23040/251450]\tLoss: 0.0809 (0.0217) \tAcc: 96.88% (96.61%) \tEmb_Norm: 32.67 (33.05)\n",
      "Train Epoch: 5 [23680/251450]\tLoss: 0.0305 (0.0214) \tAcc: 93.75% (96.65%) \tEmb_Norm: 32.65 (33.04)\n",
      "Train Epoch: 5 [24320/251450]\tLoss: 0.0116 (0.0214) \tAcc: 96.88% (96.66%) \tEmb_Norm: 32.62 (33.03)\n",
      "Train Epoch: 5 [24960/251450]\tLoss: 0.0418 (0.0216) \tAcc: 93.75% (96.63%) \tEmb_Norm: 32.60 (33.02)\n",
      "Train Epoch: 5 [25600/251450]\tLoss: 0.0000 (0.0220) \tAcc: 100.00% (96.60%) \tEmb_Norm: 32.58 (33.01)\n",
      "Train Epoch: 5 [26240/251450]\tLoss: 0.0139 (0.0218) \tAcc: 96.88% (96.61%) \tEmb_Norm: 32.56 (33.00)\n",
      "Train Epoch: 5 [26880/251450]\tLoss: 0.0274 (0.0218) \tAcc: 96.88% (96.63%) \tEmb_Norm: 32.54 (32.99)\n",
      "Train Epoch: 5 [27520/251450]\tLoss: 0.0100 (0.0217) \tAcc: 96.88% (96.62%) \tEmb_Norm: 32.52 (32.97)\n",
      "Train Epoch: 5 [28160/251450]\tLoss: 0.0112 (0.0216) \tAcc: 96.88% (96.62%) \tEmb_Norm: 32.50 (32.96)\n",
      "Train Epoch: 5 [28800/251450]\tLoss: 0.0000 (0.0214) \tAcc: 100.00% (96.64%) \tEmb_Norm: 32.47 (32.95)\n",
      "Train Epoch: 5 [29440/251450]\tLoss: 0.0355 (0.0213) \tAcc: 90.62% (96.64%) \tEmb_Norm: 32.45 (32.94)\n",
      "Train Epoch: 5 [30080/251450]\tLoss: 0.0186 (0.0216) \tAcc: 96.88% (96.62%) \tEmb_Norm: 32.43 (32.93)\n",
      "Train Epoch: 5 [30720/251450]\tLoss: 0.0291 (0.0217) \tAcc: 93.75% (96.62%) \tEmb_Norm: 32.41 (32.92)\n",
      "Train Epoch: 5 [31360/251450]\tLoss: 0.0152 (0.0216) \tAcc: 96.88% (96.65%) \tEmb_Norm: 32.39 (32.91)\n",
      "Train Epoch: 5 [32000/251450]\tLoss: 0.0088 (0.0217) \tAcc: 96.88% (96.63%) \tEmb_Norm: 32.37 (32.90)\n",
      "Train Epoch: 5 [32640/251450]\tLoss: 0.0166 (0.0216) \tAcc: 96.88% (96.64%) \tEmb_Norm: 32.34 (32.89)\n",
      "Train Epoch: 5 [33280/251450]\tLoss: 0.0135 (0.0215) \tAcc: 96.88% (96.64%) \tEmb_Norm: 32.32 (32.88)\n",
      "Train Epoch: 5 [33920/251450]\tLoss: 0.0072 (0.0215) \tAcc: 96.88% (96.64%) \tEmb_Norm: 32.30 (32.87)\n",
      "Train Epoch: 5 [34560/251450]\tLoss: 0.0128 (0.0215) \tAcc: 96.88% (96.65%) \tEmb_Norm: 32.28 (32.86)\n",
      "Train Epoch: 5 [35200/251450]\tLoss: 0.0091 (0.0215) \tAcc: 96.88% (96.63%) \tEmb_Norm: 32.26 (32.85)\n",
      "Train Epoch: 5 [35840/251450]\tLoss: 0.0185 (0.0215) \tAcc: 96.88% (96.63%) \tEmb_Norm: 32.24 (32.84)\n",
      "Train Epoch: 5 [36480/251450]\tLoss: 0.0149 (0.0216) \tAcc: 96.88% (96.61%) \tEmb_Norm: 32.22 (32.83)\n",
      "Train Epoch: 5 [37120/251450]\tLoss: 0.0051 (0.0215) \tAcc: 100.00% (96.62%) \tEmb_Norm: 32.20 (32.81)\n",
      "Train Epoch: 5 [37760/251450]\tLoss: 0.1142 (0.0215) \tAcc: 90.62% (96.63%) \tEmb_Norm: 32.17 (32.80)\n",
      "Train Epoch: 5 [38400/251450]\tLoss: 0.0093 (0.0216) \tAcc: 96.88% (96.62%) \tEmb_Norm: 32.15 (32.79)\n",
      "Train Epoch: 5 [39040/251450]\tLoss: 0.0000 (0.0216) \tAcc: 100.00% (96.62%) \tEmb_Norm: 32.13 (32.78)\n",
      "Train Epoch: 5 [39680/251450]\tLoss: 0.0695 (0.0215) \tAcc: 93.75% (96.64%) \tEmb_Norm: 32.11 (32.77)\n",
      "Train Epoch: 5 [40320/251450]\tLoss: 0.0497 (0.0215) \tAcc: 93.75% (96.64%) \tEmb_Norm: 32.09 (32.76)\n",
      "Train Epoch: 5 [40960/251450]\tLoss: 0.0287 (0.0216) \tAcc: 93.75% (96.61%) \tEmb_Norm: 32.07 (32.75)\n",
      "Train Epoch: 5 [41600/251450]\tLoss: 0.0335 (0.0217) \tAcc: 90.62% (96.59%) \tEmb_Norm: 32.05 (32.74)\n",
      "Train Epoch: 5 [42240/251450]\tLoss: 0.0223 (0.0217) \tAcc: 96.88% (96.59%) \tEmb_Norm: 32.02 (32.73)\n",
      "Train Epoch: 5 [42880/251450]\tLoss: 0.0055 (0.0218) \tAcc: 100.00% (96.58%) \tEmb_Norm: 32.00 (32.72)\n",
      "Train Epoch: 5 [43520/251450]\tLoss: 0.0000 (0.0218) \tAcc: 100.00% (96.57%) \tEmb_Norm: 31.98 (32.71)\n",
      "Train Epoch: 5 [44160/251450]\tLoss: 0.0000 (0.0217) \tAcc: 100.00% (96.60%) \tEmb_Norm: 31.96 (32.70)\n",
      "Train Epoch: 5 [44800/251450]\tLoss: 0.0113 (0.0217) \tAcc: 96.88% (96.59%) \tEmb_Norm: 31.94 (32.69)\n",
      "Train Epoch: 5 [45440/251450]\tLoss: 0.0000 (0.0216) \tAcc: 100.00% (96.61%) \tEmb_Norm: 31.92 (32.68)\n",
      "Train Epoch: 5 [46080/251450]\tLoss: 0.0808 (0.0216) \tAcc: 90.62% (96.61%) \tEmb_Norm: 31.90 (32.67)\n",
      "Train Epoch: 5 [46720/251450]\tLoss: 0.0120 (0.0216) \tAcc: 96.88% (96.60%) \tEmb_Norm: 31.88 (32.65)\n",
      "Train Epoch: 5 [47360/251450]\tLoss: 0.0069 (0.0216) \tAcc: 96.88% (96.59%) \tEmb_Norm: 31.85 (32.64)\n",
      "Train Epoch: 5 [48000/251450]\tLoss: 0.0045 (0.0217) \tAcc: 100.00% (96.58%) \tEmb_Norm: 31.83 (32.63)\n",
      "Train Epoch: 5 [48640/251450]\tLoss: 0.0196 (0.0217) \tAcc: 96.88% (96.57%) \tEmb_Norm: 31.81 (32.62)\n",
      "Train Epoch: 5 [49280/251450]\tLoss: 0.0000 (0.0217) \tAcc: 100.00% (96.58%) \tEmb_Norm: 31.79 (32.61)\n",
      "Train Epoch: 5 [49920/251450]\tLoss: 0.0380 (0.0217) \tAcc: 93.75% (96.58%) \tEmb_Norm: 31.77 (32.60)\n",
      "Train Epoch: 5 [50560/251450]\tLoss: 0.0261 (0.0216) \tAcc: 96.88% (96.60%) \tEmb_Norm: 31.75 (32.59)\n",
      "Train Epoch: 5 [51200/251450]\tLoss: 0.1031 (0.0219) \tAcc: 84.38% (96.56%) \tEmb_Norm: 31.72 (32.58)\n",
      "Train Epoch: 5 [51840/251450]\tLoss: 0.0330 (0.0220) \tAcc: 93.75% (96.54%) \tEmb_Norm: 31.70 (32.57)\n",
      "Train Epoch: 5 [52480/251450]\tLoss: 0.0508 (0.0220) \tAcc: 90.62% (96.54%) \tEmb_Norm: 31.68 (32.56)\n",
      "Train Epoch: 5 [53120/251450]\tLoss: 0.0119 (0.0221) \tAcc: 96.88% (96.53%) \tEmb_Norm: 31.65 (32.55)\n",
      "Train Epoch: 5 [53760/251450]\tLoss: 0.0018 (0.0221) \tAcc: 100.00% (96.53%) \tEmb_Norm: 31.63 (32.54)\n",
      "Train Epoch: 5 [54400/251450]\tLoss: 0.0056 (0.0221) \tAcc: 100.00% (96.53%) \tEmb_Norm: 31.61 (32.53)\n",
      "Train Epoch: 5 [55040/251450]\tLoss: 0.0738 (0.0221) \tAcc: 90.62% (96.53%) \tEmb_Norm: 31.59 (32.52)\n",
      "Train Epoch: 5 [55680/251450]\tLoss: 0.0205 (0.0221) \tAcc: 96.88% (96.53%) \tEmb_Norm: 31.57 (32.50)\n",
      "Train Epoch: 5 [56320/251450]\tLoss: 0.0000 (0.0221) \tAcc: 100.00% (96.54%) \tEmb_Norm: 31.55 (32.49)\n",
      "Train Epoch: 5 [56960/251450]\tLoss: 0.0088 (0.0221) \tAcc: 100.00% (96.54%) \tEmb_Norm: 31.53 (32.48)\n",
      "Train Epoch: 5 [57600/251450]\tLoss: 0.0762 (0.0222) \tAcc: 87.50% (96.52%) \tEmb_Norm: 31.50 (32.47)\n",
      "Train Epoch: 5 [58240/251450]\tLoss: 0.0000 (0.0222) \tAcc: 100.00% (96.52%) \tEmb_Norm: 31.48 (32.46)\n",
      "Train Epoch: 5 [58880/251450]\tLoss: 0.0052 (0.0223) \tAcc: 100.00% (96.52%) \tEmb_Norm: 31.46 (32.45)\n",
      "Train Epoch: 5 [59520/251450]\tLoss: 0.0531 (0.0223) \tAcc: 93.75% (96.52%) \tEmb_Norm: 31.44 (32.44)\n",
      "Train Epoch: 5 [60160/251450]\tLoss: 0.0102 (0.0223) \tAcc: 96.88% (96.51%) \tEmb_Norm: 31.42 (32.43)\n",
      "Train Epoch: 5 [60800/251450]\tLoss: 0.0181 (0.0223) \tAcc: 96.88% (96.51%) \tEmb_Norm: 31.39 (32.42)\n",
      "Train Epoch: 5 [61440/251450]\tLoss: 0.0322 (0.0223) \tAcc: 93.75% (96.52%) \tEmb_Norm: 31.37 (32.41)\n",
      "Train Epoch: 5 [62080/251450]\tLoss: 0.0316 (0.0223) \tAcc: 96.88% (96.51%) \tEmb_Norm: 31.35 (32.40)\n",
      "Train Epoch: 5 [62720/251450]\tLoss: 0.0020 (0.0223) \tAcc: 100.00% (96.52%) \tEmb_Norm: 31.33 (32.39)\n",
      "Train Epoch: 5 [63360/251450]\tLoss: 0.0043 (0.0223) \tAcc: 100.00% (96.52%) \tEmb_Norm: 31.31 (32.38)\n",
      "Train Epoch: 5 [64000/251450]\tLoss: 0.0000 (0.0224) \tAcc: 100.00% (96.51%) \tEmb_Norm: 31.28 (32.36)\n",
      "Train Epoch: 5 [64640/251450]\tLoss: 0.0563 (0.0225) \tAcc: 90.62% (96.50%) \tEmb_Norm: 31.26 (32.35)\n",
      "Train Epoch: 5 [65280/251450]\tLoss: 0.0598 (0.0225) \tAcc: 90.62% (96.51%) \tEmb_Norm: 31.24 (32.34)\n",
      "Train Epoch: 5 [65920/251450]\tLoss: 0.0000 (0.0225) \tAcc: 100.00% (96.51%) \tEmb_Norm: 31.22 (32.33)\n",
      "Train Epoch: 5 [66560/251450]\tLoss: 0.0157 (0.0225) \tAcc: 96.88% (96.51%) \tEmb_Norm: 31.20 (32.32)\n",
      "Train Epoch: 5 [67200/251450]\tLoss: 0.0135 (0.0224) \tAcc: 96.88% (96.51%) \tEmb_Norm: 31.18 (32.31)\n",
      "Train Epoch: 5 [67840/251450]\tLoss: 0.0106 (0.0225) \tAcc: 96.88% (96.50%) \tEmb_Norm: 31.15 (32.30)\n",
      "Train Epoch: 5 [68480/251450]\tLoss: 0.0029 (0.0225) \tAcc: 100.00% (96.50%) \tEmb_Norm: 31.13 (32.29)\n",
      "Train Epoch: 5 [69120/251450]\tLoss: 0.0225 (0.0226) \tAcc: 93.75% (96.50%) \tEmb_Norm: 31.11 (32.28)\n",
      "Train Epoch: 5 [69760/251450]\tLoss: 0.0486 (0.0225) \tAcc: 90.62% (96.51%) \tEmb_Norm: 31.09 (32.27)\n",
      "Train Epoch: 5 [70400/251450]\tLoss: 0.0000 (0.0225) \tAcc: 100.00% (96.50%) \tEmb_Norm: 31.07 (32.26)\n",
      "Train Epoch: 5 [71040/251450]\tLoss: 0.0037 (0.0225) \tAcc: 100.00% (96.50%) \tEmb_Norm: 31.05 (32.25)\n",
      "Train Epoch: 5 [71680/251450]\tLoss: 0.0408 (0.0225) \tAcc: 90.62% (96.49%) \tEmb_Norm: 31.03 (32.23)\n",
      "Train Epoch: 5 [72320/251450]\tLoss: 0.0247 (0.0225) \tAcc: 93.75% (96.49%) \tEmb_Norm: 31.00 (32.22)\n",
      "Train Epoch: 5 [72960/251450]\tLoss: 0.0323 (0.0225) \tAcc: 93.75% (96.50%) \tEmb_Norm: 30.98 (32.21)\n",
      "Train Epoch: 5 [73600/251450]\tLoss: 0.0323 (0.0225) \tAcc: 93.75% (96.49%) \tEmb_Norm: 30.96 (32.20)\n",
      "Train Epoch: 5 [74240/251450]\tLoss: 0.0206 (0.0225) \tAcc: 96.88% (96.49%) \tEmb_Norm: 30.94 (32.19)\n",
      "Train Epoch: 5 [74880/251450]\tLoss: 0.0115 (0.0225) \tAcc: 96.88% (96.49%) \tEmb_Norm: 30.92 (32.18)\n",
      "Train Epoch: 5 [75520/251450]\tLoss: 0.0121 (0.0224) \tAcc: 96.88% (96.50%) \tEmb_Norm: 30.90 (32.17)\n",
      "Train Epoch: 5 [76160/251450]\tLoss: 0.0000 (0.0224) \tAcc: 100.00% (96.51%) \tEmb_Norm: 30.88 (32.16)\n",
      "Train Epoch: 5 [76800/251450]\tLoss: 0.0088 (0.0224) \tAcc: 96.88% (96.50%) \tEmb_Norm: 30.86 (32.15)\n",
      "Train Epoch: 5 [77440/251450]\tLoss: 0.0533 (0.0224) \tAcc: 96.88% (96.51%) \tEmb_Norm: 30.83 (32.14)\n",
      "Train Epoch: 5 [78080/251450]\tLoss: 0.0156 (0.0224) \tAcc: 96.88% (96.51%) \tEmb_Norm: 30.81 (32.13)\n",
      "Train Epoch: 5 [78720/251450]\tLoss: 0.0198 (0.0224) \tAcc: 96.88% (96.51%) \tEmb_Norm: 30.79 (32.12)\n",
      "Train Epoch: 5 [79360/251450]\tLoss: 0.0486 (0.0223) \tAcc: 93.75% (96.52%) \tEmb_Norm: 30.77 (32.11)\n",
      "Train Epoch: 5 [80000/251450]\tLoss: 0.0154 (0.0223) \tAcc: 96.88% (96.52%) \tEmb_Norm: 30.75 (32.09)\n",
      "Train Epoch: 5 [80640/251450]\tLoss: 0.0037 (0.0223) \tAcc: 100.00% (96.53%) \tEmb_Norm: 30.73 (32.08)\n",
      "Train Epoch: 5 [81280/251450]\tLoss: 0.0072 (0.0223) \tAcc: 96.88% (96.53%) \tEmb_Norm: 30.71 (32.07)\n",
      "Train Epoch: 5 [81920/251450]\tLoss: 0.0000 (0.0223) \tAcc: 100.00% (96.52%) \tEmb_Norm: 30.69 (32.06)\n",
      "Train Epoch: 5 [82560/251450]\tLoss: 0.0000 (0.0223) \tAcc: 100.00% (96.52%) \tEmb_Norm: 30.66 (32.05)\n",
      "Train Epoch: 5 [83200/251450]\tLoss: 0.0053 (0.0223) \tAcc: 100.00% (96.52%) \tEmb_Norm: 30.64 (32.04)\n",
      "Train Epoch: 5 [83840/251450]\tLoss: 0.0095 (0.0223) \tAcc: 96.88% (96.51%) \tEmb_Norm: 30.62 (32.03)\n",
      "Train Epoch: 5 [84480/251450]\tLoss: 0.0000 (0.0223) \tAcc: 100.00% (96.52%) \tEmb_Norm: 30.60 (32.02)\n",
      "Train Epoch: 5 [85120/251450]\tLoss: 0.0331 (0.0222) \tAcc: 93.75% (96.52%) \tEmb_Norm: 30.58 (32.01)\n",
      "Train Epoch: 5 [85760/251450]\tLoss: 0.0732 (0.0222) \tAcc: 93.75% (96.53%) \tEmb_Norm: 30.56 (32.00)\n",
      "Train Epoch: 5 [86400/251450]\tLoss: 0.0189 (0.0222) \tAcc: 96.88% (96.52%) \tEmb_Norm: 30.54 (31.99)\n",
      "Train Epoch: 5 [87040/251450]\tLoss: 0.0000 (0.0222) \tAcc: 100.00% (96.52%) \tEmb_Norm: 30.51 (31.98)\n",
      "Train Epoch: 5 [87680/251450]\tLoss: 0.0282 (0.0222) \tAcc: 93.75% (96.53%) \tEmb_Norm: 30.49 (31.97)\n",
      "Train Epoch: 5 [88320/251450]\tLoss: 0.0608 (0.0222) \tAcc: 96.88% (96.53%) \tEmb_Norm: 30.47 (31.95)\n",
      "Train Epoch: 5 [88960/251450]\tLoss: 0.0029 (0.0222) \tAcc: 100.00% (96.53%) \tEmb_Norm: 30.45 (31.94)\n",
      "Train Epoch: 5 [89600/251450]\tLoss: 0.0145 (0.0221) \tAcc: 96.88% (96.53%) \tEmb_Norm: 30.43 (31.93)\n",
      "Train Epoch: 5 [90240/251450]\tLoss: 0.0198 (0.0221) \tAcc: 93.75% (96.53%) \tEmb_Norm: 30.41 (31.92)\n",
      "Train Epoch: 5 [90880/251450]\tLoss: 0.0000 (0.0221) \tAcc: 100.00% (96.54%) \tEmb_Norm: 30.39 (31.91)\n",
      "Train Epoch: 5 [91520/251450]\tLoss: 0.0293 (0.0220) \tAcc: 93.75% (96.55%) \tEmb_Norm: 30.37 (31.90)\n",
      "Train Epoch: 5 [92160/251450]\tLoss: 0.0471 (0.0221) \tAcc: 93.75% (96.54%) \tEmb_Norm: 30.35 (31.89)\n",
      "Train Epoch: 5 [92800/251450]\tLoss: 0.0000 (0.0220) \tAcc: 100.00% (96.55%) \tEmb_Norm: 30.32 (31.88)\n",
      "Train Epoch: 5 [93440/251450]\tLoss: 0.0386 (0.0221) \tAcc: 93.75% (96.55%) \tEmb_Norm: 30.30 (31.87)\n",
      "Train Epoch: 5 [94080/251450]\tLoss: 0.0293 (0.0220) \tAcc: 93.75% (96.56%) \tEmb_Norm: 30.28 (31.86)\n",
      "Train Epoch: 5 [94720/251450]\tLoss: 0.0075 (0.0220) \tAcc: 96.88% (96.56%) \tEmb_Norm: 30.26 (31.85)\n",
      "Train Epoch: 5 [95360/251450]\tLoss: 0.0638 (0.0221) \tAcc: 90.62% (96.56%) \tEmb_Norm: 30.24 (31.84)\n",
      "Train Epoch: 5 [96000/251450]\tLoss: 0.0054 (0.0221) \tAcc: 100.00% (96.55%) \tEmb_Norm: 30.21 (31.83)\n",
      "Train Epoch: 5 [96640/251450]\tLoss: 0.0682 (0.0222) \tAcc: 90.62% (96.55%) \tEmb_Norm: 30.19 (31.82)\n",
      "Train Epoch: 5 [97280/251450]\tLoss: 0.0000 (0.0221) \tAcc: 100.00% (96.55%) \tEmb_Norm: 30.17 (31.80)\n",
      "Train Epoch: 5 [97920/251450]\tLoss: 0.0063 (0.0222) \tAcc: 100.00% (96.54%) \tEmb_Norm: 30.15 (31.79)\n",
      "Train Epoch: 5 [98560/251450]\tLoss: 0.0027 (0.0221) \tAcc: 100.00% (96.54%) \tEmb_Norm: 30.13 (31.78)\n",
      "Train Epoch: 5 [99200/251450]\tLoss: 0.0046 (0.0222) \tAcc: 100.00% (96.53%) \tEmb_Norm: 30.11 (31.77)\n",
      "Train Epoch: 5 [99840/251450]\tLoss: 0.0099 (0.0222) \tAcc: 96.88% (96.53%) \tEmb_Norm: 30.08 (31.76)\n",
      "Train Epoch: 5 [100480/251450]\tLoss: 0.0264 (0.0222) \tAcc: 96.88% (96.53%) \tEmb_Norm: 30.06 (31.75)\n",
      "Train Epoch: 5 [101120/251450]\tLoss: 0.0126 (0.0222) \tAcc: 96.88% (96.54%) \tEmb_Norm: 30.04 (31.74)\n",
      "Train Epoch: 5 [101760/251450]\tLoss: 0.0197 (0.0221) \tAcc: 96.88% (96.54%) \tEmb_Norm: 30.02 (31.73)\n",
      "Train Epoch: 5 [102400/251450]\tLoss: 0.0559 (0.0221) \tAcc: 96.88% (96.54%) \tEmb_Norm: 30.00 (31.72)\n",
      "Train Epoch: 5 [103040/251450]\tLoss: 0.0000 (0.0222) \tAcc: 100.00% (96.54%) \tEmb_Norm: 29.98 (31.71)\n",
      "Train Epoch: 5 [103680/251450]\tLoss: 0.0157 (0.0222) \tAcc: 96.88% (96.53%) \tEmb_Norm: 29.96 (31.70)\n",
      "Train Epoch: 5 [104320/251450]\tLoss: 0.0026 (0.0222) \tAcc: 100.00% (96.53%) \tEmb_Norm: 29.93 (31.69)\n",
      "Train Epoch: 5 [104960/251450]\tLoss: 0.0359 (0.0221) \tAcc: 96.88% (96.54%) \tEmb_Norm: 29.91 (31.68)\n",
      "Train Epoch: 5 [105600/251450]\tLoss: 0.0000 (0.0222) \tAcc: 100.00% (96.54%) \tEmb_Norm: 29.89 (31.66)\n",
      "Train Epoch: 5 [106240/251450]\tLoss: 0.0420 (0.0221) \tAcc: 93.75% (96.54%) \tEmb_Norm: 29.87 (31.65)\n",
      "Train Epoch: 5 [106880/251450]\tLoss: 0.0000 (0.0221) \tAcc: 100.00% (96.55%) \tEmb_Norm: 29.85 (31.64)\n",
      "Train Epoch: 5 [107520/251450]\tLoss: 0.0419 (0.0221) \tAcc: 93.75% (96.54%) \tEmb_Norm: 29.83 (31.63)\n",
      "Train Epoch: 5 [108160/251450]\tLoss: 0.0000 (0.0221) \tAcc: 100.00% (96.55%) \tEmb_Norm: 29.81 (31.62)\n",
      "Train Epoch: 5 [108800/251450]\tLoss: 0.0338 (0.0221) \tAcc: 93.75% (96.55%) \tEmb_Norm: 29.79 (31.61)\n",
      "Train Epoch: 5 [109440/251450]\tLoss: 0.0362 (0.0221) \tAcc: 93.75% (96.54%) \tEmb_Norm: 29.76 (31.60)\n",
      "Train Epoch: 5 [110080/251450]\tLoss: 0.0321 (0.0221) \tAcc: 93.75% (96.54%) \tEmb_Norm: 29.74 (31.59)\n",
      "Train Epoch: 5 [110720/251450]\tLoss: 0.0183 (0.0221) \tAcc: 96.88% (96.55%) \tEmb_Norm: 29.72 (31.58)\n",
      "Train Epoch: 5 [111360/251450]\tLoss: 0.0179 (0.0221) \tAcc: 93.75% (96.54%) \tEmb_Norm: 29.70 (31.57)\n",
      "Train Epoch: 5 [112000/251450]\tLoss: 0.0209 (0.0221) \tAcc: 96.88% (96.54%) \tEmb_Norm: 29.68 (31.56)\n",
      "Train Epoch: 5 [112640/251450]\tLoss: 0.0368 (0.0221) \tAcc: 96.88% (96.55%) \tEmb_Norm: 29.66 (31.55)\n",
      "Train Epoch: 5 [113280/251450]\tLoss: 0.0189 (0.0221) \tAcc: 96.88% (96.55%) \tEmb_Norm: 29.63 (31.54)\n",
      "Train Epoch: 5 [113920/251450]\tLoss: 0.0000 (0.0222) \tAcc: 100.00% (96.54%) \tEmb_Norm: 29.61 (31.53)\n",
      "Train Epoch: 5 [114560/251450]\tLoss: 0.0431 (0.0221) \tAcc: 93.75% (96.55%) \tEmb_Norm: 29.59 (31.51)\n",
      "Train Epoch: 5 [115200/251450]\tLoss: 0.0000 (0.0221) \tAcc: 100.00% (96.55%) \tEmb_Norm: 29.57 (31.50)\n",
      "Train Epoch: 5 [115840/251450]\tLoss: 0.0141 (0.0221) \tAcc: 96.88% (96.55%) \tEmb_Norm: 29.55 (31.49)\n",
      "Train Epoch: 5 [116480/251450]\tLoss: 0.1025 (0.0221) \tAcc: 84.38% (96.55%) \tEmb_Norm: 29.53 (31.48)\n",
      "Train Epoch: 5 [117120/251450]\tLoss: 0.0002 (0.0221) \tAcc: 100.00% (96.56%) \tEmb_Norm: 29.50 (31.47)\n",
      "Train Epoch: 5 [117760/251450]\tLoss: 0.0437 (0.0221) \tAcc: 90.62% (96.55%) \tEmb_Norm: 29.48 (31.46)\n",
      "Train Epoch: 5 [118400/251450]\tLoss: 0.0972 (0.0221) \tAcc: 84.38% (96.55%) \tEmb_Norm: 29.46 (31.45)\n",
      "Train Epoch: 5 [119040/251450]\tLoss: 0.0105 (0.0221) \tAcc: 96.88% (96.56%) \tEmb_Norm: 29.44 (31.44)\n",
      "Train Epoch: 5 [119680/251450]\tLoss: 0.0718 (0.0220) \tAcc: 90.62% (96.56%) \tEmb_Norm: 29.42 (31.43)\n",
      "Train Epoch: 5 [120320/251450]\tLoss: 0.0045 (0.0220) \tAcc: 100.00% (96.56%) \tEmb_Norm: 29.40 (31.42)\n",
      "Train Epoch: 5 [120960/251450]\tLoss: 0.0554 (0.0220) \tAcc: 96.88% (96.57%) \tEmb_Norm: 29.38 (31.41)\n",
      "Train Epoch: 5 [121600/251450]\tLoss: 0.0151 (0.0220) \tAcc: 96.88% (96.56%) \tEmb_Norm: 29.36 (31.40)\n",
      "Train Epoch: 5 [122240/251450]\tLoss: 0.0311 (0.0219) \tAcc: 93.75% (96.57%) \tEmb_Norm: 29.34 (31.39)\n",
      "Train Epoch: 5 [122880/251450]\tLoss: 0.0133 (0.0219) \tAcc: 100.00% (96.57%) \tEmb_Norm: 29.32 (31.37)\n",
      "Train Epoch: 5 [123520/251450]\tLoss: 0.0350 (0.0219) \tAcc: 93.75% (96.57%) \tEmb_Norm: 29.30 (31.36)\n",
      "Train Epoch: 5 [124160/251450]\tLoss: 0.0000 (0.0218) \tAcc: 100.00% (96.58%) \tEmb_Norm: 29.28 (31.35)\n",
      "Train Epoch: 5 [124800/251450]\tLoss: 0.0274 (0.0218) \tAcc: 96.88% (96.58%) \tEmb_Norm: 29.25 (31.34)\n",
      "Train Epoch: 5 [125440/251450]\tLoss: 0.0379 (0.0219) \tAcc: 90.62% (96.57%) \tEmb_Norm: 29.23 (31.33)\n",
      "Train Epoch: 5 [126080/251450]\tLoss: 0.0275 (0.0219) \tAcc: 96.88% (96.57%) \tEmb_Norm: 29.21 (31.32)\n",
      "Train Epoch: 5 [126720/251450]\tLoss: 0.0108 (0.0219) \tAcc: 100.00% (96.57%) \tEmb_Norm: 29.19 (31.31)\n",
      "Train Epoch: 5 [127360/251450]\tLoss: 0.0117 (0.0218) \tAcc: 96.88% (96.57%) \tEmb_Norm: 29.17 (31.30)\n",
      "Train Epoch: 5 [128000/251450]\tLoss: 0.0089 (0.0219) \tAcc: 96.88% (96.56%) \tEmb_Norm: 29.15 (31.29)\n",
      "Train Epoch: 5 [128640/251450]\tLoss: 0.0066 (0.0218) \tAcc: 100.00% (96.56%) \tEmb_Norm: 29.13 (31.28)\n",
      "Train Epoch: 5 [129280/251450]\tLoss: 0.0000 (0.0218) \tAcc: 100.00% (96.57%) \tEmb_Norm: 29.11 (31.27)\n",
      "Train Epoch: 5 [129920/251450]\tLoss: 0.0000 (0.0218) \tAcc: 100.00% (96.57%) \tEmb_Norm: 29.09 (31.26)\n",
      "Train Epoch: 5 [130560/251450]\tLoss: 0.0209 (0.0218) \tAcc: 96.88% (96.57%) \tEmb_Norm: 29.06 (31.25)\n",
      "Train Epoch: 5 [131200/251450]\tLoss: 0.0000 (0.0217) \tAcc: 100.00% (96.57%) \tEmb_Norm: 29.04 (31.24)\n",
      "Train Epoch: 5 [131840/251450]\tLoss: 0.0662 (0.0217) \tAcc: 90.62% (96.57%) \tEmb_Norm: 29.02 (31.22)\n",
      "Train Epoch: 5 [132480/251450]\tLoss: 0.0070 (0.0218) \tAcc: 100.00% (96.57%) \tEmb_Norm: 29.00 (31.21)\n",
      "Train Epoch: 5 [133120/251450]\tLoss: 0.0076 (0.0217) \tAcc: 100.00% (96.58%) \tEmb_Norm: 28.98 (31.20)\n",
      "Train Epoch: 5 [133760/251450]\tLoss: 0.0160 (0.0218) \tAcc: 96.88% (96.57%) \tEmb_Norm: 28.96 (31.19)\n",
      "Train Epoch: 5 [134400/251450]\tLoss: 0.0068 (0.0218) \tAcc: 100.00% (96.57%) \tEmb_Norm: 28.94 (31.18)\n",
      "Train Epoch: 5 [135040/251450]\tLoss: 0.0112 (0.0218) \tAcc: 96.88% (96.57%) \tEmb_Norm: 28.91 (31.17)\n",
      "Train Epoch: 5 [135680/251450]\tLoss: 0.0173 (0.0217) \tAcc: 93.75% (96.57%) \tEmb_Norm: 28.89 (31.16)\n",
      "Train Epoch: 5 [136320/251450]\tLoss: 0.0119 (0.0217) \tAcc: 96.88% (96.58%) \tEmb_Norm: 28.87 (31.15)\n",
      "Train Epoch: 5 [136960/251450]\tLoss: 0.0138 (0.0217) \tAcc: 96.88% (96.58%) \tEmb_Norm: 28.85 (31.14)\n",
      "Train Epoch: 5 [137600/251450]\tLoss: 0.0020 (0.0216) \tAcc: 100.00% (96.58%) \tEmb_Norm: 28.83 (31.13)\n",
      "Train Epoch: 5 [138240/251450]\tLoss: 0.0343 (0.0216) \tAcc: 93.75% (96.58%) \tEmb_Norm: 28.81 (31.12)\n",
      "Train Epoch: 5 [138880/251450]\tLoss: 0.0242 (0.0217) \tAcc: 93.75% (96.58%) \tEmb_Norm: 28.79 (31.11)\n",
      "Train Epoch: 5 [139520/251450]\tLoss: 0.0238 (0.0217) \tAcc: 96.88% (96.58%) \tEmb_Norm: 28.77 (31.10)\n",
      "Train Epoch: 5 [140160/251450]\tLoss: 0.0752 (0.0217) \tAcc: 90.62% (96.58%) \tEmb_Norm: 28.75 (31.09)\n",
      "Train Epoch: 5 [140800/251450]\tLoss: 0.0110 (0.0217) \tAcc: 96.88% (96.58%) \tEmb_Norm: 28.72 (31.08)\n",
      "Train Epoch: 5 [141440/251450]\tLoss: 0.0026 (0.0217) \tAcc: 100.00% (96.58%) \tEmb_Norm: 28.70 (31.06)\n",
      "Train Epoch: 5 [142080/251450]\tLoss: 0.0229 (0.0216) \tAcc: 96.88% (96.58%) \tEmb_Norm: 28.68 (31.05)\n",
      "Train Epoch: 5 [142720/251450]\tLoss: 0.0574 (0.0216) \tAcc: 90.62% (96.58%) \tEmb_Norm: 28.66 (31.04)\n",
      "Train Epoch: 5 [143360/251450]\tLoss: 0.0000 (0.0216) \tAcc: 100.00% (96.59%) \tEmb_Norm: 28.64 (31.03)\n",
      "Train Epoch: 5 [144000/251450]\tLoss: 0.0000 (0.0216) \tAcc: 100.00% (96.59%) \tEmb_Norm: 28.62 (31.02)\n",
      "Train Epoch: 5 [144640/251450]\tLoss: 0.0300 (0.0216) \tAcc: 93.75% (96.58%) \tEmb_Norm: 28.59 (31.01)\n",
      "Train Epoch: 5 [145280/251450]\tLoss: 0.0273 (0.0216) \tAcc: 93.75% (96.58%) \tEmb_Norm: 28.57 (31.00)\n",
      "Train Epoch: 5 [145920/251450]\tLoss: 0.0212 (0.0216) \tAcc: 93.75% (96.59%) \tEmb_Norm: 28.55 (30.99)\n",
      "Train Epoch: 5 [146560/251450]\tLoss: 0.0000 (0.0216) \tAcc: 100.00% (96.59%) \tEmb_Norm: 28.53 (30.98)\n",
      "Train Epoch: 5 [147200/251450]\tLoss: 0.0117 (0.0216) \tAcc: 100.00% (96.59%) \tEmb_Norm: 28.51 (30.97)\n",
      "Train Epoch: 5 [147840/251450]\tLoss: 0.0018 (0.0215) \tAcc: 100.00% (96.59%) \tEmb_Norm: 28.49 (30.96)\n",
      "Train Epoch: 5 [148480/251450]\tLoss: 0.0031 (0.0215) \tAcc: 100.00% (96.60%) \tEmb_Norm: 28.47 (30.95)\n",
      "Train Epoch: 5 [149120/251450]\tLoss: 0.0189 (0.0215) \tAcc: 96.88% (96.60%) \tEmb_Norm: 28.45 (30.94)\n",
      "Train Epoch: 5 [149760/251450]\tLoss: 0.0175 (0.0215) \tAcc: 96.88% (96.60%) \tEmb_Norm: 28.43 (30.93)\n",
      "Train Epoch: 5 [150400/251450]\tLoss: 0.0206 (0.0215) \tAcc: 96.88% (96.60%) \tEmb_Norm: 28.41 (30.91)\n",
      "Train Epoch: 5 [151040/251450]\tLoss: 0.0122 (0.0215) \tAcc: 96.88% (96.60%) \tEmb_Norm: 28.39 (30.90)\n",
      "Train Epoch: 5 [151680/251450]\tLoss: 0.0062 (0.0214) \tAcc: 100.00% (96.61%) \tEmb_Norm: 28.37 (30.89)\n",
      "Train Epoch: 5 [152320/251450]\tLoss: 0.0000 (0.0214) \tAcc: 100.00% (96.61%) \tEmb_Norm: 28.35 (30.88)\n",
      "Train Epoch: 5 [152960/251450]\tLoss: 0.0000 (0.0213) \tAcc: 100.00% (96.62%) \tEmb_Norm: 28.33 (30.87)\n",
      "Train Epoch: 5 [153600/251450]\tLoss: 0.0478 (0.0213) \tAcc: 90.62% (96.62%) \tEmb_Norm: 28.30 (30.86)\n",
      "Train Epoch: 5 [154240/251450]\tLoss: 0.0127 (0.0213) \tAcc: 96.88% (96.62%) \tEmb_Norm: 28.28 (30.85)\n",
      "Train Epoch: 5 [154880/251450]\tLoss: 0.0060 (0.0213) \tAcc: 100.00% (96.62%) \tEmb_Norm: 28.26 (30.84)\n",
      "Train Epoch: 5 [155520/251450]\tLoss: 0.0378 (0.0214) \tAcc: 93.75% (96.61%) \tEmb_Norm: 28.24 (30.83)\n",
      "Train Epoch: 5 [156160/251450]\tLoss: 0.0000 (0.0213) \tAcc: 100.00% (96.61%) \tEmb_Norm: 28.22 (30.82)\n",
      "Train Epoch: 5 [156800/251450]\tLoss: 0.0000 (0.0213) \tAcc: 100.00% (96.61%) \tEmb_Norm: 28.20 (30.81)\n",
      "Train Epoch: 5 [157440/251450]\tLoss: 0.0470 (0.0214) \tAcc: 96.88% (96.61%) \tEmb_Norm: 28.17 (30.80)\n",
      "Train Epoch: 5 [158080/251450]\tLoss: 0.0000 (0.0213) \tAcc: 100.00% (96.61%) \tEmb_Norm: 28.15 (30.79)\n",
      "Train Epoch: 5 [158720/251450]\tLoss: 0.0153 (0.0214) \tAcc: 96.88% (96.61%) \tEmb_Norm: 28.13 (30.78)\n",
      "Train Epoch: 5 [159360/251450]\tLoss: 0.0543 (0.0214) \tAcc: 87.50% (96.61%) \tEmb_Norm: 28.11 (30.77)\n",
      "Train Epoch: 5 [160000/251450]\tLoss: 0.0408 (0.0213) \tAcc: 90.62% (96.62%) \tEmb_Norm: 28.09 (30.75)\n",
      "Train Epoch: 5 [160640/251450]\tLoss: 0.0218 (0.0213) \tAcc: 96.88% (96.62%) \tEmb_Norm: 28.07 (30.74)\n",
      "Train Epoch: 5 [161280/251450]\tLoss: 0.0078 (0.0213) \tAcc: 96.88% (96.62%) \tEmb_Norm: 28.05 (30.73)\n",
      "Train Epoch: 5 [161920/251450]\tLoss: 0.0031 (0.0213) \tAcc: 100.00% (96.62%) \tEmb_Norm: 28.03 (30.72)\n",
      "Train Epoch: 5 [162560/251450]\tLoss: 0.0100 (0.0213) \tAcc: 96.88% (96.62%) \tEmb_Norm: 28.00 (30.71)\n",
      "Train Epoch: 5 [163200/251450]\tLoss: 0.0412 (0.0213) \tAcc: 93.75% (96.62%) \tEmb_Norm: 27.98 (30.70)\n",
      "Train Epoch: 5 [163840/251450]\tLoss: 0.0108 (0.0213) \tAcc: 96.88% (96.62%) \tEmb_Norm: 27.96 (30.69)\n",
      "Train Epoch: 5 [164480/251450]\tLoss: 0.0113 (0.0212) \tAcc: 96.88% (96.63%) \tEmb_Norm: 27.94 (30.68)\n",
      "Train Epoch: 5 [165120/251450]\tLoss: 0.0045 (0.0212) \tAcc: 100.00% (96.63%) \tEmb_Norm: 27.92 (30.67)\n",
      "Train Epoch: 5 [165760/251450]\tLoss: 0.0061 (0.0212) \tAcc: 100.00% (96.64%) \tEmb_Norm: 27.90 (30.66)\n",
      "Train Epoch: 5 [166400/251450]\tLoss: 0.0000 (0.0212) \tAcc: 100.00% (96.64%) \tEmb_Norm: 27.88 (30.65)\n",
      "Train Epoch: 5 [167040/251450]\tLoss: 0.0231 (0.0211) \tAcc: 96.88% (96.65%) \tEmb_Norm: 27.86 (30.64)\n",
      "Train Epoch: 5 [167680/251450]\tLoss: 0.0137 (0.0211) \tAcc: 96.88% (96.65%) \tEmb_Norm: 27.84 (30.63)\n",
      "Train Epoch: 5 [168320/251450]\tLoss: 0.0176 (0.0211) \tAcc: 96.88% (96.65%) \tEmb_Norm: 27.82 (30.62)\n",
      "Train Epoch: 5 [168960/251450]\tLoss: 0.0221 (0.0211) \tAcc: 93.75% (96.65%) \tEmb_Norm: 27.80 (30.61)\n",
      "Train Epoch: 5 [169600/251450]\tLoss: 0.0678 (0.0211) \tAcc: 84.38% (96.65%) \tEmb_Norm: 27.78 (30.60)\n",
      "Train Epoch: 5 [170240/251450]\tLoss: 0.0117 (0.0211) \tAcc: 96.88% (96.65%) \tEmb_Norm: 27.76 (30.58)\n",
      "Train Epoch: 5 [170880/251450]\tLoss: 0.0006 (0.0211) \tAcc: 100.00% (96.65%) \tEmb_Norm: 27.73 (30.57)\n",
      "Train Epoch: 5 [171520/251450]\tLoss: 0.0359 (0.0211) \tAcc: 90.62% (96.64%) \tEmb_Norm: 27.71 (30.56)\n",
      "Train Epoch: 5 [172160/251450]\tLoss: 0.0000 (0.0211) \tAcc: 100.00% (96.65%) \tEmb_Norm: 27.69 (30.55)\n",
      "Train Epoch: 5 [172800/251450]\tLoss: 0.0309 (0.0211) \tAcc: 93.75% (96.65%) \tEmb_Norm: 27.67 (30.54)\n",
      "Train Epoch: 5 [173440/251450]\tLoss: 0.0263 (0.0211) \tAcc: 96.88% (96.65%) \tEmb_Norm: 27.65 (30.53)\n",
      "Train Epoch: 5 [174080/251450]\tLoss: 0.0000 (0.0211) \tAcc: 100.00% (96.65%) \tEmb_Norm: 27.63 (30.52)\n",
      "Train Epoch: 5 [174720/251450]\tLoss: 0.0000 (0.0211) \tAcc: 100.00% (96.66%) \tEmb_Norm: 27.60 (30.51)\n",
      "Train Epoch: 5 [175360/251450]\tLoss: 0.0084 (0.0211) \tAcc: 96.88% (96.66%) \tEmb_Norm: 27.58 (30.50)\n",
      "Train Epoch: 5 [176000/251450]\tLoss: 0.0000 (0.0210) \tAcc: 100.00% (96.66%) \tEmb_Norm: 27.56 (30.49)\n",
      "Train Epoch: 5 [176640/251450]\tLoss: 0.0242 (0.0211) \tAcc: 93.75% (96.65%) \tEmb_Norm: 27.54 (30.48)\n",
      "Train Epoch: 5 [177280/251450]\tLoss: 0.0000 (0.0211) \tAcc: 100.00% (96.65%) \tEmb_Norm: 27.52 (30.47)\n",
      "Train Epoch: 5 [177920/251450]\tLoss: 0.0000 (0.0211) \tAcc: 100.00% (96.66%) \tEmb_Norm: 27.50 (30.46)\n",
      "Train Epoch: 5 [178560/251450]\tLoss: 0.0000 (0.0210) \tAcc: 100.00% (96.66%) \tEmb_Norm: 27.48 (30.45)\n",
      "Train Epoch: 5 [179200/251450]\tLoss: 0.0252 (0.0210) \tAcc: 90.62% (96.66%) \tEmb_Norm: 27.46 (30.44)\n",
      "Train Epoch: 5 [179840/251450]\tLoss: 0.0221 (0.0210) \tAcc: 96.88% (96.66%) \tEmb_Norm: 27.44 (30.42)\n",
      "Train Epoch: 5 [180480/251450]\tLoss: 0.0000 (0.0210) \tAcc: 100.00% (96.67%) \tEmb_Norm: 27.42 (30.41)\n",
      "Train Epoch: 5 [181120/251450]\tLoss: 0.0039 (0.0210) \tAcc: 100.00% (96.67%) \tEmb_Norm: 27.39 (30.40)\n",
      "Train Epoch: 5 [181760/251450]\tLoss: 0.0063 (0.0210) \tAcc: 96.88% (96.67%) \tEmb_Norm: 27.37 (30.39)\n",
      "Train Epoch: 5 [182400/251450]\tLoss: 0.0642 (0.0210) \tAcc: 93.75% (96.67%) \tEmb_Norm: 27.35 (30.38)\n",
      "Train Epoch: 5 [183040/251450]\tLoss: 0.0000 (0.0210) \tAcc: 100.00% (96.67%) \tEmb_Norm: 27.33 (30.37)\n",
      "Train Epoch: 5 [183680/251450]\tLoss: 0.0000 (0.0210) \tAcc: 100.00% (96.67%) \tEmb_Norm: 27.31 (30.36)\n",
      "Train Epoch: 5 [184320/251450]\tLoss: 0.0129 (0.0210) \tAcc: 96.88% (96.67%) \tEmb_Norm: 27.29 (30.35)\n",
      "Train Epoch: 5 [184960/251450]\tLoss: 0.0090 (0.0210) \tAcc: 100.00% (96.67%) \tEmb_Norm: 27.26 (30.34)\n",
      "Train Epoch: 5 [185600/251450]\tLoss: 0.0210 (0.0210) \tAcc: 96.88% (96.67%) \tEmb_Norm: 27.24 (30.33)\n",
      "Train Epoch: 5 [186240/251450]\tLoss: 0.0134 (0.0210) \tAcc: 96.88% (96.67%) \tEmb_Norm: 27.22 (30.32)\n",
      "Train Epoch: 5 [186880/251450]\tLoss: 0.0000 (0.0210) \tAcc: 100.00% (96.67%) \tEmb_Norm: 27.20 (30.31)\n",
      "Train Epoch: 5 [187520/251450]\tLoss: 0.0018 (0.0209) \tAcc: 100.00% (96.67%) \tEmb_Norm: 27.18 (30.30)\n",
      "Train Epoch: 5 [188160/251450]\tLoss: 0.0052 (0.0209) \tAcc: 100.00% (96.68%) \tEmb_Norm: 27.16 (30.29)\n",
      "Train Epoch: 5 [188800/251450]\tLoss: 0.0202 (0.0209) \tAcc: 96.88% (96.68%) \tEmb_Norm: 27.14 (30.28)\n",
      "Train Epoch: 5 [189440/251450]\tLoss: 0.0391 (0.0209) \tAcc: 93.75% (96.68%) \tEmb_Norm: 27.12 (30.27)\n",
      "Train Epoch: 5 [190080/251450]\tLoss: 0.0082 (0.0208) \tAcc: 96.88% (96.68%) \tEmb_Norm: 27.10 (30.25)\n",
      "Train Epoch: 5 [190720/251450]\tLoss: 0.0229 (0.0208) \tAcc: 96.88% (96.69%) \tEmb_Norm: 27.08 (30.24)\n",
      "Train Epoch: 5 [191360/251450]\tLoss: 0.0014 (0.0208) \tAcc: 100.00% (96.69%) \tEmb_Norm: 27.06 (30.23)\n",
      "Train Epoch: 5 [192000/251450]\tLoss: 0.0000 (0.0208) \tAcc: 100.00% (96.69%) \tEmb_Norm: 27.04 (30.22)\n",
      "Train Epoch: 5 [192640/251450]\tLoss: 0.0864 (0.0208) \tAcc: 81.25% (96.69%) \tEmb_Norm: 27.02 (30.21)\n",
      "Train Epoch: 5 [193280/251450]\tLoss: 0.0029 (0.0208) \tAcc: 100.00% (96.69%) \tEmb_Norm: 26.99 (30.20)\n",
      "Train Epoch: 5 [193920/251450]\tLoss: 0.0148 (0.0208) \tAcc: 96.88% (96.69%) \tEmb_Norm: 26.97 (30.19)\n",
      "Train Epoch: 5 [194560/251450]\tLoss: 0.1040 (0.0208) \tAcc: 84.38% (96.68%) \tEmb_Norm: 26.95 (30.18)\n",
      "Train Epoch: 5 [195200/251450]\tLoss: 0.0139 (0.0208) \tAcc: 96.88% (96.69%) \tEmb_Norm: 26.93 (30.17)\n",
      "Train Epoch: 5 [195840/251450]\tLoss: 0.0103 (0.0208) \tAcc: 96.88% (96.68%) \tEmb_Norm: 26.91 (30.16)\n",
      "Train Epoch: 5 [196480/251450]\tLoss: 0.0000 (0.0208) \tAcc: 100.00% (96.69%) \tEmb_Norm: 26.88 (30.15)\n",
      "Train Epoch: 5 [197120/251450]\tLoss: 0.0052 (0.0208) \tAcc: 100.00% (96.69%) \tEmb_Norm: 26.86 (30.14)\n",
      "Train Epoch: 5 [197760/251450]\tLoss: 0.0025 (0.0208) \tAcc: 100.00% (96.69%) \tEmb_Norm: 26.84 (30.13)\n",
      "Train Epoch: 5 [198400/251450]\tLoss: 0.0224 (0.0208) \tAcc: 96.88% (96.69%) \tEmb_Norm: 26.82 (30.12)\n",
      "Train Epoch: 5 [199040/251450]\tLoss: 0.0328 (0.0208) \tAcc: 90.62% (96.69%) \tEmb_Norm: 26.80 (30.11)\n",
      "Train Epoch: 5 [199680/251450]\tLoss: 0.0415 (0.0208) \tAcc: 96.88% (96.69%) \tEmb_Norm: 26.78 (30.10)\n",
      "Train Epoch: 5 [200320/251450]\tLoss: 0.0168 (0.0208) \tAcc: 100.00% (96.68%) \tEmb_Norm: 26.76 (30.08)\n",
      "Train Epoch: 5 [200960/251450]\tLoss: 0.0028 (0.0208) \tAcc: 100.00% (96.68%) \tEmb_Norm: 26.73 (30.07)\n",
      "Train Epoch: 5 [201600/251450]\tLoss: 0.0082 (0.0208) \tAcc: 100.00% (96.68%) \tEmb_Norm: 26.71 (30.06)\n",
      "=============== TEST epoch 5 ===============\n",
      "\n",
      "Test set: Average loss: 0.0239, Accuracy: 95.88%\n",
      "\n",
      "=============== checkpoint epoch 5 ===============\n",
      "=============== TRAIN epoch 6 ===============\n",
      "Train Epoch: 6 [0/251450]\tLoss: 0.0000 (0.0000) \tAcc: 100.00% (100.00%) \tEmb_Norm: 26.70 (26.70)\n",
      "Train Epoch: 6 [640/251450]\tLoss: 0.0253 (0.0180) \tAcc: 96.88% (95.98%) \tEmb_Norm: 26.68 (26.69)\n",
      "Train Epoch: 6 [1280/251450]\tLoss: 0.0033 (0.0165) \tAcc: 100.00% (96.72%) \tEmb_Norm: 26.66 (26.68)\n",
      "Train Epoch: 6 [1920/251450]\tLoss: 0.0000 (0.0169) \tAcc: 100.00% (96.88%) \tEmb_Norm: 26.63 (26.67)\n",
      "Train Epoch: 6 [2560/251450]\tLoss: 0.0204 (0.0177) \tAcc: 96.88% (96.88%) \tEmb_Norm: 26.61 (26.66)\n",
      "Train Epoch: 6 [3200/251450]\tLoss: 0.0082 (0.0167) \tAcc: 96.88% (96.97%) \tEmb_Norm: 26.59 (26.65)\n",
      "Train Epoch: 6 [3840/251450]\tLoss: 0.0407 (0.0161) \tAcc: 96.88% (96.98%) \tEmb_Norm: 26.57 (26.64)\n",
      "Train Epoch: 6 [4480/251450]\tLoss: 0.0024 (0.0154) \tAcc: 100.00% (97.14%) \tEmb_Norm: 26.55 (26.62)\n",
      "Train Epoch: 6 [5120/251450]\tLoss: 0.0000 (0.0149) \tAcc: 100.00% (97.19%) \tEmb_Norm: 26.53 (26.61)\n",
      "Train Epoch: 6 [5760/251450]\tLoss: 0.0221 (0.0148) \tAcc: 93.75% (97.15%) \tEmb_Norm: 26.51 (26.60)\n",
      "Train Epoch: 6 [6400/251450]\tLoss: 0.0042 (0.0158) \tAcc: 100.00% (97.05%) \tEmb_Norm: 26.49 (26.59)\n",
      "Train Epoch: 6 [7040/251450]\tLoss: 0.1152 (0.0170) \tAcc: 87.50% (96.90%) \tEmb_Norm: 26.47 (26.58)\n",
      "Train Epoch: 6 [7680/251450]\tLoss: 0.0094 (0.0173) \tAcc: 96.88% (96.88%) \tEmb_Norm: 26.45 (26.57)\n",
      "Train Epoch: 6 [8320/251450]\tLoss: 0.0414 (0.0171) \tAcc: 93.75% (96.91%) \tEmb_Norm: 26.43 (26.56)\n",
      "Train Epoch: 6 [8960/251450]\tLoss: 0.0267 (0.0173) \tAcc: 93.75% (96.90%) \tEmb_Norm: 26.41 (26.55)\n",
      "Train Epoch: 6 [9600/251450]\tLoss: 0.0278 (0.0175) \tAcc: 93.75% (96.88%) \tEmb_Norm: 26.38 (26.54)\n",
      "Train Epoch: 6 [10240/251450]\tLoss: 0.0324 (0.0178) \tAcc: 90.62% (96.83%) \tEmb_Norm: 26.36 (26.53)\n",
      "Train Epoch: 6 [10880/251450]\tLoss: 0.0115 (0.0177) \tAcc: 100.00% (96.88%) \tEmb_Norm: 26.34 (26.52)\n",
      "Train Epoch: 6 [11520/251450]\tLoss: 0.0967 (0.0178) \tAcc: 87.50% (96.88%) \tEmb_Norm: 26.32 (26.51)\n",
      "Train Epoch: 6 [12160/251450]\tLoss: 0.0000 (0.0179) \tAcc: 100.00% (96.83%) \tEmb_Norm: 26.30 (26.50)\n",
      "Train Epoch: 6 [12800/251450]\tLoss: 0.0219 (0.0179) \tAcc: 93.75% (96.86%) \tEmb_Norm: 26.28 (26.49)\n",
      "Train Epoch: 6 [13440/251450]\tLoss: 0.0339 (0.0176) \tAcc: 93.75% (96.90%) \tEmb_Norm: 26.26 (26.48)\n",
      "Train Epoch: 6 [14080/251450]\tLoss: 0.0145 (0.0177) \tAcc: 96.88% (96.92%) \tEmb_Norm: 26.24 (26.47)\n",
      "Train Epoch: 6 [14720/251450]\tLoss: 0.0031 (0.0175) \tAcc: 100.00% (96.99%) \tEmb_Norm: 26.22 (26.46)\n",
      "Train Epoch: 6 [15360/251450]\tLoss: 0.0315 (0.0176) \tAcc: 96.88% (96.99%) \tEmb_Norm: 26.20 (26.45)\n",
      "Train Epoch: 6 [16000/251450]\tLoss: 0.0000 (0.0176) \tAcc: 100.00% (96.97%) \tEmb_Norm: 26.17 (26.44)\n",
      "Train Epoch: 6 [16640/251450]\tLoss: 0.0045 (0.0176) \tAcc: 100.00% (96.98%) \tEmb_Norm: 26.15 (26.43)\n",
      "Train Epoch: 6 [17280/251450]\tLoss: 0.0062 (0.0174) \tAcc: 100.00% (97.01%) \tEmb_Norm: 26.13 (26.42)\n",
      "Train Epoch: 6 [17920/251450]\tLoss: 0.0110 (0.0170) \tAcc: 96.88% (97.07%) \tEmb_Norm: 26.11 (26.41)\n",
      "Train Epoch: 6 [18560/251450]\tLoss: 0.0021 (0.0170) \tAcc: 100.00% (97.07%) \tEmb_Norm: 26.09 (26.39)\n",
      "Train Epoch: 6 [19200/251450]\tLoss: 0.0172 (0.0172) \tAcc: 96.88% (97.05%) \tEmb_Norm: 26.07 (26.38)\n",
      "Train Epoch: 6 [19840/251450]\tLoss: 0.0002 (0.0170) \tAcc: 100.00% (97.07%) \tEmb_Norm: 26.05 (26.37)\n",
      "Train Epoch: 6 [20480/251450]\tLoss: 0.0031 (0.0173) \tAcc: 100.00% (97.02%) \tEmb_Norm: 26.03 (26.36)\n",
      "Train Epoch: 6 [21120/251450]\tLoss: 0.0401 (0.0173) \tAcc: 93.75% (96.98%) \tEmb_Norm: 26.01 (26.35)\n",
      "Train Epoch: 6 [21760/251450]\tLoss: 0.0018 (0.0175) \tAcc: 100.00% (96.97%) \tEmb_Norm: 25.99 (26.34)\n",
      "Train Epoch: 6 [22400/251450]\tLoss: 0.0062 (0.0174) \tAcc: 100.00% (96.98%) \tEmb_Norm: 25.97 (26.33)\n",
      "Train Epoch: 6 [23040/251450]\tLoss: 0.0081 (0.0173) \tAcc: 100.00% (97.02%) \tEmb_Norm: 25.95 (26.32)\n",
      "Train Epoch: 6 [23680/251450]\tLoss: 0.0207 (0.0172) \tAcc: 96.88% (97.04%) \tEmb_Norm: 25.92 (26.31)\n",
      "Train Epoch: 6 [24320/251450]\tLoss: 0.0040 (0.0171) \tAcc: 100.00% (97.05%) \tEmb_Norm: 25.90 (26.30)\n",
      "Train Epoch: 6 [24960/251450]\tLoss: 0.0468 (0.0175) \tAcc: 93.75% (96.99%) \tEmb_Norm: 25.88 (26.29)\n",
      "Train Epoch: 6 [25600/251450]\tLoss: 0.0123 (0.0175) \tAcc: 96.88% (96.99%) \tEmb_Norm: 25.86 (26.28)\n",
      "Train Epoch: 6 [26240/251450]\tLoss: 0.0000 (0.0175) \tAcc: 100.00% (96.98%) \tEmb_Norm: 25.84 (26.27)\n",
      "Train Epoch: 6 [26880/251450]\tLoss: 0.0794 (0.0177) \tAcc: 96.88% (96.95%) \tEmb_Norm: 25.82 (26.26)\n",
      "Train Epoch: 6 [27520/251450]\tLoss: 0.0000 (0.0175) \tAcc: 100.00% (96.96%) \tEmb_Norm: 25.80 (26.25)\n",
      "Train Epoch: 6 [28160/251450]\tLoss: 0.0000 (0.0175) \tAcc: 100.00% (96.96%) \tEmb_Norm: 25.78 (26.24)\n",
      "Train Epoch: 6 [28800/251450]\tLoss: 0.0182 (0.0175) \tAcc: 96.88% (96.97%) \tEmb_Norm: 25.76 (26.23)\n",
      "Train Epoch: 6 [29440/251450]\tLoss: 0.0211 (0.0175) \tAcc: 96.88% (96.98%) \tEmb_Norm: 25.74 (26.22)\n",
      "Train Epoch: 6 [30080/251450]\tLoss: 0.0001 (0.0174) \tAcc: 100.00% (96.99%) \tEmb_Norm: 25.71 (26.21)\n",
      "Train Epoch: 6 [30720/251450]\tLoss: 0.0000 (0.0174) \tAcc: 100.00% (96.99%) \tEmb_Norm: 25.69 (26.20)\n",
      "Train Epoch: 6 [31360/251450]\tLoss: 0.0049 (0.0175) \tAcc: 100.00% (96.98%) \tEmb_Norm: 25.67 (26.19)\n",
      "Train Epoch: 6 [32000/251450]\tLoss: 0.0472 (0.0176) \tAcc: 87.50% (96.94%) \tEmb_Norm: 25.65 (26.18)\n",
      "Train Epoch: 6 [32640/251450]\tLoss: 0.0000 (0.0175) \tAcc: 100.00% (96.95%) \tEmb_Norm: 25.63 (26.16)\n",
      "Train Epoch: 6 [33280/251450]\tLoss: 0.0274 (0.0176) \tAcc: 96.88% (96.95%) \tEmb_Norm: 25.61 (26.15)\n",
      "Train Epoch: 6 [33920/251450]\tLoss: 0.0000 (0.0176) \tAcc: 100.00% (96.96%) \tEmb_Norm: 25.59 (26.14)\n",
      "Train Epoch: 6 [34560/251450]\tLoss: 0.0247 (0.0176) \tAcc: 96.88% (96.95%) \tEmb_Norm: 25.57 (26.13)\n",
      "Train Epoch: 6 [35200/251450]\tLoss: 0.0199 (0.0177) \tAcc: 96.88% (96.95%) \tEmb_Norm: 25.55 (26.12)\n",
      "Train Epoch: 6 [35840/251450]\tLoss: 0.0729 (0.0176) \tAcc: 87.50% (96.96%) \tEmb_Norm: 25.52 (26.11)\n",
      "Train Epoch: 6 [36480/251450]\tLoss: 0.0076 (0.0176) \tAcc: 96.88% (96.96%) \tEmb_Norm: 25.50 (26.10)\n",
      "Train Epoch: 6 [37120/251450]\tLoss: 0.0000 (0.0176) \tAcc: 100.00% (96.95%) \tEmb_Norm: 25.48 (26.09)\n",
      "Train Epoch: 6 [37760/251450]\tLoss: 0.0000 (0.0176) \tAcc: 100.00% (96.95%) \tEmb_Norm: 25.46 (26.08)\n",
      "Train Epoch: 6 [38400/251450]\tLoss: 0.0202 (0.0176) \tAcc: 96.88% (96.95%) \tEmb_Norm: 25.44 (26.07)\n",
      "Train Epoch: 6 [39040/251450]\tLoss: 0.0000 (0.0175) \tAcc: 100.00% (96.96%) \tEmb_Norm: 25.42 (26.06)\n",
      "Train Epoch: 6 [39680/251450]\tLoss: 0.0237 (0.0176) \tAcc: 96.88% (96.97%) \tEmb_Norm: 25.40 (26.05)\n",
      "Train Epoch: 6 [40320/251450]\tLoss: 0.0193 (0.0177) \tAcc: 93.75% (96.94%) \tEmb_Norm: 25.38 (26.04)\n",
      "Train Epoch: 6 [40960/251450]\tLoss: 0.0340 (0.0177) \tAcc: 93.75% (96.93%) \tEmb_Norm: 25.35 (26.03)\n",
      "Train Epoch: 6 [41600/251450]\tLoss: 0.0000 (0.0177) \tAcc: 100.00% (96.94%) \tEmb_Norm: 25.33 (26.02)\n",
      "Train Epoch: 6 [42240/251450]\tLoss: 0.0272 (0.0177) \tAcc: 90.62% (96.94%) \tEmb_Norm: 25.31 (26.01)\n",
      "Train Epoch: 6 [42880/251450]\tLoss: 0.0066 (0.0177) \tAcc: 100.00% (96.95%) \tEmb_Norm: 25.29 (26.00)\n",
      "Train Epoch: 6 [43520/251450]\tLoss: 0.0000 (0.0176) \tAcc: 100.00% (96.96%) \tEmb_Norm: 25.27 (25.99)\n",
      "Train Epoch: 6 [44160/251450]\tLoss: 0.0484 (0.0177) \tAcc: 90.62% (96.95%) \tEmb_Norm: 25.25 (25.98)\n",
      "Train Epoch: 6 [44800/251450]\tLoss: 0.0108 (0.0177) \tAcc: 100.00% (96.94%) \tEmb_Norm: 25.23 (25.97)\n",
      "Train Epoch: 6 [45440/251450]\tLoss: 0.0307 (0.0178) \tAcc: 93.75% (96.93%) \tEmb_Norm: 25.21 (25.95)\n",
      "Train Epoch: 6 [46080/251450]\tLoss: 0.0548 (0.0177) \tAcc: 90.62% (96.94%) \tEmb_Norm: 25.19 (25.94)\n",
      "Train Epoch: 6 [46720/251450]\tLoss: 0.0322 (0.0177) \tAcc: 93.75% (96.95%) \tEmb_Norm: 25.17 (25.93)\n",
      "Train Epoch: 6 [47360/251450]\tLoss: 0.0209 (0.0177) \tAcc: 96.88% (96.96%) \tEmb_Norm: 25.15 (25.92)\n",
      "Train Epoch: 6 [48000/251450]\tLoss: 0.0238 (0.0177) \tAcc: 93.75% (96.95%) \tEmb_Norm: 25.12 (25.91)\n",
      "Train Epoch: 6 [48640/251450]\tLoss: 0.0000 (0.0177) \tAcc: 100.00% (96.95%) \tEmb_Norm: 25.10 (25.90)\n",
      "Train Epoch: 6 [49280/251450]\tLoss: 0.0376 (0.0177) \tAcc: 93.75% (96.95%) \tEmb_Norm: 25.08 (25.89)\n",
      "Train Epoch: 6 [49920/251450]\tLoss: 0.0122 (0.0178) \tAcc: 100.00% (96.94%) \tEmb_Norm: 25.06 (25.88)\n",
      "Train Epoch: 6 [50560/251450]\tLoss: 0.0034 (0.0179) \tAcc: 100.00% (96.93%) \tEmb_Norm: 25.04 (25.87)\n",
      "Train Epoch: 6 [51200/251450]\tLoss: 0.0204 (0.0179) \tAcc: 96.88% (96.93%) \tEmb_Norm: 25.02 (25.86)\n",
      "Train Epoch: 6 [51840/251450]\tLoss: 0.0114 (0.0179) \tAcc: 96.88% (96.93%) \tEmb_Norm: 25.00 (25.85)\n",
      "Train Epoch: 6 [52480/251450]\tLoss: 0.0010 (0.0178) \tAcc: 100.00% (96.94%) \tEmb_Norm: 24.98 (25.84)\n",
      "Train Epoch: 6 [53120/251450]\tLoss: 0.0000 (0.0178) \tAcc: 100.00% (96.94%) \tEmb_Norm: 24.96 (25.83)\n",
      "Train Epoch: 6 [53760/251450]\tLoss: 0.0000 (0.0178) \tAcc: 100.00% (96.95%) \tEmb_Norm: 24.94 (25.82)\n",
      "Train Epoch: 6 [54400/251450]\tLoss: 0.0241 (0.0178) \tAcc: 96.88% (96.95%) \tEmb_Norm: 24.91 (25.81)\n",
      "Train Epoch: 6 [55040/251450]\tLoss: 0.0000 (0.0177) \tAcc: 100.00% (96.96%) \tEmb_Norm: 24.89 (25.80)\n",
      "Train Epoch: 6 [55680/251450]\tLoss: 0.0000 (0.0177) \tAcc: 100.00% (96.97%) \tEmb_Norm: 24.87 (25.79)\n",
      "Train Epoch: 6 [56320/251450]\tLoss: 0.0181 (0.0177) \tAcc: 96.88% (96.97%) \tEmb_Norm: 24.85 (25.78)\n",
      "Train Epoch: 6 [56960/251450]\tLoss: 0.0000 (0.0177) \tAcc: 100.00% (96.97%) \tEmb_Norm: 24.83 (25.77)\n",
      "Train Epoch: 6 [57600/251450]\tLoss: 0.0397 (0.0177) \tAcc: 93.75% (96.98%) \tEmb_Norm: 24.81 (25.76)\n",
      "Train Epoch: 6 [58240/251450]\tLoss: 0.0036 (0.0176) \tAcc: 100.00% (96.99%) \tEmb_Norm: 24.79 (25.74)\n",
      "Train Epoch: 6 [58880/251450]\tLoss: 0.0103 (0.0176) \tAcc: 96.88% (96.98%) \tEmb_Norm: 24.77 (25.73)\n",
      "Train Epoch: 6 [59520/251450]\tLoss: 0.0000 (0.0176) \tAcc: 100.00% (96.98%) \tEmb_Norm: 24.75 (25.72)\n",
      "Train Epoch: 6 [60160/251450]\tLoss: 0.0059 (0.0175) \tAcc: 100.00% (97.00%) \tEmb_Norm: 24.73 (25.71)\n",
      "Train Epoch: 6 [60800/251450]\tLoss: 0.0013 (0.0174) \tAcc: 100.00% (97.01%) \tEmb_Norm: 24.71 (25.70)\n",
      "Train Epoch: 6 [61440/251450]\tLoss: 0.0334 (0.0175) \tAcc: 93.75% (97.01%) \tEmb_Norm: 24.69 (25.69)\n",
      "Train Epoch: 6 [62080/251450]\tLoss: 0.0503 (0.0175) \tAcc: 93.75% (97.01%) \tEmb_Norm: 24.67 (25.68)\n",
      "Train Epoch: 6 [62720/251450]\tLoss: 0.0507 (0.0175) \tAcc: 90.62% (96.99%) \tEmb_Norm: 24.65 (25.67)\n",
      "Train Epoch: 6 [63360/251450]\tLoss: 0.0056 (0.0175) \tAcc: 100.00% (97.00%) \tEmb_Norm: 24.62 (25.66)\n",
      "Train Epoch: 6 [64000/251450]\tLoss: 0.0000 (0.0174) \tAcc: 100.00% (97.01%) \tEmb_Norm: 24.61 (25.65)\n",
      "Train Epoch: 6 [64640/251450]\tLoss: 0.0204 (0.0175) \tAcc: 96.88% (97.01%) \tEmb_Norm: 24.58 (25.64)\n",
      "Train Epoch: 6 [65280/251450]\tLoss: 0.0551 (0.0175) \tAcc: 90.62% (97.01%) \tEmb_Norm: 24.56 (25.63)\n",
      "Train Epoch: 6 [65920/251450]\tLoss: 0.0380 (0.0176) \tAcc: 96.88% (97.00%) \tEmb_Norm: 24.54 (25.62)\n",
      "Train Epoch: 6 [66560/251450]\tLoss: 0.0103 (0.0176) \tAcc: 100.00% (97.00%) \tEmb_Norm: 24.52 (25.61)\n",
      "Train Epoch: 6 [67200/251450]\tLoss: 0.0147 (0.0175) \tAcc: 96.88% (97.00%) \tEmb_Norm: 24.50 (25.60)\n",
      "Train Epoch: 6 [67840/251450]\tLoss: 0.0012 (0.0175) \tAcc: 100.00% (96.99%) \tEmb_Norm: 24.48 (25.59)\n",
      "Train Epoch: 6 [68480/251450]\tLoss: 0.0083 (0.0175) \tAcc: 100.00% (97.00%) \tEmb_Norm: 24.46 (25.58)\n",
      "Train Epoch: 6 [69120/251450]\tLoss: 0.0213 (0.0175) \tAcc: 96.88% (97.00%) \tEmb_Norm: 24.44 (25.57)\n",
      "Train Epoch: 6 [69760/251450]\tLoss: 0.0260 (0.0175) \tAcc: 93.75% (96.99%) \tEmb_Norm: 24.42 (25.56)\n",
      "Train Epoch: 6 [70400/251450]\tLoss: 0.0199 (0.0175) \tAcc: 96.88% (96.99%) \tEmb_Norm: 24.40 (25.55)\n",
      "Train Epoch: 6 [71040/251450]\tLoss: 0.0112 (0.0175) \tAcc: 96.88% (96.99%) \tEmb_Norm: 24.38 (25.54)\n",
      "Train Epoch: 6 [71680/251450]\tLoss: 0.0337 (0.0175) \tAcc: 93.75% (97.00%) \tEmb_Norm: 24.36 (25.53)\n",
      "Train Epoch: 6 [72320/251450]\tLoss: 0.0039 (0.0175) \tAcc: 100.00% (96.99%) \tEmb_Norm: 24.33 (25.51)\n",
      "Train Epoch: 6 [72960/251450]\tLoss: 0.0318 (0.0174) \tAcc: 96.88% (97.00%) \tEmb_Norm: 24.31 (25.50)\n",
      "Train Epoch: 6 [73600/251450]\tLoss: 0.0000 (0.0174) \tAcc: 100.00% (97.00%) \tEmb_Norm: 24.29 (25.49)\n",
      "Train Epoch: 6 [74240/251450]\tLoss: 0.0064 (0.0174) \tAcc: 96.88% (97.01%) \tEmb_Norm: 24.27 (25.48)\n",
      "Train Epoch: 6 [74880/251450]\tLoss: 0.0348 (0.0173) \tAcc: 93.75% (97.01%) \tEmb_Norm: 24.25 (25.47)\n",
      "Train Epoch: 6 [75520/251450]\tLoss: 0.0104 (0.0173) \tAcc: 96.88% (97.02%) \tEmb_Norm: 24.23 (25.46)\n",
      "Train Epoch: 6 [76160/251450]\tLoss: 0.0000 (0.0172) \tAcc: 100.00% (97.03%) \tEmb_Norm: 24.21 (25.45)\n",
      "Train Epoch: 6 [76800/251450]\tLoss: 0.0008 (0.0172) \tAcc: 100.00% (97.03%) \tEmb_Norm: 24.19 (25.44)\n",
      "Train Epoch: 6 [77440/251450]\tLoss: 0.0000 (0.0172) \tAcc: 100.00% (97.03%) \tEmb_Norm: 24.17 (25.43)\n",
      "Train Epoch: 6 [78080/251450]\tLoss: 0.0290 (0.0172) \tAcc: 93.75% (97.03%) \tEmb_Norm: 24.15 (25.42)\n",
      "Train Epoch: 6 [78720/251450]\tLoss: 0.0009 (0.0172) \tAcc: 100.00% (97.03%) \tEmb_Norm: 24.13 (25.41)\n",
      "Train Epoch: 6 [79360/251450]\tLoss: 0.0411 (0.0172) \tAcc: 96.88% (97.03%) \tEmb_Norm: 24.11 (25.40)\n",
      "Train Epoch: 6 [80000/251450]\tLoss: 0.0479 (0.0172) \tAcc: 90.62% (97.03%) \tEmb_Norm: 24.09 (25.39)\n",
      "Train Epoch: 6 [80640/251450]\tLoss: 0.0000 (0.0172) \tAcc: 100.00% (97.04%) \tEmb_Norm: 24.07 (25.38)\n",
      "Train Epoch: 6 [81280/251450]\tLoss: 0.0006 (0.0171) \tAcc: 100.00% (97.05%) \tEmb_Norm: 24.05 (25.37)\n",
      "Train Epoch: 6 [81920/251450]\tLoss: 0.0036 (0.0171) \tAcc: 100.00% (97.05%) \tEmb_Norm: 24.03 (25.36)\n",
      "Train Epoch: 6 [82560/251450]\tLoss: 0.0420 (0.0171) \tAcc: 96.88% (97.05%) \tEmb_Norm: 24.01 (25.35)\n",
      "Train Epoch: 6 [83200/251450]\tLoss: 0.0560 (0.0170) \tAcc: 90.62% (97.06%) \tEmb_Norm: 23.98 (25.34)\n",
      "Train Epoch: 6 [83840/251450]\tLoss: 0.0160 (0.0170) \tAcc: 96.88% (97.07%) \tEmb_Norm: 23.96 (25.33)\n",
      "Train Epoch: 6 [84480/251450]\tLoss: 0.0246 (0.0170) \tAcc: 93.75% (97.07%) \tEmb_Norm: 23.94 (25.32)\n",
      "Train Epoch: 6 [85120/251450]\tLoss: 0.0000 (0.0170) \tAcc: 100.00% (97.07%) \tEmb_Norm: 23.92 (25.31)\n",
      "Train Epoch: 6 [85760/251450]\tLoss: 0.0202 (0.0170) \tAcc: 96.88% (97.06%) \tEmb_Norm: 23.90 (25.30)\n",
      "Train Epoch: 6 [86400/251450]\tLoss: 0.0000 (0.0169) \tAcc: 100.00% (97.07%) \tEmb_Norm: 23.88 (25.29)\n",
      "Train Epoch: 6 [87040/251450]\tLoss: 0.0267 (0.0169) \tAcc: 93.75% (97.08%) \tEmb_Norm: 23.86 (25.28)\n",
      "Train Epoch: 6 [87680/251450]\tLoss: 0.0240 (0.0169) \tAcc: 96.88% (97.08%) \tEmb_Norm: 23.84 (25.26)\n",
      "Train Epoch: 6 [88320/251450]\tLoss: 0.0007 (0.0169) \tAcc: 100.00% (97.08%) \tEmb_Norm: 23.82 (25.25)\n",
      "Train Epoch: 6 [88960/251450]\tLoss: 0.0013 (0.0169) \tAcc: 100.00% (97.08%) \tEmb_Norm: 23.80 (25.24)\n",
      "Train Epoch: 6 [89600/251450]\tLoss: 0.0033 (0.0168) \tAcc: 100.00% (97.09%) \tEmb_Norm: 23.78 (25.23)\n",
      "Train Epoch: 6 [90240/251450]\tLoss: 0.0003 (0.0168) \tAcc: 100.00% (97.09%) \tEmb_Norm: 23.76 (25.22)\n",
      "Train Epoch: 6 [90880/251450]\tLoss: 0.0942 (0.0168) \tAcc: 87.50% (97.09%) \tEmb_Norm: 23.74 (25.21)\n",
      "Train Epoch: 6 [91520/251450]\tLoss: 0.0044 (0.0168) \tAcc: 100.00% (97.09%) \tEmb_Norm: 23.72 (25.20)\n",
      "Train Epoch: 6 [92160/251450]\tLoss: 0.0424 (0.0168) \tAcc: 90.62% (97.09%) \tEmb_Norm: 23.70 (25.19)\n",
      "Train Epoch: 6 [92800/251450]\tLoss: 0.0293 (0.0167) \tAcc: 96.88% (97.10%) \tEmb_Norm: 23.68 (25.18)\n",
      "Train Epoch: 6 [93440/251450]\tLoss: 0.0000 (0.0167) \tAcc: 100.00% (97.10%) \tEmb_Norm: 23.66 (25.17)\n",
      "Train Epoch: 6 [94080/251450]\tLoss: 0.0265 (0.0168) \tAcc: 96.88% (97.09%) \tEmb_Norm: 23.64 (25.16)\n",
      "Train Epoch: 6 [94720/251450]\tLoss: 0.0000 (0.0167) \tAcc: 100.00% (97.10%) \tEmb_Norm: 23.61 (25.15)\n",
      "Train Epoch: 6 [95360/251450]\tLoss: 0.0804 (0.0167) \tAcc: 87.50% (97.10%) \tEmb_Norm: 23.59 (25.14)\n",
      "Train Epoch: 6 [96000/251450]\tLoss: 0.0396 (0.0167) \tAcc: 93.75% (97.10%) \tEmb_Norm: 23.57 (25.13)\n",
      "Train Epoch: 6 [96640/251450]\tLoss: 0.0000 (0.0167) \tAcc: 100.00% (97.10%) \tEmb_Norm: 23.55 (25.12)\n",
      "Train Epoch: 6 [97280/251450]\tLoss: 0.0000 (0.0167) \tAcc: 100.00% (97.10%) \tEmb_Norm: 23.53 (25.11)\n",
      "Train Epoch: 6 [97920/251450]\tLoss: 0.0000 (0.0167) \tAcc: 100.00% (97.10%) \tEmb_Norm: 23.51 (25.10)\n",
      "Train Epoch: 6 [98560/251450]\tLoss: 0.0000 (0.0166) \tAcc: 100.00% (97.11%) \tEmb_Norm: 23.49 (25.09)\n",
      "Train Epoch: 6 [99200/251450]\tLoss: 0.0052 (0.0166) \tAcc: 100.00% (97.11%) \tEmb_Norm: 23.47 (25.08)\n",
      "Train Epoch: 6 [99840/251450]\tLoss: 0.0024 (0.0166) \tAcc: 100.00% (97.11%) \tEmb_Norm: 23.45 (25.07)\n",
      "Train Epoch: 6 [100480/251450]\tLoss: 0.0161 (0.0166) \tAcc: 96.88% (97.11%) \tEmb_Norm: 23.43 (25.06)\n",
      "Train Epoch: 6 [101120/251450]\tLoss: 0.0107 (0.0166) \tAcc: 96.88% (97.11%) \tEmb_Norm: 23.41 (25.05)\n",
      "Train Epoch: 6 [101760/251450]\tLoss: 0.0678 (0.0166) \tAcc: 93.75% (97.11%) \tEmb_Norm: 23.39 (25.04)\n",
      "Train Epoch: 6 [102400/251450]\tLoss: 0.0000 (0.0166) \tAcc: 100.00% (97.11%) \tEmb_Norm: 23.37 (25.03)\n",
      "Train Epoch: 6 [103040/251450]\tLoss: 0.0124 (0.0166) \tAcc: 96.88% (97.11%) \tEmb_Norm: 23.35 (25.02)\n",
      "Train Epoch: 6 [103680/251450]\tLoss: 0.0385 (0.0166) \tAcc: 90.62% (97.12%) \tEmb_Norm: 23.33 (25.01)\n",
      "Train Epoch: 6 [104320/251450]\tLoss: 0.0011 (0.0166) \tAcc: 100.00% (97.11%) \tEmb_Norm: 23.30 (25.00)\n",
      "Train Epoch: 6 [104960/251450]\tLoss: 0.0215 (0.0166) \tAcc: 93.75% (97.11%) \tEmb_Norm: 23.28 (24.98)\n",
      "Train Epoch: 6 [105600/251450]\tLoss: 0.0131 (0.0166) \tAcc: 96.88% (97.11%) \tEmb_Norm: 23.26 (24.97)\n",
      "Train Epoch: 6 [106240/251450]\tLoss: 0.0000 (0.0166) \tAcc: 100.00% (97.11%) \tEmb_Norm: 23.24 (24.96)\n",
      "Train Epoch: 6 [106880/251450]\tLoss: 0.0357 (0.0165) \tAcc: 93.75% (97.12%) \tEmb_Norm: 23.22 (24.95)\n",
      "Train Epoch: 6 [107520/251450]\tLoss: 0.0000 (0.0165) \tAcc: 100.00% (97.12%) \tEmb_Norm: 23.20 (24.94)\n",
      "Train Epoch: 6 [108160/251450]\tLoss: 0.0546 (0.0165) \tAcc: 93.75% (97.12%) \tEmb_Norm: 23.18 (24.93)\n",
      "Train Epoch: 6 [108800/251450]\tLoss: 0.0000 (0.0165) \tAcc: 100.00% (97.12%) \tEmb_Norm: 23.16 (24.92)\n",
      "Train Epoch: 6 [109440/251450]\tLoss: 0.0597 (0.0165) \tAcc: 90.62% (97.12%) \tEmb_Norm: 23.14 (24.91)\n",
      "Train Epoch: 6 [110080/251450]\tLoss: 0.0000 (0.0165) \tAcc: 100.00% (97.11%) \tEmb_Norm: 23.12 (24.90)\n",
      "Train Epoch: 6 [110720/251450]\tLoss: 0.0259 (0.0165) \tAcc: 93.75% (97.11%) \tEmb_Norm: 23.10 (24.89)\n",
      "Train Epoch: 6 [111360/251450]\tLoss: 0.0011 (0.0166) \tAcc: 100.00% (97.11%) \tEmb_Norm: 23.08 (24.88)\n",
      "Train Epoch: 6 [112000/251450]\tLoss: 0.0000 (0.0165) \tAcc: 100.00% (97.12%) \tEmb_Norm: 23.06 (24.87)\n",
      "Train Epoch: 6 [112640/251450]\tLoss: 0.0000 (0.0165) \tAcc: 100.00% (97.11%) \tEmb_Norm: 23.04 (24.86)\n",
      "Train Epoch: 6 [113280/251450]\tLoss: 0.0000 (0.0165) \tAcc: 100.00% (97.12%) \tEmb_Norm: 23.02 (24.85)\n",
      "Train Epoch: 6 [113920/251450]\tLoss: 0.0000 (0.0165) \tAcc: 100.00% (97.11%) \tEmb_Norm: 23.00 (24.84)\n",
      "Train Epoch: 6 [114560/251450]\tLoss: 0.0000 (0.0165) \tAcc: 100.00% (97.12%) \tEmb_Norm: 22.98 (24.83)\n",
      "Train Epoch: 6 [115200/251450]\tLoss: 0.0095 (0.0165) \tAcc: 96.88% (97.12%) \tEmb_Norm: 22.96 (24.82)\n",
      "Train Epoch: 6 [115840/251450]\tLoss: 0.0149 (0.0165) \tAcc: 96.88% (97.12%) \tEmb_Norm: 22.94 (24.81)\n",
      "Train Epoch: 6 [116480/251450]\tLoss: 0.0298 (0.0165) \tAcc: 96.88% (97.13%) \tEmb_Norm: 22.92 (24.80)\n",
      "Train Epoch: 6 [117120/251450]\tLoss: 0.0000 (0.0165) \tAcc: 100.00% (97.13%) \tEmb_Norm: 22.90 (24.79)\n",
      "Train Epoch: 6 [117760/251450]\tLoss: 0.0000 (0.0165) \tAcc: 100.00% (97.12%) \tEmb_Norm: 22.87 (24.78)\n",
      "Train Epoch: 6 [118400/251450]\tLoss: 0.0216 (0.0165) \tAcc: 96.88% (97.12%) \tEmb_Norm: 22.85 (24.77)\n",
      "Train Epoch: 6 [119040/251450]\tLoss: 0.0023 (0.0165) \tAcc: 100.00% (97.12%) \tEmb_Norm: 22.83 (24.76)\n",
      "Train Epoch: 6 [119680/251450]\tLoss: 0.0183 (0.0165) \tAcc: 93.75% (97.12%) \tEmb_Norm: 22.81 (24.75)\n",
      "Train Epoch: 6 [120320/251450]\tLoss: 0.0000 (0.0165) \tAcc: 100.00% (97.12%) \tEmb_Norm: 22.79 (24.74)\n",
      "Train Epoch: 6 [120960/251450]\tLoss: 0.0104 (0.0165) \tAcc: 96.88% (97.13%) \tEmb_Norm: 22.77 (24.73)\n",
      "Train Epoch: 6 [121600/251450]\tLoss: 0.0248 (0.0165) \tAcc: 93.75% (97.12%) \tEmb_Norm: 22.75 (24.72)\n",
      "Train Epoch: 6 [122240/251450]\tLoss: 0.0086 (0.0165) \tAcc: 96.88% (97.12%) \tEmb_Norm: 22.73 (24.71)\n",
      "Train Epoch: 6 [122880/251450]\tLoss: 0.0089 (0.0164) \tAcc: 100.00% (97.12%) \tEmb_Norm: 22.71 (24.69)\n",
      "Train Epoch: 6 [123520/251450]\tLoss: 0.0157 (0.0164) \tAcc: 93.75% (97.13%) \tEmb_Norm: 22.69 (24.68)\n",
      "Train Epoch: 6 [124160/251450]\tLoss: 0.0091 (0.0164) \tAcc: 96.88% (97.13%) \tEmb_Norm: 22.67 (24.67)\n",
      "Train Epoch: 6 [124800/251450]\tLoss: 0.0135 (0.0164) \tAcc: 96.88% (97.13%) \tEmb_Norm: 22.65 (24.66)\n",
      "Train Epoch: 6 [125440/251450]\tLoss: 0.0263 (0.0164) \tAcc: 96.88% (97.13%) \tEmb_Norm: 22.62 (24.65)\n",
      "Train Epoch: 6 [126080/251450]\tLoss: 0.0972 (0.0165) \tAcc: 84.38% (97.12%) \tEmb_Norm: 22.60 (24.64)\n",
      "Train Epoch: 6 [126720/251450]\tLoss: 0.0000 (0.0165) \tAcc: 100.00% (97.12%) \tEmb_Norm: 22.58 (24.63)\n",
      "Train Epoch: 6 [127360/251450]\tLoss: 0.0112 (0.0165) \tAcc: 100.00% (97.12%) \tEmb_Norm: 22.56 (24.62)\n",
      "Train Epoch: 6 [128000/251450]\tLoss: 0.0000 (0.0165) \tAcc: 100.00% (97.11%) \tEmb_Norm: 22.54 (24.61)\n",
      "Train Epoch: 6 [128640/251450]\tLoss: 0.0000 (0.0165) \tAcc: 100.00% (97.11%) \tEmb_Norm: 22.52 (24.60)\n",
      "Train Epoch: 6 [129280/251450]\tLoss: 0.0026 (0.0165) \tAcc: 100.00% (97.11%) \tEmb_Norm: 22.50 (24.59)\n",
      "Train Epoch: 6 [129920/251450]\tLoss: 0.0490 (0.0165) \tAcc: 90.62% (97.10%) \tEmb_Norm: 22.47 (24.58)\n",
      "Train Epoch: 6 [130560/251450]\tLoss: 0.0861 (0.0165) \tAcc: 93.75% (97.10%) \tEmb_Norm: 22.45 (24.57)\n",
      "Train Epoch: 6 [131200/251450]\tLoss: 0.0349 (0.0165) \tAcc: 93.75% (97.11%) \tEmb_Norm: 22.43 (24.56)\n",
      "Train Epoch: 6 [131840/251450]\tLoss: 0.0000 (0.0165) \tAcc: 100.00% (97.11%) \tEmb_Norm: 22.41 (24.55)\n",
      "Train Epoch: 6 [132480/251450]\tLoss: 0.0000 (0.0165) \tAcc: 100.00% (97.12%) \tEmb_Norm: 22.39 (24.54)\n",
      "Train Epoch: 6 [133120/251450]\tLoss: 0.0125 (0.0165) \tAcc: 96.88% (97.11%) \tEmb_Norm: 22.37 (24.53)\n",
      "Train Epoch: 6 [133760/251450]\tLoss: 0.0145 (0.0165) \tAcc: 96.88% (97.11%) \tEmb_Norm: 22.35 (24.52)\n",
      "Train Epoch: 6 [134400/251450]\tLoss: 0.0132 (0.0164) \tAcc: 96.88% (97.12%) \tEmb_Norm: 22.33 (24.51)\n",
      "Train Epoch: 6 [135040/251450]\tLoss: 0.0034 (0.0164) \tAcc: 100.00% (97.12%) \tEmb_Norm: 22.31 (24.50)\n",
      "Train Epoch: 6 [135680/251450]\tLoss: 0.0000 (0.0164) \tAcc: 100.00% (97.12%) \tEmb_Norm: 22.29 (24.49)\n",
      "Train Epoch: 6 [136320/251450]\tLoss: 0.0141 (0.0164) \tAcc: 96.88% (97.13%) \tEmb_Norm: 22.27 (24.48)\n",
      "Train Epoch: 6 [136960/251450]\tLoss: 0.0000 (0.0164) \tAcc: 100.00% (97.13%) \tEmb_Norm: 22.25 (24.47)\n",
      "Train Epoch: 6 [137600/251450]\tLoss: 0.0333 (0.0164) \tAcc: 93.75% (97.13%) \tEmb_Norm: 22.23 (24.46)\n",
      "Train Epoch: 6 [138240/251450]\tLoss: 0.0110 (0.0163) \tAcc: 96.88% (97.13%) \tEmb_Norm: 22.21 (24.45)\n",
      "Train Epoch: 6 [138880/251450]\tLoss: 0.0071 (0.0163) \tAcc: 100.00% (97.13%) \tEmb_Norm: 22.19 (24.44)\n",
      "Train Epoch: 6 [139520/251450]\tLoss: 0.0066 (0.0163) \tAcc: 96.88% (97.13%) \tEmb_Norm: 22.17 (24.43)\n",
      "Train Epoch: 6 [140160/251450]\tLoss: 0.0004 (0.0163) \tAcc: 100.00% (97.13%) \tEmb_Norm: 22.15 (24.42)\n",
      "Train Epoch: 6 [140800/251450]\tLoss: 0.0093 (0.0163) \tAcc: 100.00% (97.13%) \tEmb_Norm: 22.13 (24.40)\n",
      "Train Epoch: 6 [141440/251450]\tLoss: 0.0337 (0.0163) \tAcc: 93.75% (97.13%) \tEmb_Norm: 22.11 (24.39)\n",
      "Train Epoch: 6 [142080/251450]\tLoss: 0.0017 (0.0163) \tAcc: 100.00% (97.13%) \tEmb_Norm: 22.09 (24.38)\n",
      "Train Epoch: 6 [142720/251450]\tLoss: 0.0077 (0.0163) \tAcc: 96.88% (97.14%) \tEmb_Norm: 22.07 (24.37)\n",
      "Train Epoch: 6 [143360/251450]\tLoss: 0.0071 (0.0163) \tAcc: 96.88% (97.14%) \tEmb_Norm: 22.05 (24.36)\n",
      "Train Epoch: 6 [144000/251450]\tLoss: 0.0180 (0.0163) \tAcc: 96.88% (97.14%) \tEmb_Norm: 22.03 (24.35)\n",
      "Train Epoch: 6 [144640/251450]\tLoss: 0.0242 (0.0163) \tAcc: 96.88% (97.14%) \tEmb_Norm: 22.01 (24.34)\n",
      "Train Epoch: 6 [145280/251450]\tLoss: 0.0105 (0.0162) \tAcc: 96.88% (97.14%) \tEmb_Norm: 21.99 (24.33)\n",
      "Train Epoch: 6 [145920/251450]\tLoss: 0.0000 (0.0162) \tAcc: 100.00% (97.14%) \tEmb_Norm: 21.96 (24.32)\n",
      "Train Epoch: 6 [146560/251450]\tLoss: 0.0092 (0.0162) \tAcc: 96.88% (97.14%) \tEmb_Norm: 21.94 (24.31)\n",
      "Train Epoch: 6 [147200/251450]\tLoss: 0.0022 (0.0163) \tAcc: 100.00% (97.14%) \tEmb_Norm: 21.92 (24.30)\n",
      "Train Epoch: 6 [147840/251450]\tLoss: 0.0000 (0.0162) \tAcc: 100.00% (97.14%) \tEmb_Norm: 21.90 (24.29)\n",
      "Train Epoch: 6 [148480/251450]\tLoss: 0.0409 (0.0162) \tAcc: 93.75% (97.14%) \tEmb_Norm: 21.88 (24.28)\n",
      "Train Epoch: 6 [149120/251450]\tLoss: 0.0000 (0.0162) \tAcc: 100.00% (97.14%) \tEmb_Norm: 21.86 (24.27)\n",
      "Train Epoch: 6 [149760/251450]\tLoss: 0.0068 (0.0162) \tAcc: 96.88% (97.14%) \tEmb_Norm: 21.84 (24.26)\n",
      "Train Epoch: 6 [150400/251450]\tLoss: 0.0028 (0.0162) \tAcc: 100.00% (97.14%) \tEmb_Norm: 21.82 (24.25)\n",
      "Train Epoch: 6 [151040/251450]\tLoss: 0.0108 (0.0162) \tAcc: 96.88% (97.15%) \tEmb_Norm: 21.80 (24.24)\n",
      "Train Epoch: 6 [151680/251450]\tLoss: 0.0039 (0.0162) \tAcc: 100.00% (97.15%) \tEmb_Norm: 21.78 (24.23)\n",
      "Train Epoch: 6 [152320/251450]\tLoss: 0.0100 (0.0162) \tAcc: 96.88% (97.14%) \tEmb_Norm: 21.76 (24.22)\n",
      "Train Epoch: 6 [152960/251450]\tLoss: 0.0139 (0.0162) \tAcc: 93.75% (97.14%) \tEmb_Norm: 21.74 (24.21)\n",
      "Train Epoch: 6 [153600/251450]\tLoss: 0.0000 (0.0162) \tAcc: 100.00% (97.14%) \tEmb_Norm: 21.72 (24.20)\n",
      "Train Epoch: 6 [154240/251450]\tLoss: 0.0692 (0.0162) \tAcc: 87.50% (97.14%) \tEmb_Norm: 21.70 (24.19)\n",
      "Train Epoch: 6 [154880/251450]\tLoss: 0.0691 (0.0162) \tAcc: 93.75% (97.14%) \tEmb_Norm: 21.68 (24.18)\n",
      "Train Epoch: 6 [155520/251450]\tLoss: 0.0043 (0.0162) \tAcc: 100.00% (97.14%) \tEmb_Norm: 21.66 (24.17)\n",
      "Train Epoch: 6 [156160/251450]\tLoss: 0.0000 (0.0162) \tAcc: 100.00% (97.14%) \tEmb_Norm: 21.64 (24.16)\n",
      "Train Epoch: 6 [156800/251450]\tLoss: 0.0280 (0.0162) \tAcc: 93.75% (97.14%) \tEmb_Norm: 21.61 (24.15)\n",
      "Train Epoch: 6 [157440/251450]\tLoss: 0.0000 (0.0162) \tAcc: 100.00% (97.14%) \tEmb_Norm: 21.59 (24.14)\n",
      "Train Epoch: 6 [158080/251450]\tLoss: 0.0018 (0.0162) \tAcc: 100.00% (97.15%) \tEmb_Norm: 21.57 (24.13)\n",
      "Train Epoch: 6 [158720/251450]\tLoss: 0.0249 (0.0162) \tAcc: 96.88% (97.14%) \tEmb_Norm: 21.55 (24.12)\n",
      "Train Epoch: 6 [159360/251450]\tLoss: 0.0016 (0.0162) \tAcc: 100.00% (97.14%) \tEmb_Norm: 21.53 (24.10)\n",
      "Train Epoch: 6 [160000/251450]\tLoss: 0.0000 (0.0162) \tAcc: 100.00% (97.14%) \tEmb_Norm: 21.51 (24.09)\n",
      "Train Epoch: 6 [160640/251450]\tLoss: 0.0000 (0.0162) \tAcc: 100.00% (97.14%) \tEmb_Norm: 21.49 (24.08)\n",
      "Train Epoch: 6 [161280/251450]\tLoss: 0.0098 (0.0162) \tAcc: 96.88% (97.14%) \tEmb_Norm: 21.47 (24.07)\n",
      "Train Epoch: 6 [161920/251450]\tLoss: 0.0189 (0.0162) \tAcc: 96.88% (97.14%) \tEmb_Norm: 21.45 (24.06)\n",
      "Train Epoch: 6 [162560/251450]\tLoss: 0.0197 (0.0161) \tAcc: 96.88% (97.14%) \tEmb_Norm: 21.43 (24.05)\n",
      "Train Epoch: 6 [163200/251450]\tLoss: 0.0240 (0.0161) \tAcc: 96.88% (97.14%) \tEmb_Norm: 21.41 (24.04)\n",
      "Train Epoch: 6 [163840/251450]\tLoss: 0.0000 (0.0161) \tAcc: 100.00% (97.14%) \tEmb_Norm: 21.39 (24.03)\n",
      "Train Epoch: 6 [164480/251450]\tLoss: 0.0062 (0.0161) \tAcc: 100.00% (97.15%) \tEmb_Norm: 21.37 (24.02)\n",
      "Train Epoch: 6 [165120/251450]\tLoss: 0.0070 (0.0161) \tAcc: 100.00% (97.14%) \tEmb_Norm: 21.35 (24.01)\n",
      "Train Epoch: 6 [165760/251450]\tLoss: 0.0210 (0.0161) \tAcc: 96.88% (97.15%) \tEmb_Norm: 21.33 (24.00)\n",
      "Train Epoch: 6 [166400/251450]\tLoss: 0.0000 (0.0161) \tAcc: 100.00% (97.14%) \tEmb_Norm: 21.31 (23.99)\n",
      "Train Epoch: 6 [167040/251450]\tLoss: 0.0114 (0.0161) \tAcc: 96.88% (97.14%) \tEmb_Norm: 21.29 (23.98)\n",
      "Train Epoch: 6 [167680/251450]\tLoss: 0.0146 (0.0161) \tAcc: 96.88% (97.14%) \tEmb_Norm: 21.27 (23.97)\n",
      "Train Epoch: 6 [168320/251450]\tLoss: 0.0152 (0.0161) \tAcc: 96.88% (97.14%) \tEmb_Norm: 21.25 (23.96)\n",
      "Train Epoch: 6 [168960/251450]\tLoss: 0.0202 (0.0161) \tAcc: 93.75% (97.14%) \tEmb_Norm: 21.23 (23.95)\n",
      "Train Epoch: 6 [169600/251450]\tLoss: 0.0276 (0.0161) \tAcc: 93.75% (97.14%) \tEmb_Norm: 21.20 (23.94)\n",
      "Train Epoch: 6 [170240/251450]\tLoss: 0.0074 (0.0161) \tAcc: 96.88% (97.14%) \tEmb_Norm: 21.18 (23.93)\n",
      "Train Epoch: 6 [170880/251450]\tLoss: 0.0067 (0.0161) \tAcc: 100.00% (97.14%) \tEmb_Norm: 21.16 (23.92)\n",
      "Train Epoch: 6 [171520/251450]\tLoss: 0.0022 (0.0161) \tAcc: 100.00% (97.14%) \tEmb_Norm: 21.14 (23.91)\n",
      "Train Epoch: 6 [172160/251450]\tLoss: 0.0033 (0.0161) \tAcc: 100.00% (97.14%) \tEmb_Norm: 21.12 (23.90)\n",
      "Train Epoch: 6 [172800/251450]\tLoss: 0.0000 (0.0161) \tAcc: 100.00% (97.14%) \tEmb_Norm: 21.10 (23.89)\n",
      "Train Epoch: 6 [173440/251450]\tLoss: 0.0011 (0.0161) \tAcc: 100.00% (97.14%) \tEmb_Norm: 21.08 (23.88)\n",
      "Train Epoch: 6 [174080/251450]\tLoss: 0.0000 (0.0161) \tAcc: 100.00% (97.14%) \tEmb_Norm: 21.06 (23.87)\n",
      "Train Epoch: 6 [174720/251450]\tLoss: 0.0000 (0.0161) \tAcc: 100.00% (97.15%) \tEmb_Norm: 21.04 (23.86)\n",
      "Train Epoch: 6 [175360/251450]\tLoss: 0.0297 (0.0160) \tAcc: 93.75% (97.15%) \tEmb_Norm: 21.02 (23.85)\n",
      "Train Epoch: 6 [176000/251450]\tLoss: 0.0004 (0.0161) \tAcc: 100.00% (97.15%) \tEmb_Norm: 21.00 (23.84)\n",
      "Train Epoch: 6 [176640/251450]\tLoss: 0.0000 (0.0161) \tAcc: 100.00% (97.15%) \tEmb_Norm: 20.98 (23.83)\n",
      "Train Epoch: 6 [177280/251450]\tLoss: 0.0064 (0.0161) \tAcc: 100.00% (97.15%) \tEmb_Norm: 20.96 (23.82)\n",
      "Train Epoch: 6 [177920/251450]\tLoss: 0.0083 (0.0161) \tAcc: 100.00% (97.15%) \tEmb_Norm: 20.94 (23.81)\n",
      "Train Epoch: 6 [178560/251450]\tLoss: 0.0129 (0.0161) \tAcc: 96.88% (97.15%) \tEmb_Norm: 20.92 (23.80)\n",
      "Train Epoch: 6 [179200/251450]\tLoss: 0.0003 (0.0160) \tAcc: 100.00% (97.15%) \tEmb_Norm: 20.90 (23.78)\n",
      "Train Epoch: 6 [179840/251450]\tLoss: 0.0000 (0.0160) \tAcc: 100.00% (97.16%) \tEmb_Norm: 20.88 (23.77)\n",
      "Train Epoch: 6 [180480/251450]\tLoss: 0.0496 (0.0160) \tAcc: 93.75% (97.15%) \tEmb_Norm: 20.86 (23.76)\n",
      "Train Epoch: 6 [181120/251450]\tLoss: 0.0144 (0.0160) \tAcc: 100.00% (97.15%) \tEmb_Norm: 20.84 (23.75)\n",
      "Train Epoch: 6 [181760/251450]\tLoss: 0.0008 (0.0160) \tAcc: 100.00% (97.15%) \tEmb_Norm: 20.82 (23.74)\n",
      "Train Epoch: 6 [182400/251450]\tLoss: 0.0014 (0.0160) \tAcc: 100.00% (97.15%) \tEmb_Norm: 20.80 (23.73)\n",
      "Train Epoch: 6 [183040/251450]\tLoss: 0.0000 (0.0160) \tAcc: 100.00% (97.15%) \tEmb_Norm: 20.78 (23.72)\n",
      "Train Epoch: 6 [183680/251450]\tLoss: 0.0135 (0.0160) \tAcc: 96.88% (97.15%) \tEmb_Norm: 20.76 (23.71)\n",
      "Train Epoch: 6 [184320/251450]\tLoss: 0.0036 (0.0160) \tAcc: 100.00% (97.16%) \tEmb_Norm: 20.74 (23.70)\n",
      "Train Epoch: 6 [184960/251450]\tLoss: 0.0217 (0.0160) \tAcc: 96.88% (97.15%) \tEmb_Norm: 20.71 (23.69)\n",
      "Train Epoch: 6 [185600/251450]\tLoss: 0.0446 (0.0160) \tAcc: 90.62% (97.15%) \tEmb_Norm: 20.69 (23.68)\n",
      "Train Epoch: 6 [186240/251450]\tLoss: 0.0434 (0.0160) \tAcc: 90.62% (97.15%) \tEmb_Norm: 20.67 (23.67)\n",
      "Train Epoch: 6 [186880/251450]\tLoss: 0.0000 (0.0160) \tAcc: 100.00% (97.15%) \tEmb_Norm: 20.65 (23.66)\n",
      "Train Epoch: 6 [187520/251450]\tLoss: 0.0170 (0.0160) \tAcc: 96.88% (97.14%) \tEmb_Norm: 20.63 (23.65)\n",
      "Train Epoch: 6 [188160/251450]\tLoss: 0.0153 (0.0160) \tAcc: 96.88% (97.15%) \tEmb_Norm: 20.61 (23.64)\n",
      "Train Epoch: 6 [188800/251450]\tLoss: 0.0168 (0.0160) \tAcc: 96.88% (97.15%) \tEmb_Norm: 20.59 (23.63)\n",
      "Train Epoch: 6 [189440/251450]\tLoss: 0.0186 (0.0160) \tAcc: 96.88% (97.15%) \tEmb_Norm: 20.57 (23.62)\n",
      "Train Epoch: 6 [190080/251450]\tLoss: 0.0000 (0.0160) \tAcc: 100.00% (97.15%) \tEmb_Norm: 20.55 (23.61)\n",
      "Train Epoch: 6 [190720/251450]\tLoss: 0.0175 (0.0160) \tAcc: 96.88% (97.15%) \tEmb_Norm: 20.53 (23.60)\n",
      "Train Epoch: 6 [191360/251450]\tLoss: 0.0052 (0.0160) \tAcc: 100.00% (97.15%) \tEmb_Norm: 20.51 (23.59)\n",
      "Train Epoch: 6 [192000/251450]\tLoss: 0.0255 (0.0159) \tAcc: 96.88% (97.16%) \tEmb_Norm: 20.49 (23.58)\n",
      "Train Epoch: 6 [192640/251450]\tLoss: 0.0078 (0.0159) \tAcc: 100.00% (97.15%) \tEmb_Norm: 20.47 (23.57)\n",
      "Train Epoch: 6 [193280/251450]\tLoss: 0.0042 (0.0159) \tAcc: 100.00% (97.15%) \tEmb_Norm: 20.45 (23.56)\n",
      "Train Epoch: 6 [193920/251450]\tLoss: 0.0215 (0.0159) \tAcc: 93.75% (97.16%) \tEmb_Norm: 20.43 (23.55)\n",
      "Train Epoch: 6 [194560/251450]\tLoss: 0.0331 (0.0159) \tAcc: 96.88% (97.16%) \tEmb_Norm: 20.41 (23.54)\n",
      "Train Epoch: 6 [195200/251450]\tLoss: 0.0113 (0.0159) \tAcc: 96.88% (97.16%) \tEmb_Norm: 20.39 (23.53)\n",
      "Train Epoch: 6 [195840/251450]\tLoss: 0.0262 (0.0159) \tAcc: 96.88% (97.16%) \tEmb_Norm: 20.37 (23.52)\n",
      "Train Epoch: 6 [196480/251450]\tLoss: 0.0000 (0.0159) \tAcc: 100.00% (97.16%) \tEmb_Norm: 20.35 (23.51)\n",
      "Train Epoch: 6 [197120/251450]\tLoss: 0.0352 (0.0158) \tAcc: 93.75% (97.17%) \tEmb_Norm: 20.33 (23.50)\n",
      "Train Epoch: 6 [197760/251450]\tLoss: 0.0004 (0.0158) \tAcc: 100.00% (97.17%) \tEmb_Norm: 20.31 (23.49)\n",
      "Train Epoch: 6 [198400/251450]\tLoss: 0.0082 (0.0159) \tAcc: 100.00% (97.16%) \tEmb_Norm: 20.29 (23.48)\n",
      "Train Epoch: 6 [199040/251450]\tLoss: 0.0227 (0.0159) \tAcc: 96.88% (97.16%) \tEmb_Norm: 20.27 (23.47)\n",
      "Train Epoch: 6 [199680/251450]\tLoss: 0.0178 (0.0158) \tAcc: 96.88% (97.17%) \tEmb_Norm: 20.25 (23.46)\n",
      "Train Epoch: 6 [200320/251450]\tLoss: 0.0027 (0.0158) \tAcc: 100.00% (97.17%) \tEmb_Norm: 20.23 (23.45)\n",
      "Train Epoch: 6 [200960/251450]\tLoss: 0.0121 (0.0158) \tAcc: 96.88% (97.17%) \tEmb_Norm: 20.21 (23.43)\n",
      "Train Epoch: 6 [201600/251450]\tLoss: 0.0019 (0.0158) \tAcc: 100.00% (97.17%) \tEmb_Norm: 20.19 (23.42)\n",
      "=============== TEST epoch 6 ===============\n",
      "\n",
      "Test set: Average loss: 0.0224, Accuracy: 95.97%\n",
      "\n",
      "=============== checkpoint epoch 6 ===============\n",
      "=============== TRAIN epoch 7 ===============\n",
      "Train Epoch: 7 [0/251450]\tLoss: 0.0378 (0.0378) \tAcc: 90.62% (90.62%) \tEmb_Norm: 20.17 (20.17)\n",
      "Train Epoch: 7 [640/251450]\tLoss: 0.0006 (0.0126) \tAcc: 100.00% (97.62%) \tEmb_Norm: 20.15 (20.16)\n",
      "Train Epoch: 7 [1280/251450]\tLoss: 0.0252 (0.0113) \tAcc: 96.88% (97.79%) \tEmb_Norm: 20.13 (20.15)\n",
      "Train Epoch: 7 [1920/251450]\tLoss: 0.0152 (0.0104) \tAcc: 96.88% (98.05%) \tEmb_Norm: 20.11 (20.14)\n",
      "Train Epoch: 7 [2560/251450]\tLoss: 0.0141 (0.0100) \tAcc: 96.88% (98.07%) \tEmb_Norm: 20.09 (20.13)\n",
      "Train Epoch: 7 [3200/251450]\tLoss: 0.0047 (0.0104) \tAcc: 100.00% (98.02%) \tEmb_Norm: 20.07 (20.12)\n",
      "Train Epoch: 7 [3840/251450]\tLoss: 0.0024 (0.0107) \tAcc: 100.00% (98.01%) \tEmb_Norm: 20.05 (20.11)\n",
      "Train Epoch: 7 [4480/251450]\tLoss: 0.0000 (0.0107) \tAcc: 100.00% (97.92%) \tEmb_Norm: 20.03 (20.10)\n",
      "Train Epoch: 7 [5120/251450]\tLoss: 0.0019 (0.0108) \tAcc: 100.00% (97.86%) \tEmb_Norm: 20.01 (20.09)\n",
      "Train Epoch: 7 [5760/251450]\tLoss: 0.0338 (0.0111) \tAcc: 96.88% (97.86%) \tEmb_Norm: 19.99 (20.08)\n",
      "Train Epoch: 7 [6400/251450]\tLoss: 0.0158 (0.0110) \tAcc: 93.75% (97.84%) \tEmb_Norm: 19.97 (20.07)\n",
      "Train Epoch: 7 [7040/251450]\tLoss: 0.0082 (0.0108) \tAcc: 96.88% (97.86%) \tEmb_Norm: 19.95 (20.06)\n",
      "Train Epoch: 7 [7680/251450]\tLoss: 0.0049 (0.0107) \tAcc: 100.00% (97.87%) \tEmb_Norm: 19.93 (20.05)\n",
      "Train Epoch: 7 [8320/251450]\tLoss: 0.0117 (0.0108) \tAcc: 96.88% (97.88%) \tEmb_Norm: 19.91 (20.04)\n",
      "Train Epoch: 7 [8960/251450]\tLoss: 0.0004 (0.0109) \tAcc: 100.00% (97.82%) \tEmb_Norm: 19.89 (20.03)\n",
      "Train Epoch: 7 [9600/251450]\tLoss: 0.0181 (0.0113) \tAcc: 93.75% (97.74%) \tEmb_Norm: 19.87 (20.02)\n",
      "Train Epoch: 7 [10240/251450]\tLoss: 0.0176 (0.0112) \tAcc: 93.75% (97.74%) \tEmb_Norm: 19.85 (20.01)\n",
      "Train Epoch: 7 [10880/251450]\tLoss: 0.0046 (0.0113) \tAcc: 100.00% (97.75%) \tEmb_Norm: 19.83 (20.00)\n",
      "Train Epoch: 7 [11520/251450]\tLoss: 0.0371 (0.0116) \tAcc: 96.88% (97.67%) \tEmb_Norm: 19.81 (19.99)\n",
      "Train Epoch: 7 [12160/251450]\tLoss: 0.0016 (0.0115) \tAcc: 100.00% (97.70%) \tEmb_Norm: 19.79 (19.98)\n",
      "Train Epoch: 7 [12800/251450]\tLoss: 0.0193 (0.0113) \tAcc: 93.75% (97.73%) \tEmb_Norm: 19.77 (19.97)\n",
      "Train Epoch: 7 [13440/251450]\tLoss: 0.0291 (0.0113) \tAcc: 93.75% (97.72%) \tEmb_Norm: 19.75 (19.96)\n",
      "Train Epoch: 7 [14080/251450]\tLoss: 0.0053 (0.0114) \tAcc: 100.00% (97.72%) \tEmb_Norm: 19.73 (19.95)\n",
      "Train Epoch: 7 [14720/251450]\tLoss: 0.0085 (0.0114) \tAcc: 96.88% (97.73%) \tEmb_Norm: 19.71 (19.94)\n",
      "Train Epoch: 7 [15360/251450]\tLoss: 0.0010 (0.0114) \tAcc: 100.00% (97.72%) \tEmb_Norm: 19.69 (19.93)\n",
      "Train Epoch: 7 [16000/251450]\tLoss: 0.0047 (0.0116) \tAcc: 100.00% (97.70%) \tEmb_Norm: 19.67 (19.92)\n",
      "Train Epoch: 7 [16640/251450]\tLoss: 0.0301 (0.0117) \tAcc: 90.62% (97.68%) \tEmb_Norm: 19.65 (19.91)\n",
      "Train Epoch: 7 [17280/251450]\tLoss: 0.0018 (0.0115) \tAcc: 100.00% (97.71%) \tEmb_Norm: 19.63 (19.90)\n",
      "Train Epoch: 7 [17920/251450]\tLoss: 0.0141 (0.0116) \tAcc: 96.88% (97.69%) \tEmb_Norm: 19.61 (19.89)\n",
      "Train Epoch: 7 [18560/251450]\tLoss: 0.0000 (0.0117) \tAcc: 100.00% (97.66%) \tEmb_Norm: 19.59 (19.88)\n",
      "Train Epoch: 7 [19200/251450]\tLoss: 0.0090 (0.0116) \tAcc: 96.88% (97.68%) \tEmb_Norm: 19.58 (19.87)\n",
      "Train Epoch: 7 [19840/251450]\tLoss: 0.0035 (0.0116) \tAcc: 100.00% (97.69%) \tEmb_Norm: 19.56 (19.86)\n",
      "Train Epoch: 7 [20480/251450]\tLoss: 0.0013 (0.0115) \tAcc: 100.00% (97.69%) \tEmb_Norm: 19.54 (19.85)\n",
      "Train Epoch: 7 [21120/251450]\tLoss: 0.0271 (0.0115) \tAcc: 93.75% (97.69%) \tEmb_Norm: 19.52 (19.84)\n",
      "Train Epoch: 7 [21760/251450]\tLoss: 0.0153 (0.0116) \tAcc: 96.88% (97.69%) \tEmb_Norm: 19.50 (19.83)\n",
      "Train Epoch: 7 [22400/251450]\tLoss: 0.0170 (0.0116) \tAcc: 93.75% (97.69%) \tEmb_Norm: 19.48 (19.82)\n",
      "Train Epoch: 7 [23040/251450]\tLoss: 0.0178 (0.0117) \tAcc: 93.75% (97.66%) \tEmb_Norm: 19.46 (19.81)\n",
      "Train Epoch: 7 [23680/251450]\tLoss: 0.0119 (0.0120) \tAcc: 96.88% (97.63%) \tEmb_Norm: 19.43 (19.80)\n",
      "Train Epoch: 7 [24320/251450]\tLoss: 0.0000 (0.0120) \tAcc: 100.00% (97.64%) \tEmb_Norm: 19.41 (19.79)\n",
      "Train Epoch: 7 [24960/251450]\tLoss: 0.0000 (0.0120) \tAcc: 100.00% (97.64%) \tEmb_Norm: 19.39 (19.78)\n",
      "Train Epoch: 7 [25600/251450]\tLoss: 0.0075 (0.0120) \tAcc: 96.88% (97.64%) \tEmb_Norm: 19.37 (19.77)\n",
      "Train Epoch: 7 [26240/251450]\tLoss: 0.0068 (0.0120) \tAcc: 100.00% (97.65%) \tEmb_Norm: 19.35 (19.76)\n",
      "Train Epoch: 7 [26880/251450]\tLoss: 0.0036 (0.0120) \tAcc: 100.00% (97.65%) \tEmb_Norm: 19.33 (19.75)\n",
      "Train Epoch: 7 [27520/251450]\tLoss: 0.0017 (0.0119) \tAcc: 100.00% (97.66%) \tEmb_Norm: 19.31 (19.74)\n",
      "Train Epoch: 7 [28160/251450]\tLoss: 0.0324 (0.0121) \tAcc: 90.62% (97.63%) \tEmb_Norm: 19.29 (19.73)\n",
      "Train Epoch: 7 [28800/251450]\tLoss: 0.0047 (0.0122) \tAcc: 100.00% (97.62%) \tEmb_Norm: 19.27 (19.72)\n",
      "Train Epoch: 7 [29440/251450]\tLoss: 0.0048 (0.0120) \tAcc: 100.00% (97.64%) \tEmb_Norm: 19.25 (19.71)\n",
      "Train Epoch: 7 [30080/251450]\tLoss: 0.0545 (0.0120) \tAcc: 96.88% (97.65%) \tEmb_Norm: 19.23 (19.70)\n",
      "Train Epoch: 7 [30720/251450]\tLoss: 0.0185 (0.0121) \tAcc: 96.88% (97.63%) \tEmb_Norm: 19.21 (19.69)\n",
      "Train Epoch: 7 [31360/251450]\tLoss: 0.0387 (0.0121) \tAcc: 96.88% (97.64%) \tEmb_Norm: 19.19 (19.68)\n",
      "Train Epoch: 7 [32000/251450]\tLoss: 0.0188 (0.0121) \tAcc: 93.75% (97.62%) \tEmb_Norm: 19.17 (19.67)\n",
      "Train Epoch: 7 [32640/251450]\tLoss: 0.0054 (0.0121) \tAcc: 100.00% (97.63%) \tEmb_Norm: 19.15 (19.66)\n",
      "Train Epoch: 7 [33280/251450]\tLoss: 0.0003 (0.0120) \tAcc: 100.00% (97.64%) \tEmb_Norm: 19.13 (19.65)\n",
      "Train Epoch: 7 [33920/251450]\tLoss: 0.0143 (0.0121) \tAcc: 96.88% (97.61%) \tEmb_Norm: 19.11 (19.64)\n",
      "Train Epoch: 7 [34560/251450]\tLoss: 0.0015 (0.0121) \tAcc: 100.00% (97.62%) \tEmb_Norm: 19.09 (19.63)\n",
      "Train Epoch: 7 [35200/251450]\tLoss: 0.0000 (0.0121) \tAcc: 100.00% (97.63%) \tEmb_Norm: 19.07 (19.62)\n",
      "Train Epoch: 7 [35840/251450]\tLoss: 0.0079 (0.0120) \tAcc: 96.88% (97.64%) \tEmb_Norm: 19.05 (19.61)\n",
      "Train Epoch: 7 [36480/251450]\tLoss: 0.0279 (0.0120) \tAcc: 93.75% (97.64%) \tEmb_Norm: 19.03 (19.60)\n",
      "Train Epoch: 7 [37120/251450]\tLoss: 0.0383 (0.0121) \tAcc: 90.62% (97.64%) \tEmb_Norm: 19.01 (19.59)\n",
      "Train Epoch: 7 [37760/251450]\tLoss: 0.0032 (0.0121) \tAcc: 100.00% (97.63%) \tEmb_Norm: 18.99 (19.58)\n",
      "Train Epoch: 7 [38400/251450]\tLoss: 0.0220 (0.0121) \tAcc: 93.75% (97.63%) \tEmb_Norm: 18.97 (19.57)\n",
      "Train Epoch: 7 [39040/251450]\tLoss: 0.0084 (0.0121) \tAcc: 100.00% (97.64%) \tEmb_Norm: 18.96 (19.56)\n",
      "Train Epoch: 7 [39680/251450]\tLoss: 0.0056 (0.0120) \tAcc: 100.00% (97.65%) \tEmb_Norm: 18.94 (19.55)\n",
      "Train Epoch: 7 [40320/251450]\tLoss: 0.0002 (0.0121) \tAcc: 100.00% (97.63%) \tEmb_Norm: 18.92 (19.54)\n",
      "Train Epoch: 7 [40960/251450]\tLoss: 0.0034 (0.0121) \tAcc: 100.00% (97.62%) \tEmb_Norm: 18.90 (19.53)\n",
      "Train Epoch: 7 [41600/251450]\tLoss: 0.0859 (0.0121) \tAcc: 93.75% (97.63%) \tEmb_Norm: 18.88 (19.52)\n",
      "Train Epoch: 7 [42240/251450]\tLoss: 0.0251 (0.0122) \tAcc: 93.75% (97.61%) \tEmb_Norm: 18.86 (19.51)\n",
      "Train Epoch: 7 [42880/251450]\tLoss: 0.0065 (0.0122) \tAcc: 100.00% (97.61%) \tEmb_Norm: 18.84 (19.50)\n",
      "Train Epoch: 7 [43520/251450]\tLoss: 0.0378 (0.0122) \tAcc: 93.75% (97.61%) \tEmb_Norm: 18.82 (19.49)\n",
      "Train Epoch: 7 [44160/251450]\tLoss: 0.0186 (0.0122) \tAcc: 96.88% (97.59%) \tEmb_Norm: 18.80 (19.48)\n",
      "Train Epoch: 7 [44800/251450]\tLoss: 0.0414 (0.0122) \tAcc: 93.75% (97.60%) \tEmb_Norm: 18.78 (19.47)\n",
      "Train Epoch: 7 [45440/251450]\tLoss: 0.0085 (0.0122) \tAcc: 96.88% (97.59%) \tEmb_Norm: 18.76 (19.46)\n",
      "Train Epoch: 7 [46080/251450]\tLoss: 0.0000 (0.0123) \tAcc: 100.00% (97.59%) \tEmb_Norm: 18.73 (19.45)\n",
      "Train Epoch: 7 [46720/251450]\tLoss: 0.0277 (0.0122) \tAcc: 93.75% (97.59%) \tEmb_Norm: 18.72 (19.44)\n",
      "Train Epoch: 7 [47360/251450]\tLoss: 0.0258 (0.0122) \tAcc: 93.75% (97.59%) \tEmb_Norm: 18.70 (19.43)\n",
      "Train Epoch: 7 [48000/251450]\tLoss: 0.0144 (0.0123) \tAcc: 93.75% (97.58%) \tEmb_Norm: 18.68 (19.42)\n",
      "Train Epoch: 7 [48640/251450]\tLoss: 0.0158 (0.0123) \tAcc: 96.88% (97.58%) \tEmb_Norm: 18.65 (19.41)\n",
      "Train Epoch: 7 [49280/251450]\tLoss: 0.0140 (0.0123) \tAcc: 96.88% (97.57%) \tEmb_Norm: 18.63 (19.40)\n",
      "Train Epoch: 7 [49920/251450]\tLoss: 0.0081 (0.0124) \tAcc: 96.88% (97.57%) \tEmb_Norm: 18.61 (19.39)\n",
      "Train Epoch: 7 [50560/251450]\tLoss: 0.0055 (0.0123) \tAcc: 100.00% (97.57%) \tEmb_Norm: 18.59 (19.38)\n",
      "Train Epoch: 7 [51200/251450]\tLoss: 0.0199 (0.0124) \tAcc: 96.88% (97.57%) \tEmb_Norm: 18.57 (19.37)\n",
      "Train Epoch: 7 [51840/251450]\tLoss: 0.0348 (0.0124) \tAcc: 96.88% (97.57%) \tEmb_Norm: 18.55 (19.36)\n",
      "Train Epoch: 7 [52480/251450]\tLoss: 0.0169 (0.0125) \tAcc: 96.88% (97.55%) \tEmb_Norm: 18.53 (19.35)\n",
      "Train Epoch: 7 [53120/251450]\tLoss: 0.0167 (0.0125) \tAcc: 96.88% (97.55%) \tEmb_Norm: 18.51 (19.34)\n",
      "Train Epoch: 7 [53760/251450]\tLoss: 0.0107 (0.0125) \tAcc: 100.00% (97.55%) \tEmb_Norm: 18.49 (19.33)\n",
      "Train Epoch: 7 [54400/251450]\tLoss: 0.0240 (0.0126) \tAcc: 93.75% (97.54%) \tEmb_Norm: 18.47 (19.32)\n",
      "Train Epoch: 7 [55040/251450]\tLoss: 0.0039 (0.0126) \tAcc: 100.00% (97.54%) \tEmb_Norm: 18.45 (19.31)\n",
      "Train Epoch: 7 [55680/251450]\tLoss: 0.0001 (0.0126) \tAcc: 100.00% (97.55%) \tEmb_Norm: 18.43 (19.30)\n",
      "Train Epoch: 7 [56320/251450]\tLoss: 0.0189 (0.0126) \tAcc: 96.88% (97.55%) \tEmb_Norm: 18.41 (19.29)\n",
      "Train Epoch: 7 [56960/251450]\tLoss: 0.0050 (0.0126) \tAcc: 100.00% (97.54%) \tEmb_Norm: 18.39 (19.28)\n",
      "Train Epoch: 7 [57600/251450]\tLoss: 0.0007 (0.0126) \tAcc: 100.00% (97.54%) \tEmb_Norm: 18.37 (19.27)\n",
      "Train Epoch: 7 [58240/251450]\tLoss: 0.0214 (0.0126) \tAcc: 93.75% (97.54%) \tEmb_Norm: 18.35 (19.26)\n",
      "Train Epoch: 7 [58880/251450]\tLoss: 0.0075 (0.0126) \tAcc: 96.88% (97.54%) \tEmb_Norm: 18.33 (19.25)\n",
      "Train Epoch: 7 [59520/251450]\tLoss: 0.0034 (0.0126) \tAcc: 100.00% (97.55%) \tEmb_Norm: 18.31 (19.24)\n",
      "Train Epoch: 7 [60160/251450]\tLoss: 0.0154 (0.0126) \tAcc: 96.88% (97.56%) \tEmb_Norm: 18.29 (19.23)\n",
      "Train Epoch: 7 [60800/251450]\tLoss: 0.0085 (0.0126) \tAcc: 96.88% (97.55%) \tEmb_Norm: 18.27 (19.22)\n",
      "Train Epoch: 7 [61440/251450]\tLoss: 0.0223 (0.0125) \tAcc: 96.88% (97.55%) \tEmb_Norm: 18.25 (19.21)\n",
      "Train Epoch: 7 [62080/251450]\tLoss: 0.0319 (0.0126) \tAcc: 93.75% (97.54%) \tEmb_Norm: 18.23 (19.20)\n",
      "Train Epoch: 7 [62720/251450]\tLoss: 0.0222 (0.0126) \tAcc: 96.88% (97.53%) \tEmb_Norm: 18.21 (19.19)\n",
      "Train Epoch: 7 [63360/251450]\tLoss: 0.0129 (0.0126) \tAcc: 96.88% (97.53%) \tEmb_Norm: 18.19 (19.18)\n",
      "Train Epoch: 7 [64000/251450]\tLoss: 0.0144 (0.0126) \tAcc: 96.88% (97.53%) \tEmb_Norm: 18.17 (19.17)\n",
      "Train Epoch: 7 [64640/251450]\tLoss: 0.0172 (0.0127) \tAcc: 96.88% (97.51%) \tEmb_Norm: 18.15 (19.16)\n",
      "Train Epoch: 7 [65280/251450]\tLoss: 0.0155 (0.0126) \tAcc: 96.88% (97.52%) \tEmb_Norm: 18.13 (19.15)\n",
      "Train Epoch: 7 [65920/251450]\tLoss: 0.0242 (0.0126) \tAcc: 96.88% (97.53%) \tEmb_Norm: 18.11 (19.14)\n",
      "Train Epoch: 7 [66560/251450]\tLoss: 0.0538 (0.0127) \tAcc: 93.75% (97.53%) \tEmb_Norm: 18.09 (19.13)\n",
      "Train Epoch: 7 [67200/251450]\tLoss: 0.0000 (0.0127) \tAcc: 100.00% (97.52%) \tEmb_Norm: 18.07 (19.12)\n",
      "Train Epoch: 7 [67840/251450]\tLoss: 0.0000 (0.0127) \tAcc: 100.00% (97.52%) \tEmb_Norm: 18.05 (19.11)\n",
      "Train Epoch: 7 [68480/251450]\tLoss: 0.0164 (0.0127) \tAcc: 96.88% (97.52%) \tEmb_Norm: 18.03 (19.10)\n",
      "Train Epoch: 7 [69120/251450]\tLoss: 0.0136 (0.0127) \tAcc: 96.88% (97.52%) \tEmb_Norm: 18.01 (19.09)\n",
      "Train Epoch: 7 [69760/251450]\tLoss: 0.0633 (0.0128) \tAcc: 93.75% (97.51%) \tEmb_Norm: 17.99 (19.08)\n",
      "Train Epoch: 7 [70400/251450]\tLoss: 0.0016 (0.0128) \tAcc: 100.00% (97.51%) \tEmb_Norm: 17.97 (19.07)\n",
      "Train Epoch: 7 [71040/251450]\tLoss: 0.0000 (0.0127) \tAcc: 100.00% (97.52%) \tEmb_Norm: 17.95 (19.06)\n",
      "Train Epoch: 7 [71680/251450]\tLoss: 0.0202 (0.0127) \tAcc: 96.88% (97.52%) \tEmb_Norm: 17.93 (19.05)\n",
      "Train Epoch: 7 [72320/251450]\tLoss: 0.0082 (0.0127) \tAcc: 96.88% (97.52%) \tEmb_Norm: 17.91 (19.04)\n",
      "Train Epoch: 7 [72960/251450]\tLoss: 0.0000 (0.0127) \tAcc: 100.00% (97.53%) \tEmb_Norm: 17.89 (19.03)\n",
      "Train Epoch: 7 [73600/251450]\tLoss: 0.0076 (0.0126) \tAcc: 96.88% (97.53%) \tEmb_Norm: 17.87 (19.02)\n",
      "Train Epoch: 7 [74240/251450]\tLoss: 0.0319 (0.0126) \tAcc: 93.75% (97.53%) \tEmb_Norm: 17.85 (19.01)\n",
      "Train Epoch: 7 [74880/251450]\tLoss: 0.0409 (0.0126) \tAcc: 93.75% (97.54%) \tEmb_Norm: 17.83 (19.00)\n",
      "Train Epoch: 7 [75520/251450]\tLoss: 0.0000 (0.0126) \tAcc: 100.00% (97.54%) \tEmb_Norm: 17.81 (18.99)\n",
      "Train Epoch: 7 [76160/251450]\tLoss: 0.0490 (0.0126) \tAcc: 90.62% (97.54%) \tEmb_Norm: 17.79 (18.98)\n",
      "Train Epoch: 7 [76800/251450]\tLoss: 0.0057 (0.0126) \tAcc: 100.00% (97.54%) \tEmb_Norm: 17.77 (18.97)\n",
      "Train Epoch: 7 [77440/251450]\tLoss: 0.0134 (0.0126) \tAcc: 96.88% (97.54%) \tEmb_Norm: 17.75 (18.96)\n",
      "Train Epoch: 7 [78080/251450]\tLoss: 0.0031 (0.0126) \tAcc: 100.00% (97.55%) \tEmb_Norm: 17.73 (18.95)\n",
      "Train Epoch: 7 [78720/251450]\tLoss: 0.0091 (0.0126) \tAcc: 96.88% (97.54%) \tEmb_Norm: 17.71 (18.94)\n",
      "Train Epoch: 7 [79360/251450]\tLoss: 0.0059 (0.0126) \tAcc: 100.00% (97.54%) \tEmb_Norm: 17.69 (18.93)\n",
      "Train Epoch: 7 [80000/251450]\tLoss: 0.0000 (0.0126) \tAcc: 100.00% (97.54%) \tEmb_Norm: 17.67 (18.92)\n",
      "Train Epoch: 7 [80640/251450]\tLoss: 0.0121 (0.0126) \tAcc: 96.88% (97.54%) \tEmb_Norm: 17.65 (18.91)\n",
      "Train Epoch: 7 [81280/251450]\tLoss: 0.0161 (0.0126) \tAcc: 96.88% (97.54%) \tEmb_Norm: 17.63 (18.90)\n",
      "Train Epoch: 7 [81920/251450]\tLoss: 0.0060 (0.0126) \tAcc: 100.00% (97.53%) \tEmb_Norm: 17.61 (18.89)\n",
      "Train Epoch: 7 [82560/251450]\tLoss: 0.0000 (0.0126) \tAcc: 100.00% (97.54%) \tEmb_Norm: 17.59 (18.88)\n",
      "Train Epoch: 7 [83200/251450]\tLoss: 0.0000 (0.0126) \tAcc: 100.00% (97.54%) \tEmb_Norm: 17.57 (18.87)\n",
      "Train Epoch: 7 [83840/251450]\tLoss: 0.0286 (0.0126) \tAcc: 93.75% (97.54%) \tEmb_Norm: 17.55 (18.86)\n",
      "Train Epoch: 7 [84480/251450]\tLoss: 0.0000 (0.0126) \tAcc: 100.00% (97.54%) \tEmb_Norm: 17.53 (18.85)\n",
      "Train Epoch: 7 [85120/251450]\tLoss: 0.0166 (0.0126) \tAcc: 96.88% (97.54%) \tEmb_Norm: 17.51 (18.84)\n",
      "Train Epoch: 7 [85760/251450]\tLoss: 0.0490 (0.0126) \tAcc: 90.62% (97.54%) \tEmb_Norm: 17.49 (18.83)\n",
      "Train Epoch: 7 [86400/251450]\tLoss: 0.0000 (0.0126) \tAcc: 100.00% (97.54%) \tEmb_Norm: 17.47 (18.82)\n",
      "Train Epoch: 7 [87040/251450]\tLoss: 0.0088 (0.0125) \tAcc: 96.88% (97.54%) \tEmb_Norm: 17.45 (18.81)\n",
      "Train Epoch: 7 [87680/251450]\tLoss: 0.0054 (0.0126) \tAcc: 100.00% (97.54%) \tEmb_Norm: 17.43 (18.80)\n",
      "Train Epoch: 7 [88320/251450]\tLoss: 0.0384 (0.0126) \tAcc: 90.62% (97.54%) \tEmb_Norm: 17.41 (18.79)\n",
      "Train Epoch: 7 [88960/251450]\tLoss: 0.0075 (0.0125) \tAcc: 100.00% (97.55%) \tEmb_Norm: 17.39 (18.78)\n",
      "Train Epoch: 7 [89600/251450]\tLoss: 0.0023 (0.0125) \tAcc: 100.00% (97.55%) \tEmb_Norm: 17.38 (18.77)\n",
      "Train Epoch: 7 [90240/251450]\tLoss: 0.0000 (0.0125) \tAcc: 100.00% (97.56%) \tEmb_Norm: 17.36 (18.76)\n",
      "Train Epoch: 7 [90880/251450]\tLoss: 0.0000 (0.0125) \tAcc: 100.00% (97.56%) \tEmb_Norm: 17.34 (18.75)\n",
      "Train Epoch: 7 [91520/251450]\tLoss: 0.0030 (0.0125) \tAcc: 100.00% (97.55%) \tEmb_Norm: 17.32 (18.74)\n",
      "Train Epoch: 7 [92160/251450]\tLoss: 0.0012 (0.0125) \tAcc: 100.00% (97.56%) \tEmb_Norm: 17.30 (18.73)\n",
      "Train Epoch: 7 [92800/251450]\tLoss: 0.0000 (0.0125) \tAcc: 100.00% (97.56%) \tEmb_Norm: 17.28 (18.72)\n",
      "Train Epoch: 7 [93440/251450]\tLoss: 0.0231 (0.0125) \tAcc: 93.75% (97.56%) \tEmb_Norm: 17.26 (18.71)\n",
      "Train Epoch: 7 [94080/251450]\tLoss: 0.0117 (0.0125) \tAcc: 96.88% (97.56%) \tEmb_Norm: 17.24 (18.70)\n",
      "Train Epoch: 7 [94720/251450]\tLoss: 0.0175 (0.0125) \tAcc: 96.88% (97.55%) \tEmb_Norm: 17.22 (18.69)\n",
      "Train Epoch: 7 [95360/251450]\tLoss: 0.0000 (0.0125) \tAcc: 100.00% (97.56%) \tEmb_Norm: 17.20 (18.68)\n",
      "Train Epoch: 7 [96000/251450]\tLoss: 0.0040 (0.0125) \tAcc: 100.00% (97.56%) \tEmb_Norm: 17.18 (18.67)\n",
      "Train Epoch: 7 [96640/251450]\tLoss: 0.0072 (0.0125) \tAcc: 100.00% (97.56%) \tEmb_Norm: 17.16 (18.66)\n",
      "Train Epoch: 7 [97280/251450]\tLoss: 0.0071 (0.0125) \tAcc: 100.00% (97.57%) \tEmb_Norm: 17.14 (18.65)\n",
      "Train Epoch: 7 [97920/251450]\tLoss: 0.0037 (0.0125) \tAcc: 100.00% (97.56%) \tEmb_Norm: 17.12 (18.64)\n",
      "Train Epoch: 7 [98560/251450]\tLoss: 0.0562 (0.0125) \tAcc: 93.75% (97.56%) \tEmb_Norm: 17.10 (18.63)\n",
      "Train Epoch: 7 [99200/251450]\tLoss: 0.0000 (0.0125) \tAcc: 100.00% (97.56%) \tEmb_Norm: 17.08 (18.62)\n",
      "Train Epoch: 7 [99840/251450]\tLoss: 0.0241 (0.0125) \tAcc: 96.88% (97.56%) \tEmb_Norm: 17.06 (18.61)\n",
      "Train Epoch: 7 [100480/251450]\tLoss: 0.0172 (0.0125) \tAcc: 96.88% (97.56%) \tEmb_Norm: 17.04 (18.60)\n",
      "Train Epoch: 7 [101120/251450]\tLoss: 0.0046 (0.0125) \tAcc: 100.00% (97.56%) \tEmb_Norm: 17.02 (18.59)\n",
      "Train Epoch: 7 [101760/251450]\tLoss: 0.0113 (0.0125) \tAcc: 100.00% (97.56%) \tEmb_Norm: 17.00 (18.58)\n",
      "Train Epoch: 7 [102400/251450]\tLoss: 0.0021 (0.0125) \tAcc: 100.00% (97.57%) \tEmb_Norm: 16.98 (18.57)\n",
      "Train Epoch: 7 [103040/251450]\tLoss: 0.0087 (0.0125) \tAcc: 100.00% (97.56%) \tEmb_Norm: 16.96 (18.56)\n",
      "Train Epoch: 7 [103680/251450]\tLoss: 0.0201 (0.0125) \tAcc: 96.88% (97.57%) \tEmb_Norm: 16.94 (18.55)\n",
      "Train Epoch: 7 [104320/251450]\tLoss: 0.0134 (0.0125) \tAcc: 100.00% (97.57%) \tEmb_Norm: 16.92 (18.54)\n",
      "Train Epoch: 7 [104960/251450]\tLoss: 0.0000 (0.0125) \tAcc: 100.00% (97.57%) \tEmb_Norm: 16.90 (18.53)\n",
      "Train Epoch: 7 [105600/251450]\tLoss: 0.0000 (0.0125) \tAcc: 100.00% (97.57%) \tEmb_Norm: 16.88 (18.52)\n",
      "Train Epoch: 7 [106240/251450]\tLoss: 0.0016 (0.0125) \tAcc: 100.00% (97.57%) \tEmb_Norm: 16.86 (18.51)\n",
      "Train Epoch: 7 [106880/251450]\tLoss: 0.0322 (0.0125) \tAcc: 96.88% (97.57%) \tEmb_Norm: 16.84 (18.50)\n",
      "Train Epoch: 7 [107520/251450]\tLoss: 0.0055 (0.0125) \tAcc: 100.00% (97.57%) \tEmb_Norm: 16.82 (18.49)\n",
      "Train Epoch: 7 [108160/251450]\tLoss: 0.0233 (0.0125) \tAcc: 96.88% (97.58%) \tEmb_Norm: 16.80 (18.48)\n",
      "Train Epoch: 7 [108800/251450]\tLoss: 0.0043 (0.0124) \tAcc: 100.00% (97.58%) \tEmb_Norm: 16.78 (18.47)\n",
      "Train Epoch: 7 [109440/251450]\tLoss: 0.0071 (0.0124) \tAcc: 96.88% (97.58%) \tEmb_Norm: 16.76 (18.46)\n",
      "Train Epoch: 7 [110080/251450]\tLoss: 0.0104 (0.0124) \tAcc: 96.88% (97.59%) \tEmb_Norm: 16.74 (18.45)\n",
      "Train Epoch: 7 [110720/251450]\tLoss: 0.0205 (0.0124) \tAcc: 90.62% (97.59%) \tEmb_Norm: 16.73 (18.44)\n",
      "Train Epoch: 7 [111360/251450]\tLoss: 0.0000 (0.0124) \tAcc: 100.00% (97.60%) \tEmb_Norm: 16.71 (18.43)\n",
      "Train Epoch: 7 [112000/251450]\tLoss: 0.0000 (0.0124) \tAcc: 100.00% (97.60%) \tEmb_Norm: 16.69 (18.42)\n",
      "Train Epoch: 7 [112640/251450]\tLoss: 0.0250 (0.0124) \tAcc: 93.75% (97.59%) \tEmb_Norm: 16.67 (18.41)\n",
      "Train Epoch: 7 [113280/251450]\tLoss: 0.0000 (0.0124) \tAcc: 100.00% (97.59%) \tEmb_Norm: 16.65 (18.40)\n",
      "Train Epoch: 7 [113920/251450]\tLoss: 0.0088 (0.0124) \tAcc: 96.88% (97.59%) \tEmb_Norm: 16.63 (18.39)\n",
      "Train Epoch: 7 [114560/251450]\tLoss: 0.0028 (0.0124) \tAcc: 100.00% (97.59%) \tEmb_Norm: 16.61 (18.38)\n",
      "Train Epoch: 7 [115200/251450]\tLoss: 0.0251 (0.0124) \tAcc: 93.75% (97.58%) \tEmb_Norm: 16.59 (18.37)\n",
      "Train Epoch: 7 [115840/251450]\tLoss: 0.0123 (0.0124) \tAcc: 96.88% (97.58%) \tEmb_Norm: 16.57 (18.36)\n",
      "Train Epoch: 7 [116480/251450]\tLoss: 0.0000 (0.0124) \tAcc: 100.00% (97.59%) \tEmb_Norm: 16.55 (18.35)\n",
      "Train Epoch: 7 [117120/251450]\tLoss: 0.0112 (0.0124) \tAcc: 96.88% (97.59%) \tEmb_Norm: 16.53 (18.34)\n",
      "Train Epoch: 7 [117760/251450]\tLoss: 0.0224 (0.0124) \tAcc: 96.88% (97.59%) \tEmb_Norm: 16.51 (18.33)\n",
      "Train Epoch: 7 [118400/251450]\tLoss: 0.0080 (0.0124) \tAcc: 96.88% (97.58%) \tEmb_Norm: 16.49 (18.32)\n",
      "Train Epoch: 7 [119040/251450]\tLoss: 0.0122 (0.0124) \tAcc: 96.88% (97.58%) \tEmb_Norm: 16.47 (18.31)\n",
      "Train Epoch: 7 [119680/251450]\tLoss: 0.0573 (0.0124) \tAcc: 93.75% (97.58%) \tEmb_Norm: 16.45 (18.31)\n",
      "Train Epoch: 7 [120320/251450]\tLoss: 0.0000 (0.0124) \tAcc: 100.00% (97.58%) \tEmb_Norm: 16.43 (18.30)\n",
      "Train Epoch: 7 [120960/251450]\tLoss: 0.0080 (0.0124) \tAcc: 96.88% (97.58%) \tEmb_Norm: 16.41 (18.29)\n",
      "Train Epoch: 7 [121600/251450]\tLoss: 0.0216 (0.0124) \tAcc: 96.88% (97.58%) \tEmb_Norm: 16.39 (18.28)\n",
      "Train Epoch: 7 [122240/251450]\tLoss: 0.0000 (0.0124) \tAcc: 100.00% (97.58%) \tEmb_Norm: 16.37 (18.27)\n",
      "Train Epoch: 7 [122880/251450]\tLoss: 0.0000 (0.0124) \tAcc: 100.00% (97.59%) \tEmb_Norm: 16.35 (18.26)\n",
      "Train Epoch: 7 [123520/251450]\tLoss: 0.0132 (0.0124) \tAcc: 96.88% (97.58%) \tEmb_Norm: 16.33 (18.25)\n",
      "Train Epoch: 7 [124160/251450]\tLoss: 0.0270 (0.0124) \tAcc: 93.75% (97.59%) \tEmb_Norm: 16.31 (18.24)\n",
      "Train Epoch: 7 [124800/251450]\tLoss: 0.0000 (0.0124) \tAcc: 100.00% (97.59%) \tEmb_Norm: 16.29 (18.23)\n",
      "Train Epoch: 7 [125440/251450]\tLoss: 0.0018 (0.0124) \tAcc: 100.00% (97.59%) \tEmb_Norm: 16.27 (18.22)\n",
      "Train Epoch: 7 [126080/251450]\tLoss: 0.0185 (0.0124) \tAcc: 96.88% (97.59%) \tEmb_Norm: 16.25 (18.21)\n",
      "Train Epoch: 7 [126720/251450]\tLoss: 0.0093 (0.0124) \tAcc: 96.88% (97.58%) \tEmb_Norm: 16.23 (18.20)\n",
      "Train Epoch: 7 [127360/251450]\tLoss: 0.0172 (0.0124) \tAcc: 96.88% (97.59%) \tEmb_Norm: 16.21 (18.19)\n",
      "Train Epoch: 7 [128000/251450]\tLoss: 0.0000 (0.0124) \tAcc: 100.00% (97.59%) \tEmb_Norm: 16.19 (18.18)\n",
      "Train Epoch: 7 [128640/251450]\tLoss: 0.0000 (0.0124) \tAcc: 100.00% (97.59%) \tEmb_Norm: 16.17 (18.17)\n",
      "Train Epoch: 7 [129280/251450]\tLoss: 0.0000 (0.0124) \tAcc: 100.00% (97.59%) \tEmb_Norm: 16.15 (18.16)\n",
      "Train Epoch: 7 [129920/251450]\tLoss: 0.0059 (0.0124) \tAcc: 100.00% (97.59%) \tEmb_Norm: 16.13 (18.15)\n",
      "Train Epoch: 7 [130560/251450]\tLoss: 0.0128 (0.0124) \tAcc: 96.88% (97.59%) \tEmb_Norm: 16.11 (18.14)\n",
      "Train Epoch: 7 [131200/251450]\tLoss: 0.0592 (0.0124) \tAcc: 93.75% (97.60%) \tEmb_Norm: 16.09 (18.13)\n",
      "Train Epoch: 7 [131840/251450]\tLoss: 0.0009 (0.0124) \tAcc: 100.00% (97.60%) \tEmb_Norm: 16.07 (18.12)\n",
      "Train Epoch: 7 [132480/251450]\tLoss: 0.0361 (0.0124) \tAcc: 90.62% (97.60%) \tEmb_Norm: 16.05 (18.11)\n",
      "Train Epoch: 7 [133120/251450]\tLoss: 0.0100 (0.0123) \tAcc: 96.88% (97.60%) \tEmb_Norm: 16.03 (18.10)\n",
      "Train Epoch: 7 [133760/251450]\tLoss: 0.0205 (0.0124) \tAcc: 96.88% (97.60%) \tEmb_Norm: 16.01 (18.09)\n",
      "Train Epoch: 7 [134400/251450]\tLoss: 0.0052 (0.0124) \tAcc: 100.00% (97.59%) \tEmb_Norm: 15.99 (18.08)\n",
      "Train Epoch: 7 [135040/251450]\tLoss: 0.0000 (0.0124) \tAcc: 100.00% (97.59%) \tEmb_Norm: 15.97 (18.07)\n",
      "Train Epoch: 7 [135680/251450]\tLoss: 0.0120 (0.0124) \tAcc: 96.88% (97.59%) \tEmb_Norm: 15.96 (18.06)\n",
      "Train Epoch: 7 [136320/251450]\tLoss: 0.0022 (0.0124) \tAcc: 100.00% (97.59%) \tEmb_Norm: 15.94 (18.05)\n",
      "Train Epoch: 7 [136960/251450]\tLoss: 0.0113 (0.0124) \tAcc: 96.88% (97.59%) \tEmb_Norm: 15.92 (18.04)\n",
      "Train Epoch: 7 [137600/251450]\tLoss: 0.0197 (0.0124) \tAcc: 93.75% (97.60%) \tEmb_Norm: 15.90 (18.03)\n",
      "Train Epoch: 7 [138240/251450]\tLoss: 0.0117 (0.0124) \tAcc: 96.88% (97.59%) \tEmb_Norm: 15.88 (18.02)\n",
      "Train Epoch: 7 [138880/251450]\tLoss: 0.0042 (0.0124) \tAcc: 100.00% (97.59%) \tEmb_Norm: 15.86 (18.01)\n",
      "Train Epoch: 7 [139520/251450]\tLoss: 0.0363 (0.0124) \tAcc: 90.62% (97.59%) \tEmb_Norm: 15.84 (18.00)\n",
      "Train Epoch: 7 [140160/251450]\tLoss: 0.0216 (0.0124) \tAcc: 93.75% (97.59%) \tEmb_Norm: 15.82 (17.99)\n",
      "Train Epoch: 7 [140800/251450]\tLoss: 0.0116 (0.0124) \tAcc: 100.00% (97.59%) \tEmb_Norm: 15.80 (17.98)\n",
      "Train Epoch: 7 [141440/251450]\tLoss: 0.0000 (0.0124) \tAcc: 100.00% (97.59%) \tEmb_Norm: 15.78 (17.97)\n",
      "Train Epoch: 7 [142080/251450]\tLoss: 0.0061 (0.0124) \tAcc: 100.00% (97.60%) \tEmb_Norm: 15.76 (17.96)\n",
      "Train Epoch: 7 [142720/251450]\tLoss: 0.0248 (0.0124) \tAcc: 93.75% (97.60%) \tEmb_Norm: 15.74 (17.95)\n",
      "Train Epoch: 7 [143360/251450]\tLoss: 0.0129 (0.0124) \tAcc: 96.88% (97.60%) \tEmb_Norm: 15.72 (17.94)\n",
      "Train Epoch: 7 [144000/251450]\tLoss: 0.0519 (0.0124) \tAcc: 90.62% (97.60%) \tEmb_Norm: 15.70 (17.93)\n",
      "Train Epoch: 7 [144640/251450]\tLoss: 0.0000 (0.0124) \tAcc: 100.00% (97.60%) \tEmb_Norm: 15.68 (17.92)\n",
      "Train Epoch: 7 [145280/251450]\tLoss: 0.0524 (0.0124) \tAcc: 90.62% (97.59%) \tEmb_Norm: 15.66 (17.91)\n",
      "Train Epoch: 7 [145920/251450]\tLoss: 0.0110 (0.0124) \tAcc: 100.00% (97.59%) \tEmb_Norm: 15.64 (17.90)\n",
      "Train Epoch: 7 [146560/251450]\tLoss: 0.0221 (0.0124) \tAcc: 96.88% (97.59%) \tEmb_Norm: 15.62 (17.89)\n",
      "Train Epoch: 7 [147200/251450]\tLoss: 0.0109 (0.0124) \tAcc: 96.88% (97.59%) \tEmb_Norm: 15.60 (17.88)\n",
      "Train Epoch: 7 [147840/251450]\tLoss: 0.0000 (0.0124) \tAcc: 100.00% (97.59%) \tEmb_Norm: 15.58 (17.87)\n",
      "Train Epoch: 7 [148480/251450]\tLoss: 0.0307 (0.0124) \tAcc: 96.88% (97.59%) \tEmb_Norm: 15.56 (17.86)\n",
      "Train Epoch: 7 [149120/251450]\tLoss: 0.0025 (0.0124) \tAcc: 100.00% (97.59%) \tEmb_Norm: 15.54 (17.85)\n",
      "Train Epoch: 7 [149760/251450]\tLoss: 0.0264 (0.0124) \tAcc: 96.88% (97.59%) \tEmb_Norm: 15.52 (17.84)\n",
      "Train Epoch: 7 [150400/251450]\tLoss: 0.0038 (0.0124) \tAcc: 100.00% (97.60%) \tEmb_Norm: 15.51 (17.83)\n",
      "Train Epoch: 7 [151040/251450]\tLoss: 0.0256 (0.0124) \tAcc: 96.88% (97.60%) \tEmb_Norm: 15.49 (17.82)\n",
      "Train Epoch: 7 [151680/251450]\tLoss: 0.0000 (0.0123) \tAcc: 100.00% (97.60%) \tEmb_Norm: 15.47 (17.81)\n",
      "Train Epoch: 7 [152320/251450]\tLoss: 0.0002 (0.0123) \tAcc: 100.00% (97.60%) \tEmb_Norm: 15.45 (17.80)\n",
      "Train Epoch: 7 [152960/251450]\tLoss: 0.0124 (0.0123) \tAcc: 96.88% (97.61%) \tEmb_Norm: 15.43 (17.79)\n",
      "Train Epoch: 7 [153600/251450]\tLoss: 0.0000 (0.0123) \tAcc: 100.00% (97.61%) \tEmb_Norm: 15.41 (17.78)\n",
      "Train Epoch: 7 [154240/251450]\tLoss: 0.0170 (0.0123) \tAcc: 96.88% (97.61%) \tEmb_Norm: 15.39 (17.77)\n",
      "Train Epoch: 7 [154880/251450]\tLoss: 0.0230 (0.0123) \tAcc: 93.75% (97.61%) \tEmb_Norm: 15.37 (17.76)\n",
      "Train Epoch: 7 [155520/251450]\tLoss: 0.0078 (0.0123) \tAcc: 100.00% (97.61%) \tEmb_Norm: 15.35 (17.75)\n",
      "Train Epoch: 7 [156160/251450]\tLoss: 0.0169 (0.0123) \tAcc: 96.88% (97.61%) \tEmb_Norm: 15.33 (17.74)\n",
      "Train Epoch: 7 [156800/251450]\tLoss: 0.0006 (0.0123) \tAcc: 100.00% (97.61%) \tEmb_Norm: 15.31 (17.73)\n",
      "Train Epoch: 7 [157440/251450]\tLoss: 0.0200 (0.0123) \tAcc: 100.00% (97.61%) \tEmb_Norm: 15.29 (17.72)\n",
      "Train Epoch: 7 [158080/251450]\tLoss: 0.0142 (0.0123) \tAcc: 96.88% (97.61%) \tEmb_Norm: 15.27 (17.71)\n",
      "Train Epoch: 7 [158720/251450]\tLoss: 0.0003 (0.0123) \tAcc: 100.00% (97.61%) \tEmb_Norm: 15.25 (17.70)\n",
      "Train Epoch: 7 [159360/251450]\tLoss: 0.0000 (0.0123) \tAcc: 100.00% (97.61%) \tEmb_Norm: 15.23 (17.69)\n",
      "Train Epoch: 7 [160000/251450]\tLoss: 0.0204 (0.0123) \tAcc: 96.88% (97.61%) \tEmb_Norm: 15.21 (17.68)\n",
      "Train Epoch: 7 [160640/251450]\tLoss: 0.0086 (0.0123) \tAcc: 96.88% (97.61%) \tEmb_Norm: 15.19 (17.67)\n",
      "Train Epoch: 7 [161280/251450]\tLoss: 0.0000 (0.0123) \tAcc: 100.00% (97.61%) \tEmb_Norm: 15.17 (17.66)\n",
      "Train Epoch: 7 [161920/251450]\tLoss: 0.0015 (0.0123) \tAcc: 100.00% (97.61%) \tEmb_Norm: 15.15 (17.65)\n",
      "Train Epoch: 7 [162560/251450]\tLoss: 0.0073 (0.0123) \tAcc: 96.88% (97.61%) \tEmb_Norm: 15.13 (17.64)\n",
      "Train Epoch: 7 [163200/251450]\tLoss: 0.0111 (0.0123) \tAcc: 96.88% (97.62%) \tEmb_Norm: 15.11 (17.63)\n",
      "Train Epoch: 7 [163840/251450]\tLoss: 0.0045 (0.0123) \tAcc: 100.00% (97.62%) \tEmb_Norm: 15.09 (17.62)\n",
      "Train Epoch: 7 [164480/251450]\tLoss: 0.0015 (0.0123) \tAcc: 100.00% (97.61%) \tEmb_Norm: 15.08 (17.61)\n",
      "Train Epoch: 7 [165120/251450]\tLoss: 0.0022 (0.0123) \tAcc: 100.00% (97.61%) \tEmb_Norm: 15.06 (17.60)\n",
      "Train Epoch: 7 [165760/251450]\tLoss: 0.0206 (0.0123) \tAcc: 96.88% (97.61%) \tEmb_Norm: 15.04 (17.59)\n",
      "Train Epoch: 7 [166400/251450]\tLoss: 0.0028 (0.0123) \tAcc: 100.00% (97.61%) \tEmb_Norm: 15.02 (17.58)\n",
      "Train Epoch: 7 [167040/251450]\tLoss: 0.0025 (0.0123) \tAcc: 100.00% (97.61%) \tEmb_Norm: 15.00 (17.57)\n",
      "Train Epoch: 7 [167680/251450]\tLoss: 0.0153 (0.0123) \tAcc: 96.88% (97.61%) \tEmb_Norm: 14.98 (17.56)\n",
      "Train Epoch: 7 [168320/251450]\tLoss: 0.0263 (0.0123) \tAcc: 96.88% (97.61%) \tEmb_Norm: 14.96 (17.55)\n",
      "Train Epoch: 7 [168960/251450]\tLoss: 0.0118 (0.0123) \tAcc: 96.88% (97.61%) \tEmb_Norm: 14.94 (17.54)\n",
      "Train Epoch: 7 [169600/251450]\tLoss: 0.0215 (0.0123) \tAcc: 96.88% (97.61%) \tEmb_Norm: 14.92 (17.53)\n",
      "Train Epoch: 7 [170240/251450]\tLoss: 0.0131 (0.0123) \tAcc: 96.88% (97.61%) \tEmb_Norm: 14.90 (17.52)\n",
      "Train Epoch: 7 [170880/251450]\tLoss: 0.0005 (0.0123) \tAcc: 100.00% (97.61%) \tEmb_Norm: 14.88 (17.51)\n",
      "Train Epoch: 7 [171520/251450]\tLoss: 0.0318 (0.0123) \tAcc: 90.62% (97.61%) \tEmb_Norm: 14.86 (17.50)\n",
      "Train Epoch: 7 [172160/251450]\tLoss: 0.0073 (0.0123) \tAcc: 96.88% (97.61%) \tEmb_Norm: 14.84 (17.49)\n",
      "Train Epoch: 7 [172800/251450]\tLoss: 0.0075 (0.0123) \tAcc: 96.88% (97.60%) \tEmb_Norm: 14.82 (17.48)\n",
      "Train Epoch: 7 [173440/251450]\tLoss: 0.0172 (0.0123) \tAcc: 96.88% (97.61%) \tEmb_Norm: 14.80 (17.47)\n",
      "Train Epoch: 7 [174080/251450]\tLoss: 0.0315 (0.0123) \tAcc: 93.75% (97.61%) \tEmb_Norm: 14.78 (17.46)\n",
      "Train Epoch: 7 [174720/251450]\tLoss: 0.0013 (0.0123) \tAcc: 100.00% (97.61%) \tEmb_Norm: 14.76 (17.45)\n",
      "Train Epoch: 7 [175360/251450]\tLoss: 0.0148 (0.0123) \tAcc: 96.88% (97.61%) \tEmb_Norm: 14.74 (17.44)\n",
      "Train Epoch: 7 [176000/251450]\tLoss: 0.0058 (0.0123) \tAcc: 100.00% (97.61%) \tEmb_Norm: 14.72 (17.43)\n",
      "Train Epoch: 7 [176640/251450]\tLoss: 0.0242 (0.0123) \tAcc: 93.75% (97.61%) \tEmb_Norm: 14.70 (17.42)\n",
      "Train Epoch: 7 [177280/251450]\tLoss: 0.0229 (0.0123) \tAcc: 96.88% (97.61%) \tEmb_Norm: 14.69 (17.41)\n",
      "Train Epoch: 7 [177920/251450]\tLoss: 0.0101 (0.0123) \tAcc: 96.88% (97.61%) \tEmb_Norm: 14.66 (17.40)\n",
      "Train Epoch: 7 [178560/251450]\tLoss: 0.0191 (0.0123) \tAcc: 96.88% (97.61%) \tEmb_Norm: 14.64 (17.39)\n",
      "Train Epoch: 7 [179200/251450]\tLoss: 0.0000 (0.0123) \tAcc: 100.00% (97.61%) \tEmb_Norm: 14.63 (17.39)\n",
      "Train Epoch: 7 [179840/251450]\tLoss: 0.0089 (0.0123) \tAcc: 100.00% (97.61%) \tEmb_Norm: 14.61 (17.38)\n",
      "Train Epoch: 7 [180480/251450]\tLoss: 0.0000 (0.0123) \tAcc: 100.00% (97.62%) \tEmb_Norm: 14.59 (17.37)\n",
      "Train Epoch: 7 [181120/251450]\tLoss: 0.0078 (0.0123) \tAcc: 100.00% (97.61%) \tEmb_Norm: 14.57 (17.36)\n",
      "Train Epoch: 7 [181760/251450]\tLoss: 0.0099 (0.0123) \tAcc: 96.88% (97.61%) \tEmb_Norm: 14.55 (17.35)\n",
      "Train Epoch: 7 [182400/251450]\tLoss: 0.0034 (0.0123) \tAcc: 100.00% (97.62%) \tEmb_Norm: 14.53 (17.34)\n",
      "Train Epoch: 7 [183040/251450]\tLoss: 0.0386 (0.0123) \tAcc: 96.88% (97.61%) \tEmb_Norm: 14.51 (17.33)\n",
      "Train Epoch: 7 [183680/251450]\tLoss: 0.0142 (0.0123) \tAcc: 96.88% (97.61%) \tEmb_Norm: 14.49 (17.32)\n",
      "Train Epoch: 7 [184320/251450]\tLoss: 0.0004 (0.0123) \tAcc: 100.00% (97.62%) \tEmb_Norm: 14.47 (17.31)\n",
      "Train Epoch: 7 [184960/251450]\tLoss: 0.0058 (0.0123) \tAcc: 100.00% (97.62%) \tEmb_Norm: 14.45 (17.30)\n",
      "Train Epoch: 7 [185600/251450]\tLoss: 0.0056 (0.0123) \tAcc: 100.00% (97.62%) \tEmb_Norm: 14.44 (17.29)\n",
      "Train Epoch: 7 [186240/251450]\tLoss: 0.0074 (0.0123) \tAcc: 96.88% (97.62%) \tEmb_Norm: 14.42 (17.28)\n",
      "Train Epoch: 7 [186880/251450]\tLoss: 0.0018 (0.0123) \tAcc: 100.00% (97.63%) \tEmb_Norm: 14.40 (17.27)\n",
      "Train Epoch: 7 [187520/251450]\tLoss: 0.0033 (0.0123) \tAcc: 100.00% (97.63%) \tEmb_Norm: 14.38 (17.26)\n",
      "Train Epoch: 7 [188160/251450]\tLoss: 0.0000 (0.0122) \tAcc: 100.00% (97.63%) \tEmb_Norm: 14.36 (17.25)\n",
      "Train Epoch: 7 [188800/251450]\tLoss: 0.0113 (0.0123) \tAcc: 96.88% (97.63%) \tEmb_Norm: 14.34 (17.24)\n",
      "Train Epoch: 7 [189440/251450]\tLoss: 0.0065 (0.0122) \tAcc: 100.00% (97.63%) \tEmb_Norm: 14.32 (17.23)\n",
      "Train Epoch: 7 [190080/251450]\tLoss: 0.0057 (0.0122) \tAcc: 100.00% (97.63%) \tEmb_Norm: 14.30 (17.22)\n",
      "Train Epoch: 7 [190720/251450]\tLoss: 0.0004 (0.0122) \tAcc: 100.00% (97.63%) \tEmb_Norm: 14.28 (17.21)\n",
      "Train Epoch: 7 [191360/251450]\tLoss: 0.0167 (0.0122) \tAcc: 96.88% (97.64%) \tEmb_Norm: 14.27 (17.20)\n",
      "Train Epoch: 7 [192000/251450]\tLoss: 0.0073 (0.0122) \tAcc: 100.00% (97.64%) \tEmb_Norm: 14.25 (17.19)\n",
      "Train Epoch: 7 [192640/251450]\tLoss: 0.0109 (0.0122) \tAcc: 96.88% (97.64%) \tEmb_Norm: 14.23 (17.18)\n",
      "Train Epoch: 7 [193280/251450]\tLoss: 0.0092 (0.0122) \tAcc: 96.88% (97.64%) \tEmb_Norm: 14.21 (17.17)\n",
      "Train Epoch: 7 [193920/251450]\tLoss: 0.0025 (0.0122) \tAcc: 100.00% (97.64%) \tEmb_Norm: 14.19 (17.16)\n",
      "Train Epoch: 7 [194560/251450]\tLoss: 0.0063 (0.0122) \tAcc: 100.00% (97.64%) \tEmb_Norm: 14.17 (17.15)\n",
      "Train Epoch: 7 [195200/251450]\tLoss: 0.0113 (0.0122) \tAcc: 96.88% (97.64%) \tEmb_Norm: 14.15 (17.14)\n",
      "Train Epoch: 7 [195840/251450]\tLoss: 0.0102 (0.0122) \tAcc: 96.88% (97.64%) \tEmb_Norm: 14.14 (17.13)\n",
      "Train Epoch: 7 [196480/251450]\tLoss: 0.0127 (0.0122) \tAcc: 96.88% (97.64%) \tEmb_Norm: 14.12 (17.12)\n",
      "Train Epoch: 7 [197120/251450]\tLoss: 0.0057 (0.0122) \tAcc: 100.00% (97.64%) \tEmb_Norm: 14.10 (17.11)\n",
      "Train Epoch: 7 [197760/251450]\tLoss: 0.0038 (0.0122) \tAcc: 100.00% (97.64%) \tEmb_Norm: 14.08 (17.10)\n",
      "Train Epoch: 7 [198400/251450]\tLoss: 0.0151 (0.0122) \tAcc: 93.75% (97.64%) \tEmb_Norm: 14.06 (17.09)\n",
      "Train Epoch: 7 [199040/251450]\tLoss: 0.0131 (0.0122) \tAcc: 96.88% (97.64%) \tEmb_Norm: 14.04 (17.08)\n",
      "Train Epoch: 7 [199680/251450]\tLoss: 0.0232 (0.0122) \tAcc: 96.88% (97.64%) \tEmb_Norm: 14.02 (17.07)\n",
      "Train Epoch: 7 [200320/251450]\tLoss: 0.0070 (0.0122) \tAcc: 96.88% (97.64%) \tEmb_Norm: 14.00 (17.06)\n",
      "Train Epoch: 7 [200960/251450]\tLoss: 0.0135 (0.0122) \tAcc: 96.88% (97.64%) \tEmb_Norm: 13.98 (17.05)\n",
      "Train Epoch: 7 [201600/251450]\tLoss: 0.0247 (0.0122) \tAcc: 96.88% (97.64%) \tEmb_Norm: 13.96 (17.04)\n",
      "=============== TEST epoch 7 ===============\n",
      "\n",
      "Test set: Average loss: 0.0179, Accuracy: 96.43%\n",
      "\n",
      "=============== checkpoint epoch 7 ===============\n",
      "=============== TRAIN epoch 8 ===============\n",
      "Train Epoch: 8 [0/251450]\tLoss: 0.0000 (0.0000) \tAcc: 100.00% (100.00%) \tEmb_Norm: 13.95 (13.95)\n",
      "Train Epoch: 8 [640/251450]\tLoss: 0.0000 (0.0087) \tAcc: 100.00% (98.21%) \tEmb_Norm: 13.93 (13.94)\n",
      "Train Epoch: 8 [1280/251450]\tLoss: 0.0123 (0.0086) \tAcc: 96.88% (98.40%) \tEmb_Norm: 13.91 (13.93)\n",
      "Train Epoch: 8 [1920/251450]\tLoss: 0.0078 (0.0092) \tAcc: 100.00% (98.31%) \tEmb_Norm: 13.89 (13.92)\n",
      "Train Epoch: 8 [2560/251450]\tLoss: 0.0000 (0.0090) \tAcc: 100.00% (98.42%) \tEmb_Norm: 13.87 (13.91)\n",
      "Train Epoch: 8 [3200/251450]\tLoss: 0.0016 (0.0094) \tAcc: 100.00% (98.24%) \tEmb_Norm: 13.86 (13.90)\n",
      "Train Epoch: 8 [3840/251450]\tLoss: 0.0116 (0.0103) \tAcc: 96.88% (97.86%) \tEmb_Norm: 13.84 (13.89)\n",
      "Train Epoch: 8 [4480/251450]\tLoss: 0.0059 (0.0100) \tAcc: 100.00% (97.87%) \tEmb_Norm: 13.82 (13.88)\n",
      "Train Epoch: 8 [5120/251450]\tLoss: 0.0054 (0.0099) \tAcc: 100.00% (97.94%) \tEmb_Norm: 13.80 (13.87)\n",
      "Train Epoch: 8 [5760/251450]\tLoss: 0.0514 (0.0106) \tAcc: 90.62% (97.82%) \tEmb_Norm: 13.78 (13.86)\n",
      "Train Epoch: 8 [6400/251450]\tLoss: 0.0027 (0.0107) \tAcc: 100.00% (97.87%) \tEmb_Norm: 13.76 (13.86)\n",
      "Train Epoch: 8 [7040/251450]\tLoss: 0.0000 (0.0106) \tAcc: 100.00% (97.95%) \tEmb_Norm: 13.74 (13.85)\n",
      "Train Epoch: 8 [7680/251450]\tLoss: 0.0000 (0.0104) \tAcc: 100.00% (97.96%) \tEmb_Norm: 13.72 (13.84)\n",
      "Train Epoch: 8 [8320/251450]\tLoss: 0.0000 (0.0103) \tAcc: 100.00% (97.96%) \tEmb_Norm: 13.70 (13.83)\n",
      "Train Epoch: 8 [8960/251450]\tLoss: 0.0113 (0.0106) \tAcc: 96.88% (97.90%) \tEmb_Norm: 13.68 (13.82)\n",
      "Train Epoch: 8 [9600/251450]\tLoss: 0.0053 (0.0105) \tAcc: 100.00% (97.92%) \tEmb_Norm: 13.67 (13.81)\n",
      "Train Epoch: 8 [10240/251450]\tLoss: 0.0066 (0.0106) \tAcc: 96.88% (97.90%) \tEmb_Norm: 13.65 (13.80)\n",
      "Train Epoch: 8 [10880/251450]\tLoss: 0.0071 (0.0105) \tAcc: 100.00% (97.90%) \tEmb_Norm: 13.63 (13.79)\n",
      "Train Epoch: 8 [11520/251450]\tLoss: 0.0207 (0.0104) \tAcc: 96.88% (97.91%) \tEmb_Norm: 13.61 (13.78)\n",
      "Train Epoch: 8 [12160/251450]\tLoss: 0.0235 (0.0103) \tAcc: 93.75% (97.93%) \tEmb_Norm: 13.59 (13.77)\n",
      "Train Epoch: 8 [12800/251450]\tLoss: 0.0299 (0.0102) \tAcc: 93.75% (97.97%) \tEmb_Norm: 13.57 (13.76)\n",
      "Train Epoch: 8 [13440/251450]\tLoss: 0.0036 (0.0103) \tAcc: 100.00% (97.94%) \tEmb_Norm: 13.56 (13.75)\n",
      "Train Epoch: 8 [14080/251450]\tLoss: 0.0369 (0.0104) \tAcc: 93.75% (97.95%) \tEmb_Norm: 13.54 (13.74)\n",
      "Train Epoch: 8 [14720/251450]\tLoss: 0.0116 (0.0105) \tAcc: 96.88% (97.91%) \tEmb_Norm: 13.52 (13.73)\n",
      "Train Epoch: 8 [15360/251450]\tLoss: 0.0051 (0.0103) \tAcc: 100.00% (97.93%) \tEmb_Norm: 13.50 (13.72)\n",
      "Train Epoch: 8 [16000/251450]\tLoss: 0.0000 (0.0104) \tAcc: 100.00% (97.92%) \tEmb_Norm: 13.48 (13.71)\n",
      "Train Epoch: 8 [16640/251450]\tLoss: 0.0007 (0.0104) \tAcc: 100.00% (97.91%) \tEmb_Norm: 13.46 (13.71)\n",
      "Train Epoch: 8 [17280/251450]\tLoss: 0.0019 (0.0104) \tAcc: 100.00% (97.93%) \tEmb_Norm: 13.45 (13.70)\n",
      "Train Epoch: 8 [17920/251450]\tLoss: 0.0111 (0.0105) \tAcc: 100.00% (97.92%) \tEmb_Norm: 13.43 (13.69)\n",
      "Train Epoch: 8 [18560/251450]\tLoss: 0.0040 (0.0105) \tAcc: 100.00% (97.92%) \tEmb_Norm: 13.41 (13.68)\n",
      "Train Epoch: 8 [19200/251450]\tLoss: 0.0040 (0.0104) \tAcc: 100.00% (97.95%) \tEmb_Norm: 13.39 (13.67)\n",
      "Train Epoch: 8 [19840/251450]\tLoss: 0.0000 (0.0103) \tAcc: 100.00% (97.98%) \tEmb_Norm: 13.37 (13.66)\n",
      "Train Epoch: 8 [20480/251450]\tLoss: 0.0305 (0.0104) \tAcc: 93.75% (97.97%) \tEmb_Norm: 13.35 (13.65)\n",
      "Train Epoch: 8 [21120/251450]\tLoss: 0.0042 (0.0104) \tAcc: 100.00% (97.99%) \tEmb_Norm: 13.33 (13.64)\n",
      "Train Epoch: 8 [21760/251450]\tLoss: 0.0046 (0.0103) \tAcc: 100.00% (98.01%) \tEmb_Norm: 13.31 (13.63)\n",
      "Train Epoch: 8 [22400/251450]\tLoss: 0.0022 (0.0102) \tAcc: 100.00% (98.04%) \tEmb_Norm: 13.30 (13.62)\n",
      "Train Epoch: 8 [23040/251450]\tLoss: 0.0118 (0.0101) \tAcc: 96.88% (98.06%) \tEmb_Norm: 13.28 (13.61)\n",
      "Train Epoch: 8 [23680/251450]\tLoss: 0.0046 (0.0102) \tAcc: 100.00% (98.06%) \tEmb_Norm: 13.26 (13.60)\n",
      "Train Epoch: 8 [24320/251450]\tLoss: 0.0176 (0.0103) \tAcc: 96.88% (98.03%) \tEmb_Norm: 13.24 (13.59)\n",
      "Train Epoch: 8 [24960/251450]\tLoss: 0.0616 (0.0103) \tAcc: 90.62% (98.04%) \tEmb_Norm: 13.22 (13.58)\n",
      "Train Epoch: 8 [25600/251450]\tLoss: 0.0000 (0.0103) \tAcc: 100.00% (98.06%) \tEmb_Norm: 13.20 (13.58)\n",
      "Train Epoch: 8 [26240/251450]\tLoss: 0.0031 (0.0102) \tAcc: 100.00% (98.07%) \tEmb_Norm: 13.19 (13.57)\n",
      "Train Epoch: 8 [26880/251450]\tLoss: 0.0151 (0.0101) \tAcc: 100.00% (98.08%) \tEmb_Norm: 13.17 (13.56)\n",
      "Train Epoch: 8 [27520/251450]\tLoss: 0.0025 (0.0100) \tAcc: 100.00% (98.11%) \tEmb_Norm: 13.15 (13.55)\n",
      "Train Epoch: 8 [28160/251450]\tLoss: 0.0200 (0.0100) \tAcc: 96.88% (98.12%) \tEmb_Norm: 13.13 (13.54)\n",
      "Train Epoch: 8 [28800/251450]\tLoss: 0.0047 (0.0100) \tAcc: 100.00% (98.11%) \tEmb_Norm: 13.11 (13.53)\n",
      "Train Epoch: 8 [29440/251450]\tLoss: 0.0223 (0.0100) \tAcc: 96.88% (98.09%) \tEmb_Norm: 13.09 (13.52)\n",
      "Train Epoch: 8 [30080/251450]\tLoss: 0.0181 (0.0101) \tAcc: 96.88% (98.07%) \tEmb_Norm: 13.07 (13.51)\n",
      "Train Epoch: 8 [30720/251450]\tLoss: 0.0033 (0.0101) \tAcc: 100.00% (98.07%) \tEmb_Norm: 13.05 (13.50)\n",
      "Train Epoch: 8 [31360/251450]\tLoss: 0.0203 (0.0100) \tAcc: 96.88% (98.09%) \tEmb_Norm: 13.04 (13.49)\n",
      "Train Epoch: 8 [32000/251450]\tLoss: 0.0155 (0.0100) \tAcc: 96.88% (98.08%) \tEmb_Norm: 13.02 (13.48)\n",
      "Train Epoch: 8 [32640/251450]\tLoss: 0.0020 (0.0100) \tAcc: 100.00% (98.09%) \tEmb_Norm: 13.00 (13.47)\n",
      "Train Epoch: 8 [33280/251450]\tLoss: 0.0102 (0.0100) \tAcc: 96.88% (98.08%) \tEmb_Norm: 12.98 (13.46)\n",
      "Train Epoch: 8 [33920/251450]\tLoss: 0.0552 (0.0101) \tAcc: 90.62% (98.07%) \tEmb_Norm: 12.96 (13.45)\n",
      "Train Epoch: 8 [34560/251450]\tLoss: 0.0054 (0.0101) \tAcc: 100.00% (98.07%) \tEmb_Norm: 12.94 (13.45)\n",
      "Train Epoch: 8 [35200/251450]\tLoss: 0.0081 (0.0102) \tAcc: 100.00% (98.06%) \tEmb_Norm: 12.92 (13.44)\n",
      "Train Epoch: 8 [35840/251450]\tLoss: 0.0000 (0.0102) \tAcc: 100.00% (98.06%) \tEmb_Norm: 12.90 (13.43)\n",
      "Train Epoch: 8 [36480/251450]\tLoss: 0.0067 (0.0101) \tAcc: 100.00% (98.07%) \tEmb_Norm: 12.89 (13.42)\n",
      "Train Epoch: 8 [37120/251450]\tLoss: 0.0007 (0.0101) \tAcc: 100.00% (98.07%) \tEmb_Norm: 12.87 (13.41)\n",
      "Train Epoch: 8 [37760/251450]\tLoss: 0.0124 (0.0101) \tAcc: 96.88% (98.06%) \tEmb_Norm: 12.85 (13.40)\n",
      "Train Epoch: 8 [38400/251450]\tLoss: 0.0131 (0.0102) \tAcc: 96.88% (98.06%) \tEmb_Norm: 12.83 (13.39)\n",
      "Train Epoch: 8 [39040/251450]\tLoss: 0.0188 (0.0102) \tAcc: 96.88% (98.05%) \tEmb_Norm: 12.81 (13.38)\n",
      "Train Epoch: 8 [39680/251450]\tLoss: 0.0000 (0.0102) \tAcc: 100.00% (98.05%) \tEmb_Norm: 12.79 (13.37)\n",
      "Train Epoch: 8 [40320/251450]\tLoss: 0.0267 (0.0102) \tAcc: 93.75% (98.05%) \tEmb_Norm: 12.77 (13.36)\n",
      "Train Epoch: 8 [40960/251450]\tLoss: 0.0000 (0.0102) \tAcc: 100.00% (98.05%) \tEmb_Norm: 12.75 (13.35)\n",
      "Train Epoch: 8 [41600/251450]\tLoss: 0.0294 (0.0102) \tAcc: 93.75% (98.04%) \tEmb_Norm: 12.74 (13.34)\n",
      "Train Epoch: 8 [42240/251450]\tLoss: 0.0272 (0.0102) \tAcc: 93.75% (98.05%) \tEmb_Norm: 12.72 (13.33)\n",
      "Train Epoch: 8 [42880/251450]\tLoss: 0.0149 (0.0103) \tAcc: 96.88% (98.04%) \tEmb_Norm: 12.70 (13.32)\n",
      "Train Epoch: 8 [43520/251450]\tLoss: 0.0000 (0.0103) \tAcc: 100.00% (98.04%) \tEmb_Norm: 12.68 (13.31)\n",
      "Train Epoch: 8 [44160/251450]\tLoss: 0.0029 (0.0102) \tAcc: 100.00% (98.05%) \tEmb_Norm: 12.66 (13.31)\n",
      "Train Epoch: 8 [44800/251450]\tLoss: 0.0374 (0.0102) \tAcc: 93.75% (98.05%) \tEmb_Norm: 12.64 (13.30)\n",
      "Train Epoch: 8 [45440/251450]\tLoss: 0.0094 (0.0103) \tAcc: 96.88% (98.04%) \tEmb_Norm: 12.62 (13.29)\n",
      "Train Epoch: 8 [46080/251450]\tLoss: 0.0161 (0.0103) \tAcc: 96.88% (98.02%) \tEmb_Norm: 12.60 (13.28)\n",
      "Train Epoch: 8 [46720/251450]\tLoss: 0.0269 (0.0103) \tAcc: 93.75% (98.02%) \tEmb_Norm: 12.59 (13.27)\n",
      "Train Epoch: 8 [47360/251450]\tLoss: 0.0175 (0.0103) \tAcc: 96.88% (98.02%) \tEmb_Norm: 12.57 (13.26)\n",
      "Train Epoch: 8 [48000/251450]\tLoss: 0.0156 (0.0103) \tAcc: 93.75% (98.02%) \tEmb_Norm: 12.55 (13.25)\n",
      "Train Epoch: 8 [48640/251450]\tLoss: 0.0012 (0.0103) \tAcc: 100.00% (98.02%) \tEmb_Norm: 12.53 (13.24)\n",
      "Train Epoch: 8 [49280/251450]\tLoss: 0.0142 (0.0103) \tAcc: 93.75% (98.03%) \tEmb_Norm: 12.51 (13.23)\n",
      "Train Epoch: 8 [49920/251450]\tLoss: 0.0143 (0.0103) \tAcc: 96.88% (98.03%) \tEmb_Norm: 12.49 (13.22)\n",
      "Train Epoch: 8 [50560/251450]\tLoss: 0.0174 (0.0103) \tAcc: 96.88% (98.03%) \tEmb_Norm: 12.48 (13.21)\n",
      "Train Epoch: 8 [51200/251450]\tLoss: 0.0043 (0.0103) \tAcc: 100.00% (98.03%) \tEmb_Norm: 12.46 (13.20)\n",
      "Train Epoch: 8 [51840/251450]\tLoss: 0.0262 (0.0103) \tAcc: 93.75% (98.03%) \tEmb_Norm: 12.44 (13.19)\n",
      "Train Epoch: 8 [52480/251450]\tLoss: 0.0082 (0.0103) \tAcc: 96.88% (98.03%) \tEmb_Norm: 12.42 (13.18)\n",
      "Train Epoch: 8 [53120/251450]\tLoss: 0.0028 (0.0102) \tAcc: 100.00% (98.04%) \tEmb_Norm: 12.40 (13.17)\n",
      "Train Epoch: 8 [53760/251450]\tLoss: 0.0000 (0.0102) \tAcc: 100.00% (98.04%) \tEmb_Norm: 12.39 (13.17)\n",
      "Train Epoch: 8 [54400/251450]\tLoss: 0.0000 (0.0103) \tAcc: 100.00% (98.03%) \tEmb_Norm: 12.37 (13.16)\n",
      "Train Epoch: 8 [55040/251450]\tLoss: 0.0145 (0.0103) \tAcc: 96.88% (98.02%) \tEmb_Norm: 12.35 (13.15)\n",
      "Train Epoch: 8 [55680/251450]\tLoss: 0.0217 (0.0103) \tAcc: 93.75% (98.02%) \tEmb_Norm: 12.33 (13.14)\n",
      "Train Epoch: 8 [56320/251450]\tLoss: 0.0283 (0.0103) \tAcc: 93.75% (98.01%) \tEmb_Norm: 12.31 (13.13)\n",
      "Train Epoch: 8 [56960/251450]\tLoss: 0.0282 (0.0104) \tAcc: 93.75% (98.00%) \tEmb_Norm: 12.29 (13.12)\n",
      "Train Epoch: 8 [57600/251450]\tLoss: 0.0016 (0.0104) \tAcc: 100.00% (98.00%) \tEmb_Norm: 12.27 (13.11)\n",
      "Train Epoch: 8 [58240/251450]\tLoss: 0.0178 (0.0104) \tAcc: 96.88% (97.99%) \tEmb_Norm: 12.25 (13.10)\n",
      "Train Epoch: 8 [58880/251450]\tLoss: 0.0054 (0.0104) \tAcc: 100.00% (97.99%) \tEmb_Norm: 12.23 (13.09)\n",
      "Train Epoch: 8 [59520/251450]\tLoss: 0.0196 (0.0104) \tAcc: 96.88% (97.98%) \tEmb_Norm: 12.21 (13.08)\n",
      "Train Epoch: 8 [60160/251450]\tLoss: 0.0000 (0.0105) \tAcc: 100.00% (97.98%) \tEmb_Norm: 12.20 (13.07)\n",
      "Train Epoch: 8 [60800/251450]\tLoss: 0.0000 (0.0105) \tAcc: 100.00% (97.97%) \tEmb_Norm: 12.18 (13.06)\n",
      "Train Epoch: 8 [61440/251450]\tLoss: 0.0144 (0.0105) \tAcc: 96.88% (97.97%) \tEmb_Norm: 12.16 (13.05)\n",
      "Train Epoch: 8 [62080/251450]\tLoss: 0.0151 (0.0106) \tAcc: 93.75% (97.97%) \tEmb_Norm: 12.14 (13.04)\n",
      "Train Epoch: 8 [62720/251450]\tLoss: 0.0000 (0.0106) \tAcc: 100.00% (97.97%) \tEmb_Norm: 12.12 (13.03)\n",
      "Train Epoch: 8 [63360/251450]\tLoss: 0.0104 (0.0106) \tAcc: 100.00% (97.96%) \tEmb_Norm: 12.10 (13.03)\n",
      "Train Epoch: 8 [64000/251450]\tLoss: 0.0251 (0.0106) \tAcc: 96.88% (97.96%) \tEmb_Norm: 12.08 (13.02)\n",
      "Train Epoch: 8 [64640/251450]\tLoss: 0.0081 (0.0106) \tAcc: 96.88% (97.97%) \tEmb_Norm: 12.07 (13.01)\n",
      "Train Epoch: 8 [65280/251450]\tLoss: 0.0000 (0.0106) \tAcc: 100.00% (97.96%) \tEmb_Norm: 12.05 (13.00)\n",
      "Train Epoch: 8 [65920/251450]\tLoss: 0.0028 (0.0105) \tAcc: 100.00% (97.97%) \tEmb_Norm: 12.03 (12.99)\n",
      "Train Epoch: 8 [66560/251450]\tLoss: 0.0107 (0.0105) \tAcc: 96.88% (97.98%) \tEmb_Norm: 12.01 (12.98)\n",
      "Train Epoch: 8 [67200/251450]\tLoss: 0.0033 (0.0105) \tAcc: 100.00% (97.98%) \tEmb_Norm: 11.99 (12.97)\n",
      "Train Epoch: 8 [67840/251450]\tLoss: 0.0033 (0.0105) \tAcc: 100.00% (97.98%) \tEmb_Norm: 11.97 (12.96)\n",
      "Train Epoch: 8 [68480/251450]\tLoss: 0.0007 (0.0105) \tAcc: 100.00% (97.99%) \tEmb_Norm: 11.95 (12.95)\n",
      "Train Epoch: 8 [69120/251450]\tLoss: 0.0000 (0.0105) \tAcc: 100.00% (97.99%) \tEmb_Norm: 11.94 (12.94)\n",
      "Train Epoch: 8 [69760/251450]\tLoss: 0.0255 (0.0105) \tAcc: 93.75% (97.99%) \tEmb_Norm: 11.92 (12.93)\n",
      "Train Epoch: 8 [70400/251450]\tLoss: 0.0070 (0.0105) \tAcc: 100.00% (97.98%) \tEmb_Norm: 11.90 (12.92)\n",
      "Train Epoch: 8 [71040/251450]\tLoss: 0.0087 (0.0105) \tAcc: 100.00% (97.99%) \tEmb_Norm: 11.88 (12.91)\n",
      "Train Epoch: 8 [71680/251450]\tLoss: 0.0053 (0.0105) \tAcc: 100.00% (97.99%) \tEmb_Norm: 11.86 (12.90)\n",
      "Train Epoch: 8 [72320/251450]\tLoss: 0.0187 (0.0105) \tAcc: 96.88% (98.00%) \tEmb_Norm: 11.85 (12.90)\n",
      "Train Epoch: 8 [72960/251450]\tLoss: 0.0092 (0.0105) \tAcc: 96.88% (97.99%) \tEmb_Norm: 11.83 (12.89)\n",
      "Train Epoch: 8 [73600/251450]\tLoss: 0.0442 (0.0105) \tAcc: 93.75% (97.99%) \tEmb_Norm: 11.81 (12.88)\n",
      "Train Epoch: 8 [74240/251450]\tLoss: 0.0043 (0.0105) \tAcc: 100.00% (97.99%) \tEmb_Norm: 11.79 (12.87)\n",
      "Train Epoch: 8 [74880/251450]\tLoss: 0.0110 (0.0105) \tAcc: 96.88% (97.99%) \tEmb_Norm: 11.77 (12.86)\n",
      "Train Epoch: 8 [75520/251450]\tLoss: 0.0059 (0.0105) \tAcc: 100.00% (97.99%) \tEmb_Norm: 11.76 (12.85)\n",
      "Train Epoch: 8 [76160/251450]\tLoss: 0.0056 (0.0105) \tAcc: 100.00% (98.00%) \tEmb_Norm: 11.74 (12.84)\n",
      "Train Epoch: 8 [76800/251450]\tLoss: 0.0000 (0.0105) \tAcc: 100.00% (98.00%) \tEmb_Norm: 11.72 (12.83)\n",
      "Train Epoch: 8 [77440/251450]\tLoss: 0.0064 (0.0105) \tAcc: 100.00% (98.00%) \tEmb_Norm: 11.70 (12.82)\n",
      "Train Epoch: 8 [78080/251450]\tLoss: 0.0033 (0.0105) \tAcc: 100.00% (98.00%) \tEmb_Norm: 11.68 (12.81)\n",
      "Train Epoch: 8 [78720/251450]\tLoss: 0.0047 (0.0105) \tAcc: 100.00% (97.99%) \tEmb_Norm: 11.67 (12.80)\n",
      "Train Epoch: 8 [79360/251450]\tLoss: 0.0478 (0.0105) \tAcc: 90.62% (97.99%) \tEmb_Norm: 11.65 (12.79)\n",
      "Train Epoch: 8 [80000/251450]\tLoss: 0.0181 (0.0105) \tAcc: 96.88% (97.99%) \tEmb_Norm: 11.63 (12.78)\n",
      "Train Epoch: 8 [80640/251450]\tLoss: 0.0196 (0.0106) \tAcc: 100.00% (97.98%) \tEmb_Norm: 11.61 (12.77)\n",
      "Train Epoch: 8 [81280/251450]\tLoss: 0.0060 (0.0106) \tAcc: 100.00% (97.98%) \tEmb_Norm: 11.59 (12.77)\n",
      "Train Epoch: 8 [81920/251450]\tLoss: 0.0178 (0.0105) \tAcc: 93.75% (97.98%) \tEmb_Norm: 11.57 (12.76)\n",
      "Train Epoch: 8 [82560/251450]\tLoss: 0.0139 (0.0106) \tAcc: 96.88% (97.97%) \tEmb_Norm: 11.55 (12.75)\n",
      "Train Epoch: 8 [83200/251450]\tLoss: 0.0250 (0.0106) \tAcc: 93.75% (97.97%) \tEmb_Norm: 11.54 (12.74)\n",
      "Train Epoch: 8 [83840/251450]\tLoss: 0.0365 (0.0106) \tAcc: 90.62% (97.96%) \tEmb_Norm: 11.52 (12.73)\n",
      "Train Epoch: 8 [84480/251450]\tLoss: 0.0114 (0.0106) \tAcc: 96.88% (97.96%) \tEmb_Norm: 11.50 (12.72)\n",
      "Train Epoch: 8 [85120/251450]\tLoss: 0.0241 (0.0106) \tAcc: 96.88% (97.96%) \tEmb_Norm: 11.48 (12.71)\n",
      "Train Epoch: 8 [85760/251450]\tLoss: 0.0602 (0.0106) \tAcc: 93.75% (97.96%) \tEmb_Norm: 11.46 (12.70)\n",
      "Train Epoch: 8 [86400/251450]\tLoss: 0.0079 (0.0106) \tAcc: 100.00% (97.96%) \tEmb_Norm: 11.44 (12.69)\n",
      "Train Epoch: 8 [87040/251450]\tLoss: 0.0014 (0.0106) \tAcc: 100.00% (97.97%) \tEmb_Norm: 11.43 (12.68)\n",
      "Train Epoch: 8 [87680/251450]\tLoss: 0.0084 (0.0106) \tAcc: 96.88% (97.97%) \tEmb_Norm: 11.41 (12.67)\n",
      "Train Epoch: 8 [88320/251450]\tLoss: 0.0186 (0.0106) \tAcc: 96.88% (97.96%) \tEmb_Norm: 11.39 (12.66)\n",
      "Train Epoch: 8 [88960/251450]\tLoss: 0.0134 (0.0107) \tAcc: 96.88% (97.96%) \tEmb_Norm: 11.37 (12.65)\n",
      "Train Epoch: 8 [89600/251450]\tLoss: 0.0109 (0.0107) \tAcc: 100.00% (97.96%) \tEmb_Norm: 11.35 (12.65)\n",
      "Train Epoch: 8 [90240/251450]\tLoss: 0.0066 (0.0106) \tAcc: 100.00% (97.96%) \tEmb_Norm: 11.33 (12.64)\n",
      "Train Epoch: 8 [90880/251450]\tLoss: 0.0018 (0.0106) \tAcc: 100.00% (97.96%) \tEmb_Norm: 11.31 (12.63)\n",
      "Train Epoch: 8 [91520/251450]\tLoss: 0.0018 (0.0106) \tAcc: 100.00% (97.97%) \tEmb_Norm: 11.30 (12.62)\n",
      "Train Epoch: 8 [92160/251450]\tLoss: 0.0035 (0.0106) \tAcc: 100.00% (97.98%) \tEmb_Norm: 11.28 (12.61)\n",
      "Train Epoch: 8 [92800/251450]\tLoss: 0.0139 (0.0106) \tAcc: 96.88% (97.97%) \tEmb_Norm: 11.26 (12.60)\n",
      "Train Epoch: 8 [93440/251450]\tLoss: 0.0109 (0.0106) \tAcc: 96.88% (97.97%) \tEmb_Norm: 11.24 (12.59)\n",
      "Train Epoch: 8 [94080/251450]\tLoss: 0.0007 (0.0106) \tAcc: 100.00% (97.97%) \tEmb_Norm: 11.22 (12.58)\n",
      "Train Epoch: 8 [94720/251450]\tLoss: 0.0165 (0.0106) \tAcc: 96.88% (97.97%) \tEmb_Norm: 11.21 (12.57)\n",
      "Train Epoch: 8 [95360/251450]\tLoss: 0.0029 (0.0106) \tAcc: 100.00% (97.97%) \tEmb_Norm: 11.19 (12.56)\n",
      "Train Epoch: 8 [96000/251450]\tLoss: 0.0103 (0.0106) \tAcc: 96.88% (97.97%) \tEmb_Norm: 11.17 (12.55)\n",
      "Train Epoch: 8 [96640/251450]\tLoss: 0.0141 (0.0106) \tAcc: 96.88% (97.98%) \tEmb_Norm: 11.15 (12.54)\n",
      "Train Epoch: 8 [97280/251450]\tLoss: 0.0385 (0.0106) \tAcc: 93.75% (97.97%) \tEmb_Norm: 11.13 (12.53)\n",
      "Train Epoch: 8 [97920/251450]\tLoss: 0.0139 (0.0106) \tAcc: 96.88% (97.97%) \tEmb_Norm: 11.12 (12.53)\n",
      "Train Epoch: 8 [98560/251450]\tLoss: 0.0121 (0.0106) \tAcc: 96.88% (97.97%) \tEmb_Norm: 11.10 (12.52)\n",
      "Train Epoch: 8 [99200/251450]\tLoss: 0.0058 (0.0106) \tAcc: 100.00% (97.97%) \tEmb_Norm: 11.08 (12.51)\n",
      "Train Epoch: 8 [99840/251450]\tLoss: 0.0095 (0.0106) \tAcc: 96.88% (97.96%) \tEmb_Norm: 11.06 (12.50)\n",
      "Train Epoch: 8 [100480/251450]\tLoss: 0.0093 (0.0106) \tAcc: 96.88% (97.96%) \tEmb_Norm: 11.05 (12.49)\n",
      "Train Epoch: 8 [101120/251450]\tLoss: 0.0012 (0.0106) \tAcc: 100.00% (97.96%) \tEmb_Norm: 11.03 (12.48)\n",
      "Train Epoch: 8 [101760/251450]\tLoss: 0.0024 (0.0106) \tAcc: 100.00% (97.96%) \tEmb_Norm: 11.01 (12.47)\n",
      "Train Epoch: 8 [102400/251450]\tLoss: 0.0207 (0.0106) \tAcc: 93.75% (97.96%) \tEmb_Norm: 10.99 (12.46)\n",
      "Train Epoch: 8 [103040/251450]\tLoss: 0.0312 (0.0106) \tAcc: 93.75% (97.96%) \tEmb_Norm: 10.97 (12.45)\n",
      "Train Epoch: 8 [103680/251450]\tLoss: 0.0165 (0.0107) \tAcc: 93.75% (97.95%) \tEmb_Norm: 10.96 (12.44)\n",
      "Train Epoch: 8 [104320/251450]\tLoss: 0.0266 (0.0106) \tAcc: 93.75% (97.95%) \tEmb_Norm: 10.94 (12.43)\n",
      "Train Epoch: 8 [104960/251450]\tLoss: 0.0000 (0.0107) \tAcc: 100.00% (97.95%) \tEmb_Norm: 10.92 (12.42)\n",
      "Train Epoch: 8 [105600/251450]\tLoss: 0.0029 (0.0107) \tAcc: 100.00% (97.94%) \tEmb_Norm: 10.90 (12.42)\n",
      "Train Epoch: 8 [106240/251450]\tLoss: 0.0126 (0.0107) \tAcc: 96.88% (97.95%) \tEmb_Norm: 10.88 (12.41)\n",
      "Train Epoch: 8 [106880/251450]\tLoss: 0.0142 (0.0107) \tAcc: 96.88% (97.95%) \tEmb_Norm: 10.87 (12.40)\n",
      "Train Epoch: 8 [107520/251450]\tLoss: 0.0331 (0.0107) \tAcc: 93.75% (97.95%) \tEmb_Norm: 10.85 (12.39)\n",
      "Train Epoch: 8 [108160/251450]\tLoss: 0.0194 (0.0107) \tAcc: 96.88% (97.95%) \tEmb_Norm: 10.83 (12.38)\n",
      "Train Epoch: 8 [108800/251450]\tLoss: 0.0128 (0.0107) \tAcc: 96.88% (97.95%) \tEmb_Norm: 10.81 (12.37)\n",
      "Train Epoch: 8 [109440/251450]\tLoss: 0.0165 (0.0106) \tAcc: 96.88% (97.96%) \tEmb_Norm: 10.79 (12.36)\n",
      "Train Epoch: 8 [110080/251450]\tLoss: 0.0080 (0.0106) \tAcc: 96.88% (97.96%) \tEmb_Norm: 10.77 (12.35)\n",
      "Train Epoch: 8 [110720/251450]\tLoss: 0.0065 (0.0106) \tAcc: 96.88% (97.96%) \tEmb_Norm: 10.76 (12.34)\n",
      "Train Epoch: 8 [111360/251450]\tLoss: 0.0037 (0.0106) \tAcc: 100.00% (97.96%) \tEmb_Norm: 10.74 (12.33)\n",
      "Train Epoch: 8 [112000/251450]\tLoss: 0.0007 (0.0106) \tAcc: 100.00% (97.96%) \tEmb_Norm: 10.72 (12.32)\n",
      "Train Epoch: 8 [112640/251450]\tLoss: 0.0032 (0.0106) \tAcc: 100.00% (97.97%) \tEmb_Norm: 10.70 (12.31)\n",
      "Train Epoch: 8 [113280/251450]\tLoss: 0.0240 (0.0106) \tAcc: 93.75% (97.97%) \tEmb_Norm: 10.69 (12.31)\n",
      "Train Epoch: 8 [113920/251450]\tLoss: 0.0020 (0.0106) \tAcc: 100.00% (97.97%) \tEmb_Norm: 10.67 (12.30)\n",
      "Train Epoch: 8 [114560/251450]\tLoss: 0.0082 (0.0106) \tAcc: 96.88% (97.97%) \tEmb_Norm: 10.65 (12.29)\n",
      "Train Epoch: 8 [115200/251450]\tLoss: 0.0268 (0.0106) \tAcc: 96.88% (97.97%) \tEmb_Norm: 10.63 (12.28)\n",
      "Train Epoch: 8 [115840/251450]\tLoss: 0.0102 (0.0106) \tAcc: 96.88% (97.97%) \tEmb_Norm: 10.62 (12.27)\n",
      "Train Epoch: 8 [116480/251450]\tLoss: 0.0063 (0.0106) \tAcc: 100.00% (97.97%) \tEmb_Norm: 10.60 (12.26)\n",
      "Train Epoch: 8 [117120/251450]\tLoss: 0.0068 (0.0106) \tAcc: 100.00% (97.97%) \tEmb_Norm: 10.58 (12.25)\n",
      "Train Epoch: 8 [117760/251450]\tLoss: 0.0056 (0.0106) \tAcc: 100.00% (97.98%) \tEmb_Norm: 10.56 (12.24)\n",
      "Train Epoch: 8 [118400/251450]\tLoss: 0.0000 (0.0106) \tAcc: 100.00% (97.98%) \tEmb_Norm: 10.55 (12.23)\n",
      "Train Epoch: 8 [119040/251450]\tLoss: 0.0111 (0.0106) \tAcc: 96.88% (97.97%) \tEmb_Norm: 10.53 (12.22)\n",
      "Train Epoch: 8 [119680/251450]\tLoss: 0.0061 (0.0106) \tAcc: 100.00% (97.97%) \tEmb_Norm: 10.51 (12.21)\n",
      "Train Epoch: 8 [120320/251450]\tLoss: 0.0170 (0.0106) \tAcc: 96.88% (97.97%) \tEmb_Norm: 10.49 (12.20)\n",
      "Train Epoch: 8 [120960/251450]\tLoss: 0.0129 (0.0106) \tAcc: 96.88% (97.97%) \tEmb_Norm: 10.48 (12.20)\n",
      "Train Epoch: 8 [121600/251450]\tLoss: 0.0087 (0.0106) \tAcc: 96.88% (97.97%) \tEmb_Norm: 10.46 (12.19)\n",
      "Train Epoch: 8 [122240/251450]\tLoss: 0.0026 (0.0106) \tAcc: 100.00% (97.97%) \tEmb_Norm: 10.44 (12.18)\n",
      "Train Epoch: 8 [122880/251450]\tLoss: 0.0025 (0.0106) \tAcc: 100.00% (97.97%) \tEmb_Norm: 10.42 (12.17)\n",
      "Train Epoch: 8 [123520/251450]\tLoss: 0.0004 (0.0105) \tAcc: 100.00% (97.97%) \tEmb_Norm: 10.41 (12.16)\n",
      "Train Epoch: 8 [124160/251450]\tLoss: 0.0000 (0.0105) \tAcc: 100.00% (97.97%) \tEmb_Norm: 10.39 (12.15)\n",
      "Train Epoch: 8 [124800/251450]\tLoss: 0.0081 (0.0105) \tAcc: 100.00% (97.97%) \tEmb_Norm: 10.37 (12.14)\n",
      "Train Epoch: 8 [125440/251450]\tLoss: 0.0000 (0.0105) \tAcc: 100.00% (97.97%) \tEmb_Norm: 10.35 (12.13)\n",
      "Train Epoch: 8 [126080/251450]\tLoss: 0.0000 (0.0105) \tAcc: 100.00% (97.98%) \tEmb_Norm: 10.33 (12.12)\n",
      "Train Epoch: 8 [126720/251450]\tLoss: 0.0090 (0.0105) \tAcc: 96.88% (97.98%) \tEmb_Norm: 10.32 (12.11)\n",
      "Train Epoch: 8 [127360/251450]\tLoss: 0.0039 (0.0105) \tAcc: 100.00% (97.98%) \tEmb_Norm: 10.30 (12.10)\n",
      "Train Epoch: 8 [128000/251450]\tLoss: 0.0022 (0.0105) \tAcc: 100.00% (97.98%) \tEmb_Norm: 10.28 (12.10)\n",
      "Train Epoch: 8 [128640/251450]\tLoss: 0.0302 (0.0105) \tAcc: 93.75% (97.98%) \tEmb_Norm: 10.27 (12.09)\n",
      "Train Epoch: 8 [129280/251450]\tLoss: 0.0085 (0.0105) \tAcc: 100.00% (97.98%) \tEmb_Norm: 10.25 (12.08)\n",
      "Train Epoch: 8 [129920/251450]\tLoss: 0.0000 (0.0105) \tAcc: 100.00% (97.98%) \tEmb_Norm: 10.23 (12.07)\n",
      "Train Epoch: 8 [130560/251450]\tLoss: 0.0036 (0.0105) \tAcc: 100.00% (97.98%) \tEmb_Norm: 10.22 (12.06)\n",
      "Train Epoch: 8 [131200/251450]\tLoss: 0.0062 (0.0105) \tAcc: 100.00% (97.98%) \tEmb_Norm: 10.20 (12.05)\n",
      "Train Epoch: 8 [131840/251450]\tLoss: 0.0098 (0.0105) \tAcc: 96.88% (97.97%) \tEmb_Norm: 10.18 (12.04)\n",
      "Train Epoch: 8 [132480/251450]\tLoss: 0.0023 (0.0105) \tAcc: 100.00% (97.98%) \tEmb_Norm: 10.16 (12.03)\n",
      "Train Epoch: 8 [133120/251450]\tLoss: 0.0068 (0.0105) \tAcc: 100.00% (97.98%) \tEmb_Norm: 10.15 (12.02)\n",
      "Train Epoch: 8 [133760/251450]\tLoss: 0.0190 (0.0105) \tAcc: 96.88% (97.98%) \tEmb_Norm: 10.13 (12.01)\n",
      "Train Epoch: 8 [134400/251450]\tLoss: 0.0127 (0.0106) \tAcc: 96.88% (97.97%) \tEmb_Norm: 10.11 (12.01)\n",
      "Train Epoch: 8 [135040/251450]\tLoss: 0.0009 (0.0106) \tAcc: 100.00% (97.97%) \tEmb_Norm: 10.09 (12.00)\n",
      "Train Epoch: 8 [135680/251450]\tLoss: 0.0010 (0.0106) \tAcc: 100.00% (97.97%) \tEmb_Norm: 10.08 (11.99)\n",
      "Train Epoch: 8 [136320/251450]\tLoss: 0.0006 (0.0106) \tAcc: 100.00% (97.97%) \tEmb_Norm: 10.06 (11.98)\n",
      "Train Epoch: 8 [136960/251450]\tLoss: 0.0000 (0.0106) \tAcc: 100.00% (97.97%) \tEmb_Norm: 10.04 (11.97)\n",
      "Train Epoch: 8 [137600/251450]\tLoss: 0.0249 (0.0106) \tAcc: 96.88% (97.97%) \tEmb_Norm: 10.03 (11.96)\n",
      "Train Epoch: 8 [138240/251450]\tLoss: 0.0358 (0.0106) \tAcc: 90.62% (97.97%) \tEmb_Norm: 10.01 (11.95)\n",
      "Train Epoch: 8 [138880/251450]\tLoss: 0.0047 (0.0106) \tAcc: 100.00% (97.97%) \tEmb_Norm: 9.99 (11.94)\n",
      "Train Epoch: 8 [139520/251450]\tLoss: 0.0088 (0.0106) \tAcc: 100.00% (97.97%) \tEmb_Norm: 9.97 (11.93)\n",
      "Train Epoch: 8 [140160/251450]\tLoss: 0.0040 (0.0106) \tAcc: 100.00% (97.98%) \tEmb_Norm: 9.96 (11.92)\n",
      "Train Epoch: 8 [140800/251450]\tLoss: 0.0220 (0.0106) \tAcc: 93.75% (97.98%) \tEmb_Norm: 9.94 (11.92)\n",
      "Train Epoch: 8 [141440/251450]\tLoss: 0.0000 (0.0106) \tAcc: 100.00% (97.98%) \tEmb_Norm: 9.92 (11.91)\n",
      "Train Epoch: 8 [142080/251450]\tLoss: 0.0164 (0.0106) \tAcc: 96.88% (97.98%) \tEmb_Norm: 9.90 (11.90)\n",
      "Train Epoch: 8 [142720/251450]\tLoss: 0.0103 (0.0105) \tAcc: 96.88% (97.98%) \tEmb_Norm: 9.89 (11.89)\n",
      "Train Epoch: 8 [143360/251450]\tLoss: 0.0136 (0.0105) \tAcc: 96.88% (97.98%) \tEmb_Norm: 9.87 (11.88)\n",
      "Train Epoch: 8 [144000/251450]\tLoss: 0.0110 (0.0106) \tAcc: 96.88% (97.98%) \tEmb_Norm: 9.85 (11.87)\n",
      "Train Epoch: 8 [144640/251450]\tLoss: 0.0152 (0.0106) \tAcc: 93.75% (97.98%) \tEmb_Norm: 9.83 (11.86)\n",
      "Train Epoch: 8 [145280/251450]\tLoss: 0.0093 (0.0106) \tAcc: 96.88% (97.97%) \tEmb_Norm: 9.81 (11.85)\n",
      "Train Epoch: 8 [145920/251450]\tLoss: 0.0090 (0.0106) \tAcc: 96.88% (97.97%) \tEmb_Norm: 9.80 (11.84)\n",
      "Train Epoch: 8 [146560/251450]\tLoss: 0.0071 (0.0106) \tAcc: 96.88% (97.97%) \tEmb_Norm: 9.78 (11.83)\n",
      "Train Epoch: 8 [147200/251450]\tLoss: 0.0130 (0.0106) \tAcc: 96.88% (97.97%) \tEmb_Norm: 9.76 (11.83)\n",
      "Train Epoch: 8 [147840/251450]\tLoss: 0.0089 (0.0106) \tAcc: 96.88% (97.97%) \tEmb_Norm: 9.74 (11.82)\n",
      "Train Epoch: 8 [148480/251450]\tLoss: 0.0175 (0.0106) \tAcc: 93.75% (97.97%) \tEmb_Norm: 9.73 (11.81)\n",
      "Train Epoch: 8 [149120/251450]\tLoss: 0.0094 (0.0106) \tAcc: 100.00% (97.97%) \tEmb_Norm: 9.71 (11.80)\n",
      "Train Epoch: 8 [149760/251450]\tLoss: 0.0098 (0.0106) \tAcc: 100.00% (97.97%) \tEmb_Norm: 9.69 (11.79)\n",
      "Train Epoch: 8 [150400/251450]\tLoss: 0.0230 (0.0106) \tAcc: 96.88% (97.97%) \tEmb_Norm: 9.67 (11.78)\n",
      "Train Epoch: 8 [151040/251450]\tLoss: 0.0141 (0.0106) \tAcc: 96.88% (97.97%) \tEmb_Norm: 9.66 (11.77)\n",
      "Train Epoch: 8 [151680/251450]\tLoss: 0.0029 (0.0106) \tAcc: 100.00% (97.97%) \tEmb_Norm: 9.64 (11.76)\n",
      "Train Epoch: 8 [152320/251450]\tLoss: 0.0052 (0.0106) \tAcc: 100.00% (97.97%) \tEmb_Norm: 9.63 (11.75)\n",
      "Train Epoch: 8 [152960/251450]\tLoss: 0.0009 (0.0106) \tAcc: 100.00% (97.97%) \tEmb_Norm: 9.61 (11.75)\n",
      "Train Epoch: 8 [153600/251450]\tLoss: 0.0267 (0.0106) \tAcc: 93.75% (97.97%) \tEmb_Norm: 9.59 (11.74)\n",
      "Train Epoch: 8 [154240/251450]\tLoss: 0.0067 (0.0106) \tAcc: 100.00% (97.97%) \tEmb_Norm: 9.57 (11.73)\n",
      "Train Epoch: 8 [154880/251450]\tLoss: 0.0066 (0.0106) \tAcc: 100.00% (97.97%) \tEmb_Norm: 9.56 (11.72)\n",
      "Train Epoch: 8 [155520/251450]\tLoss: 0.0000 (0.0106) \tAcc: 100.00% (97.97%) \tEmb_Norm: 9.54 (11.71)\n",
      "Train Epoch: 8 [156160/251450]\tLoss: 0.0210 (0.0106) \tAcc: 96.88% (97.97%) \tEmb_Norm: 9.52 (11.70)\n",
      "Train Epoch: 8 [156800/251450]\tLoss: 0.0091 (0.0106) \tAcc: 96.88% (97.98%) \tEmb_Norm: 9.51 (11.69)\n",
      "Train Epoch: 8 [157440/251450]\tLoss: 0.0011 (0.0106) \tAcc: 100.00% (97.98%) \tEmb_Norm: 9.49 (11.68)\n",
      "Train Epoch: 8 [158080/251450]\tLoss: 0.0079 (0.0106) \tAcc: 100.00% (97.98%) \tEmb_Norm: 9.48 (11.67)\n",
      "Train Epoch: 8 [158720/251450]\tLoss: 0.0155 (0.0106) \tAcc: 96.88% (97.98%) \tEmb_Norm: 9.46 (11.66)\n",
      "Train Epoch: 8 [159360/251450]\tLoss: 0.0259 (0.0106) \tAcc: 96.88% (97.98%) \tEmb_Norm: 9.44 (11.66)\n",
      "Train Epoch: 8 [160000/251450]\tLoss: 0.0006 (0.0106) \tAcc: 100.00% (97.98%) \tEmb_Norm: 9.42 (11.65)\n",
      "Train Epoch: 8 [160640/251450]\tLoss: 0.0022 (0.0105) \tAcc: 100.00% (97.98%) \tEmb_Norm: 9.41 (11.64)\n",
      "Train Epoch: 8 [161280/251450]\tLoss: 0.0045 (0.0105) \tAcc: 100.00% (97.99%) \tEmb_Norm: 9.39 (11.63)\n",
      "Train Epoch: 8 [161920/251450]\tLoss: 0.0074 (0.0105) \tAcc: 96.88% (97.99%) \tEmb_Norm: 9.38 (11.62)\n",
      "Train Epoch: 8 [162560/251450]\tLoss: 0.0000 (0.0105) \tAcc: 100.00% (97.99%) \tEmb_Norm: 9.36 (11.61)\n",
      "Train Epoch: 8 [163200/251450]\tLoss: 0.0038 (0.0105) \tAcc: 100.00% (97.99%) \tEmb_Norm: 9.35 (11.60)\n",
      "Train Epoch: 8 [163840/251450]\tLoss: 0.0015 (0.0105) \tAcc: 100.00% (98.00%) \tEmb_Norm: 9.33 (11.59)\n",
      "Train Epoch: 8 [164480/251450]\tLoss: 0.0009 (0.0105) \tAcc: 100.00% (98.00%) \tEmb_Norm: 9.31 (11.58)\n",
      "Train Epoch: 8 [165120/251450]\tLoss: 0.0076 (0.0105) \tAcc: 96.88% (98.00%) \tEmb_Norm: 9.30 (11.58)\n",
      "Train Epoch: 8 [165760/251450]\tLoss: 0.0117 (0.0105) \tAcc: 96.88% (98.00%) \tEmb_Norm: 9.28 (11.57)\n",
      "Train Epoch: 8 [166400/251450]\tLoss: 0.0050 (0.0105) \tAcc: 100.00% (98.00%) \tEmb_Norm: 9.26 (11.56)\n",
      "Train Epoch: 8 [167040/251450]\tLoss: 0.0181 (0.0105) \tAcc: 96.88% (98.00%) \tEmb_Norm: 9.25 (11.55)\n",
      "Train Epoch: 8 [167680/251450]\tLoss: 0.0125 (0.0105) \tAcc: 96.88% (98.00%) \tEmb_Norm: 9.23 (11.54)\n",
      "Train Epoch: 8 [168320/251450]\tLoss: 0.0157 (0.0105) \tAcc: 96.88% (98.00%) \tEmb_Norm: 9.22 (11.53)\n",
      "Train Epoch: 8 [168960/251450]\tLoss: 0.0050 (0.0105) \tAcc: 100.00% (98.01%) \tEmb_Norm: 9.20 (11.52)\n",
      "Train Epoch: 8 [169600/251450]\tLoss: 0.0100 (0.0105) \tAcc: 100.00% (98.00%) \tEmb_Norm: 9.18 (11.51)\n",
      "Train Epoch: 8 [170240/251450]\tLoss: 0.0037 (0.0105) \tAcc: 100.00% (98.00%) \tEmb_Norm: 9.17 (11.51)\n",
      "Train Epoch: 8 [170880/251450]\tLoss: 0.0000 (0.0105) \tAcc: 100.00% (98.00%) \tEmb_Norm: 9.15 (11.50)\n",
      "Train Epoch: 8 [171520/251450]\tLoss: 0.0299 (0.0105) \tAcc: 96.88% (98.01%) \tEmb_Norm: 9.13 (11.49)\n",
      "Train Epoch: 8 [172160/251450]\tLoss: 0.0000 (0.0105) \tAcc: 100.00% (98.01%) \tEmb_Norm: 9.12 (11.48)\n",
      "Train Epoch: 8 [172800/251450]\tLoss: 0.0113 (0.0105) \tAcc: 100.00% (98.01%) \tEmb_Norm: 9.10 (11.47)\n",
      "Train Epoch: 8 [173440/251450]\tLoss: 0.0009 (0.0105) \tAcc: 100.00% (98.01%) \tEmb_Norm: 9.08 (11.46)\n",
      "Train Epoch: 8 [174080/251450]\tLoss: 0.0046 (0.0105) \tAcc: 100.00% (98.01%) \tEmb_Norm: 9.07 (11.45)\n",
      "Train Epoch: 8 [174720/251450]\tLoss: 0.0366 (0.0105) \tAcc: 90.62% (98.01%) \tEmb_Norm: 9.05 (11.44)\n",
      "Train Epoch: 8 [175360/251450]\tLoss: 0.0174 (0.0105) \tAcc: 96.88% (98.01%) \tEmb_Norm: 9.03 (11.44)\n",
      "Train Epoch: 8 [176000/251450]\tLoss: 0.0044 (0.0105) \tAcc: 100.00% (98.01%) \tEmb_Norm: 9.02 (11.43)\n",
      "Train Epoch: 8 [176640/251450]\tLoss: 0.0055 (0.0104) \tAcc: 100.00% (98.02%) \tEmb_Norm: 9.00 (11.42)\n",
      "Train Epoch: 8 [177280/251450]\tLoss: 0.0000 (0.0104) \tAcc: 100.00% (98.02%) \tEmb_Norm: 8.98 (11.41)\n",
      "Train Epoch: 8 [177920/251450]\tLoss: 0.0008 (0.0105) \tAcc: 100.00% (98.02%) \tEmb_Norm: 8.97 (11.40)\n",
      "Train Epoch: 8 [178560/251450]\tLoss: 0.0000 (0.0105) \tAcc: 100.00% (98.02%) \tEmb_Norm: 8.95 (11.39)\n",
      "Train Epoch: 8 [179200/251450]\tLoss: 0.0185 (0.0105) \tAcc: 96.88% (98.02%) \tEmb_Norm: 8.94 (11.38)\n",
      "Train Epoch: 8 [179840/251450]\tLoss: 0.0158 (0.0105) \tAcc: 96.88% (98.02%) \tEmb_Norm: 8.92 (11.37)\n",
      "Train Epoch: 8 [180480/251450]\tLoss: 0.0073 (0.0105) \tAcc: 96.88% (98.02%) \tEmb_Norm: 8.90 (11.37)\n",
      "Train Epoch: 8 [181120/251450]\tLoss: 0.0014 (0.0105) \tAcc: 100.00% (98.02%) \tEmb_Norm: 8.89 (11.36)\n",
      "Train Epoch: 8 [181760/251450]\tLoss: 0.0129 (0.0105) \tAcc: 96.88% (98.02%) \tEmb_Norm: 8.87 (11.35)\n",
      "Train Epoch: 8 [182400/251450]\tLoss: 0.0116 (0.0105) \tAcc: 96.88% (98.01%) \tEmb_Norm: 8.85 (11.34)\n",
      "Train Epoch: 8 [183040/251450]\tLoss: 0.0077 (0.0105) \tAcc: 100.00% (98.02%) \tEmb_Norm: 8.84 (11.33)\n",
      "Train Epoch: 8 [183680/251450]\tLoss: 0.0158 (0.0105) \tAcc: 96.88% (98.01%) \tEmb_Norm: 8.82 (11.32)\n",
      "Train Epoch: 8 [184320/251450]\tLoss: 0.0047 (0.0105) \tAcc: 100.00% (98.01%) \tEmb_Norm: 8.81 (11.31)\n",
      "Train Epoch: 8 [184960/251450]\tLoss: 0.0024 (0.0105) \tAcc: 100.00% (98.01%) \tEmb_Norm: 8.79 (11.30)\n",
      "Train Epoch: 8 [185600/251450]\tLoss: 0.0395 (0.0105) \tAcc: 93.75% (98.01%) \tEmb_Norm: 8.78 (11.30)\n",
      "Train Epoch: 8 [186240/251450]\tLoss: 0.0108 (0.0105) \tAcc: 96.88% (98.01%) \tEmb_Norm: 8.76 (11.29)\n",
      "Train Epoch: 8 [186880/251450]\tLoss: 0.0198 (0.0105) \tAcc: 93.75% (98.01%) \tEmb_Norm: 8.74 (11.28)\n",
      "Train Epoch: 8 [187520/251450]\tLoss: 0.0062 (0.0105) \tAcc: 100.00% (98.01%) \tEmb_Norm: 8.72 (11.27)\n",
      "Train Epoch: 8 [188160/251450]\tLoss: 0.0061 (0.0105) \tAcc: 100.00% (98.01%) \tEmb_Norm: 8.71 (11.26)\n",
      "Train Epoch: 8 [188800/251450]\tLoss: 0.0183 (0.0105) \tAcc: 96.88% (98.01%) \tEmb_Norm: 8.69 (11.25)\n",
      "Train Epoch: 8 [189440/251450]\tLoss: 0.0045 (0.0105) \tAcc: 100.00% (98.01%) \tEmb_Norm: 8.68 (11.24)\n",
      "Train Epoch: 8 [190080/251450]\tLoss: 0.0061 (0.0105) \tAcc: 100.00% (98.01%) \tEmb_Norm: 8.66 (11.23)\n",
      "Train Epoch: 8 [190720/251450]\tLoss: 0.0152 (0.0105) \tAcc: 96.88% (98.01%) \tEmb_Norm: 8.65 (11.23)\n",
      "Train Epoch: 8 [191360/251450]\tLoss: 0.0059 (0.0105) \tAcc: 100.00% (98.02%) \tEmb_Norm: 8.63 (11.22)\n",
      "Train Epoch: 8 [192000/251450]\tLoss: 0.0033 (0.0105) \tAcc: 100.00% (98.02%) \tEmb_Norm: 8.61 (11.21)\n",
      "Train Epoch: 8 [192640/251450]\tLoss: 0.0168 (0.0105) \tAcc: 96.88% (98.02%) \tEmb_Norm: 8.60 (11.20)\n",
      "Train Epoch: 8 [193280/251450]\tLoss: 0.0070 (0.0105) \tAcc: 100.00% (98.02%) \tEmb_Norm: 8.58 (11.19)\n",
      "Train Epoch: 8 [193920/251450]\tLoss: 0.0134 (0.0105) \tAcc: 96.88% (98.02%) \tEmb_Norm: 8.56 (11.18)\n",
      "Train Epoch: 8 [194560/251450]\tLoss: 0.0097 (0.0105) \tAcc: 96.88% (98.02%) \tEmb_Norm: 8.55 (11.17)\n",
      "Train Epoch: 8 [195200/251450]\tLoss: 0.0011 (0.0105) \tAcc: 100.00% (98.02%) \tEmb_Norm: 8.53 (11.17)\n",
      "Train Epoch: 8 [195840/251450]\tLoss: 0.0083 (0.0105) \tAcc: 96.88% (98.02%) \tEmb_Norm: 8.52 (11.16)\n",
      "Train Epoch: 8 [196480/251450]\tLoss: 0.0076 (0.0105) \tAcc: 100.00% (98.02%) \tEmb_Norm: 8.50 (11.15)\n",
      "Train Epoch: 8 [197120/251450]\tLoss: 0.0091 (0.0105) \tAcc: 100.00% (98.02%) \tEmb_Norm: 8.49 (11.14)\n",
      "Train Epoch: 8 [197760/251450]\tLoss: 0.0106 (0.0105) \tAcc: 96.88% (98.03%) \tEmb_Norm: 8.47 (11.13)\n",
      "Train Epoch: 8 [198400/251450]\tLoss: 0.0083 (0.0105) \tAcc: 100.00% (98.03%) \tEmb_Norm: 8.46 (11.12)\n",
      "Train Epoch: 8 [199040/251450]\tLoss: 0.0180 (0.0105) \tAcc: 96.88% (98.02%) \tEmb_Norm: 8.44 (11.11)\n",
      "Train Epoch: 8 [199680/251450]\tLoss: 0.0223 (0.0105) \tAcc: 96.88% (98.02%) \tEmb_Norm: 8.42 (11.11)\n",
      "Train Epoch: 8 [200320/251450]\tLoss: 0.0071 (0.0105) \tAcc: 96.88% (98.02%) \tEmb_Norm: 8.41 (11.10)\n",
      "Train Epoch: 8 [200960/251450]\tLoss: 0.0008 (0.0105) \tAcc: 100.00% (98.03%) \tEmb_Norm: 8.39 (11.09)\n",
      "Train Epoch: 8 [201600/251450]\tLoss: 0.0108 (0.0105) \tAcc: 96.88% (98.03%) \tEmb_Norm: 8.38 (11.08)\n",
      "=============== TEST epoch 8 ===============\n",
      "\n",
      "Test set: Average loss: 0.0169, Accuracy: 96.64%\n",
      "\n",
      "=============== checkpoint epoch 8 ===============\n",
      "=============== TRAIN epoch 9 ===============\n",
      "Train Epoch: 9 [0/251450]\tLoss: 0.0128 (0.0128) \tAcc: 96.88% (96.88%) \tEmb_Norm: 8.36 (8.36)\n",
      "Train Epoch: 9 [640/251450]\tLoss: 0.0002 (0.0091) \tAcc: 100.00% (98.36%) \tEmb_Norm: 8.35 (8.36)\n",
      "Train Epoch: 9 [1280/251450]\tLoss: 0.0102 (0.0097) \tAcc: 100.00% (98.48%) \tEmb_Norm: 8.33 (8.35)\n",
      "Train Epoch: 9 [1920/251450]\tLoss: 0.0151 (0.0106) \tAcc: 96.88% (98.05%) \tEmb_Norm: 8.31 (8.34)\n",
      "Train Epoch: 9 [2560/251450]\tLoss: 0.0061 (0.0105) \tAcc: 100.00% (98.19%) \tEmb_Norm: 8.30 (8.33)\n",
      "Train Epoch: 9 [3200/251450]\tLoss: 0.0111 (0.0111) \tAcc: 100.00% (98.14%) \tEmb_Norm: 8.28 (8.32)\n",
      "Train Epoch: 9 [3840/251450]\tLoss: 0.0081 (0.0111) \tAcc: 96.88% (98.14%) \tEmb_Norm: 8.27 (8.32)\n",
      "Train Epoch: 9 [4480/251450]\tLoss: 0.0123 (0.0112) \tAcc: 100.00% (98.14%) \tEmb_Norm: 8.25 (8.31)\n",
      "Train Epoch: 9 [5120/251450]\tLoss: 0.0037 (0.0109) \tAcc: 100.00% (98.18%) \tEmb_Norm: 8.24 (8.30)\n",
      "Train Epoch: 9 [5760/251450]\tLoss: 0.0000 (0.0107) \tAcc: 100.00% (98.19%) \tEmb_Norm: 8.22 (8.29)\n",
      "Train Epoch: 9 [6400/251450]\tLoss: 0.0181 (0.0108) \tAcc: 96.88% (98.17%) \tEmb_Norm: 8.20 (8.28)\n",
      "Train Epoch: 9 [7040/251450]\tLoss: 0.0000 (0.0108) \tAcc: 100.00% (98.19%) \tEmb_Norm: 8.19 (8.28)\n",
      "Train Epoch: 9 [7680/251450]\tLoss: 0.0000 (0.0106) \tAcc: 100.00% (98.18%) \tEmb_Norm: 8.17 (8.27)\n",
      "Train Epoch: 9 [8320/251450]\tLoss: 0.0098 (0.0105) \tAcc: 100.00% (98.19%) \tEmb_Norm: 8.16 (8.26)\n",
      "Train Epoch: 9 [8960/251450]\tLoss: 0.0000 (0.0105) \tAcc: 100.00% (98.13%) \tEmb_Norm: 8.14 (8.25)\n",
      "Train Epoch: 9 [9600/251450]\tLoss: 0.0005 (0.0104) \tAcc: 100.00% (98.14%) \tEmb_Norm: 8.13 (8.24)\n",
      "Train Epoch: 9 [10240/251450]\tLoss: 0.0014 (0.0105) \tAcc: 100.00% (98.13%) \tEmb_Norm: 8.11 (8.24)\n",
      "Train Epoch: 9 [10880/251450]\tLoss: 0.0022 (0.0105) \tAcc: 100.00% (98.18%) \tEmb_Norm: 8.10 (8.23)\n",
      "Train Epoch: 9 [11520/251450]\tLoss: 0.0138 (0.0103) \tAcc: 96.88% (98.22%) \tEmb_Norm: 8.08 (8.22)\n",
      "Train Epoch: 9 [12160/251450]\tLoss: 0.0065 (0.0103) \tAcc: 100.00% (98.24%) \tEmb_Norm: 8.07 (8.21)\n",
      "Train Epoch: 9 [12800/251450]\tLoss: 0.0016 (0.0102) \tAcc: 100.00% (98.25%) \tEmb_Norm: 8.05 (8.21)\n",
      "Train Epoch: 9 [13440/251450]\tLoss: 0.0081 (0.0103) \tAcc: 96.88% (98.25%) \tEmb_Norm: 8.04 (8.20)\n",
      "Train Epoch: 9 [14080/251450]\tLoss: 0.0155 (0.0103) \tAcc: 96.88% (98.23%) \tEmb_Norm: 8.02 (8.19)\n",
      "Train Epoch: 9 [14720/251450]\tLoss: 0.0307 (0.0105) \tAcc: 93.75% (98.20%) \tEmb_Norm: 8.01 (8.18)\n",
      "Train Epoch: 9 [15360/251450]\tLoss: 0.0127 (0.0105) \tAcc: 100.00% (98.19%) \tEmb_Norm: 7.99 (8.18)\n",
      "Train Epoch: 9 [16000/251450]\tLoss: 0.0066 (0.0106) \tAcc: 100.00% (98.17%) \tEmb_Norm: 7.97 (8.17)\n",
      "Train Epoch: 9 [16640/251450]\tLoss: 0.0210 (0.0105) \tAcc: 96.88% (98.19%) \tEmb_Norm: 7.96 (8.16)\n",
      "Train Epoch: 9 [17280/251450]\tLoss: 0.0141 (0.0106) \tAcc: 96.88% (98.18%) \tEmb_Norm: 7.94 (8.15)\n",
      "Train Epoch: 9 [17920/251450]\tLoss: 0.0021 (0.0105) \tAcc: 100.00% (98.19%) \tEmb_Norm: 7.92 (8.14)\n",
      "Train Epoch: 9 [18560/251450]\tLoss: 0.0018 (0.0104) \tAcc: 100.00% (98.21%) \tEmb_Norm: 7.91 (8.14)\n",
      "Train Epoch: 9 [19200/251450]\tLoss: 0.0279 (0.0104) \tAcc: 96.88% (98.19%) \tEmb_Norm: 7.89 (8.13)\n",
      "Train Epoch: 9 [19840/251450]\tLoss: 0.0113 (0.0104) \tAcc: 96.88% (98.20%) \tEmb_Norm: 7.88 (8.12)\n",
      "Train Epoch: 9 [20480/251450]\tLoss: 0.0049 (0.0103) \tAcc: 100.00% (98.23%) \tEmb_Norm: 7.87 (8.11)\n",
      "Train Epoch: 9 [21120/251450]\tLoss: 0.0087 (0.0103) \tAcc: 100.00% (98.25%) \tEmb_Norm: 7.85 (8.11)\n",
      "Train Epoch: 9 [21760/251450]\tLoss: 0.0260 (0.0102) \tAcc: 96.88% (98.26%) \tEmb_Norm: 7.83 (8.10)\n",
      "Train Epoch: 9 [22400/251450]\tLoss: 0.0238 (0.0102) \tAcc: 96.88% (98.26%) \tEmb_Norm: 7.82 (8.09)\n",
      "Train Epoch: 9 [23040/251450]\tLoss: 0.0000 (0.0102) \tAcc: 100.00% (98.25%) \tEmb_Norm: 7.80 (8.08)\n",
      "Train Epoch: 9 [23680/251450]\tLoss: 0.0196 (0.0102) \tAcc: 93.75% (98.24%) \tEmb_Norm: 7.79 (8.07)\n",
      "Train Epoch: 9 [24320/251450]\tLoss: 0.0033 (0.0102) \tAcc: 100.00% (98.23%) \tEmb_Norm: 7.77 (8.07)\n",
      "Train Epoch: 9 [24960/251450]\tLoss: 0.0017 (0.0101) \tAcc: 100.00% (98.26%) \tEmb_Norm: 7.76 (8.06)\n",
      "Train Epoch: 9 [25600/251450]\tLoss: 0.0050 (0.0102) \tAcc: 100.00% (98.22%) \tEmb_Norm: 7.74 (8.05)\n",
      "Train Epoch: 9 [26240/251450]\tLoss: 0.0041 (0.0101) \tAcc: 100.00% (98.23%) \tEmb_Norm: 7.73 (8.04)\n",
      "Train Epoch: 9 [26880/251450]\tLoss: 0.0077 (0.0102) \tAcc: 100.00% (98.23%) \tEmb_Norm: 7.71 (8.04)\n",
      "Train Epoch: 9 [27520/251450]\tLoss: 0.0073 (0.0102) \tAcc: 100.00% (98.23%) \tEmb_Norm: 7.70 (8.03)\n",
      "Train Epoch: 9 [28160/251450]\tLoss: 0.0099 (0.0103) \tAcc: 96.88% (98.21%) \tEmb_Norm: 7.68 (8.02)\n",
      "Train Epoch: 9 [28800/251450]\tLoss: 0.0092 (0.0104) \tAcc: 96.88% (98.19%) \tEmb_Norm: 7.67 (8.01)\n",
      "Train Epoch: 9 [29440/251450]\tLoss: 0.0095 (0.0104) \tAcc: 100.00% (98.19%) \tEmb_Norm: 7.65 (8.00)\n",
      "Train Epoch: 9 [30080/251450]\tLoss: 0.0216 (0.0104) \tAcc: 93.75% (98.20%) \tEmb_Norm: 7.64 (8.00)\n",
      "Train Epoch: 9 [30720/251450]\tLoss: 0.0027 (0.0104) \tAcc: 100.00% (98.20%) \tEmb_Norm: 7.62 (7.99)\n",
      "Train Epoch: 9 [31360/251450]\tLoss: 0.0225 (0.0104) \tAcc: 96.88% (98.19%) \tEmb_Norm: 7.61 (7.98)\n",
      "Train Epoch: 9 [32000/251450]\tLoss: 0.0170 (0.0105) \tAcc: 96.88% (98.17%) \tEmb_Norm: 7.59 (7.97)\n",
      "Train Epoch: 9 [32640/251450]\tLoss: 0.0500 (0.0105) \tAcc: 90.62% (98.15%) \tEmb_Norm: 7.58 (7.97)\n",
      "Train Epoch: 9 [33280/251450]\tLoss: 0.0252 (0.0106) \tAcc: 93.75% (98.14%) \tEmb_Norm: 7.56 (7.96)\n",
      "Train Epoch: 9 [33920/251450]\tLoss: 0.0000 (0.0105) \tAcc: 100.00% (98.15%) \tEmb_Norm: 7.55 (7.95)\n",
      "Train Epoch: 9 [34560/251450]\tLoss: 0.0211 (0.0105) \tAcc: 96.88% (98.15%) \tEmb_Norm: 7.53 (7.94)\n",
      "Train Epoch: 9 [35200/251450]\tLoss: 0.0124 (0.0106) \tAcc: 96.88% (98.14%) \tEmb_Norm: 7.52 (7.94)\n",
      "Train Epoch: 9 [35840/251450]\tLoss: 0.0070 (0.0105) \tAcc: 100.00% (98.15%) \tEmb_Norm: 7.50 (7.93)\n",
      "Train Epoch: 9 [36480/251450]\tLoss: 0.0191 (0.0105) \tAcc: 93.75% (98.15%) \tEmb_Norm: 7.49 (7.92)\n",
      "Train Epoch: 9 [37120/251450]\tLoss: 0.0100 (0.0105) \tAcc: 96.88% (98.15%) \tEmb_Norm: 7.47 (7.91)\n",
      "Train Epoch: 9 [37760/251450]\tLoss: 0.0129 (0.0105) \tAcc: 96.88% (98.15%) \tEmb_Norm: 7.46 (7.91)\n",
      "Train Epoch: 9 [38400/251450]\tLoss: 0.0011 (0.0105) \tAcc: 100.00% (98.16%) \tEmb_Norm: 7.44 (7.90)\n",
      "Train Epoch: 9 [39040/251450]\tLoss: 0.0137 (0.0105) \tAcc: 100.00% (98.16%) \tEmb_Norm: 7.43 (7.89)\n",
      "Train Epoch: 9 [39680/251450]\tLoss: 0.0053 (0.0105) \tAcc: 100.00% (98.18%) \tEmb_Norm: 7.41 (7.88)\n",
      "Train Epoch: 9 [40320/251450]\tLoss: 0.0259 (0.0105) \tAcc: 90.62% (98.18%) \tEmb_Norm: 7.40 (7.88)\n",
      "Train Epoch: 9 [40960/251450]\tLoss: 0.0158 (0.0105) \tAcc: 96.88% (98.18%) \tEmb_Norm: 7.39 (7.87)\n",
      "Train Epoch: 9 [41600/251450]\tLoss: 0.0000 (0.0105) \tAcc: 100.00% (98.16%) \tEmb_Norm: 7.37 (7.86)\n",
      "Train Epoch: 9 [42240/251450]\tLoss: 0.0177 (0.0105) \tAcc: 96.88% (98.15%) \tEmb_Norm: 7.36 (7.85)\n",
      "Train Epoch: 9 [42880/251450]\tLoss: 0.0070 (0.0106) \tAcc: 96.88% (98.15%) \tEmb_Norm: 7.34 (7.85)\n",
      "Train Epoch: 9 [43520/251450]\tLoss: 0.0062 (0.0105) \tAcc: 100.00% (98.16%) \tEmb_Norm: 7.33 (7.84)\n",
      "Train Epoch: 9 [44160/251450]\tLoss: 0.0079 (0.0105) \tAcc: 100.00% (98.17%) \tEmb_Norm: 7.31 (7.83)\n",
      "Train Epoch: 9 [44800/251450]\tLoss: 0.0061 (0.0105) \tAcc: 100.00% (98.17%) \tEmb_Norm: 7.30 (7.82)\n",
      "Train Epoch: 9 [45440/251450]\tLoss: 0.0166 (0.0105) \tAcc: 96.88% (98.17%) \tEmb_Norm: 7.29 (7.82)\n",
      "Train Epoch: 9 [46080/251450]\tLoss: 0.0000 (0.0105) \tAcc: 100.00% (98.17%) \tEmb_Norm: 7.27 (7.81)\n",
      "Train Epoch: 9 [46720/251450]\tLoss: 0.0066 (0.0105) \tAcc: 96.88% (98.16%) \tEmb_Norm: 7.26 (7.80)\n",
      "Train Epoch: 9 [47360/251450]\tLoss: 0.0122 (0.0105) \tAcc: 96.88% (98.16%) \tEmb_Norm: 7.24 (7.79)\n",
      "Train Epoch: 9 [48000/251450]\tLoss: 0.0118 (0.0105) \tAcc: 96.88% (98.16%) \tEmb_Norm: 7.23 (7.79)\n",
      "Train Epoch: 9 [48640/251450]\tLoss: 0.0072 (0.0104) \tAcc: 100.00% (98.18%) \tEmb_Norm: 7.21 (7.78)\n",
      "Train Epoch: 9 [49280/251450]\tLoss: 0.0075 (0.0105) \tAcc: 100.00% (98.17%) \tEmb_Norm: 7.20 (7.77)\n",
      "Train Epoch: 9 [49920/251450]\tLoss: 0.0154 (0.0105) \tAcc: 96.88% (98.17%) \tEmb_Norm: 7.18 (7.76)\n",
      "Train Epoch: 9 [50560/251450]\tLoss: 0.0235 (0.0105) \tAcc: 96.88% (98.16%) \tEmb_Norm: 7.17 (7.76)\n",
      "Train Epoch: 9 [51200/251450]\tLoss: 0.0027 (0.0104) \tAcc: 100.00% (98.17%) \tEmb_Norm: 7.16 (7.75)\n",
      "Train Epoch: 9 [51840/251450]\tLoss: 0.0198 (0.0104) \tAcc: 93.75% (98.17%) \tEmb_Norm: 7.14 (7.74)\n",
      "Train Epoch: 9 [52480/251450]\tLoss: 0.0087 (0.0105) \tAcc: 100.00% (98.16%) \tEmb_Norm: 7.13 (7.73)\n",
      "Train Epoch: 9 [53120/251450]\tLoss: 0.0066 (0.0105) \tAcc: 96.88% (98.16%) \tEmb_Norm: 7.11 (7.73)\n",
      "Train Epoch: 9 [53760/251450]\tLoss: 0.0068 (0.0105) \tAcc: 100.00% (98.16%) \tEmb_Norm: 7.10 (7.72)\n",
      "Train Epoch: 9 [54400/251450]\tLoss: 0.0002 (0.0105) \tAcc: 100.00% (98.17%) \tEmb_Norm: 7.09 (7.71)\n",
      "Train Epoch: 9 [55040/251450]\tLoss: 0.0261 (0.0104) \tAcc: 93.75% (98.17%) \tEmb_Norm: 7.07 (7.70)\n",
      "Train Epoch: 9 [55680/251450]\tLoss: 0.0055 (0.0104) \tAcc: 100.00% (98.17%) \tEmb_Norm: 7.06 (7.70)\n",
      "Train Epoch: 9 [56320/251450]\tLoss: 0.0162 (0.0104) \tAcc: 93.75% (98.17%) \tEmb_Norm: 7.04 (7.69)\n",
      "Train Epoch: 9 [56960/251450]\tLoss: 0.0085 (0.0104) \tAcc: 100.00% (98.17%) \tEmb_Norm: 7.03 (7.68)\n",
      "Train Epoch: 9 [57600/251450]\tLoss: 0.0091 (0.0104) \tAcc: 100.00% (98.18%) \tEmb_Norm: 7.02 (7.67)\n",
      "Train Epoch: 9 [58240/251450]\tLoss: 0.0106 (0.0104) \tAcc: 100.00% (98.19%) \tEmb_Norm: 7.00 (7.67)\n",
      "Train Epoch: 9 [58880/251450]\tLoss: 0.0031 (0.0104) \tAcc: 100.00% (98.18%) \tEmb_Norm: 6.99 (7.66)\n",
      "Train Epoch: 9 [59520/251450]\tLoss: 0.0222 (0.0104) \tAcc: 96.88% (98.18%) \tEmb_Norm: 6.98 (7.65)\n",
      "Train Epoch: 9 [60160/251450]\tLoss: 0.0022 (0.0104) \tAcc: 100.00% (98.19%) \tEmb_Norm: 6.96 (7.65)\n",
      "Train Epoch: 9 [60800/251450]\tLoss: 0.0051 (0.0104) \tAcc: 100.00% (98.19%) \tEmb_Norm: 6.95 (7.64)\n",
      "Train Epoch: 9 [61440/251450]\tLoss: 0.0291 (0.0104) \tAcc: 93.75% (98.19%) \tEmb_Norm: 6.94 (7.63)\n",
      "Train Epoch: 9 [62080/251450]\tLoss: 0.0153 (0.0105) \tAcc: 96.88% (98.19%) \tEmb_Norm: 6.93 (7.62)\n",
      "Train Epoch: 9 [62720/251450]\tLoss: 0.0232 (0.0105) \tAcc: 96.88% (98.18%) \tEmb_Norm: 6.91 (7.62)\n",
      "Train Epoch: 9 [63360/251450]\tLoss: 0.0261 (0.0105) \tAcc: 96.88% (98.18%) \tEmb_Norm: 6.90 (7.61)\n",
      "Train Epoch: 9 [64000/251450]\tLoss: 0.0131 (0.0105) \tAcc: 96.88% (98.19%) \tEmb_Norm: 6.88 (7.60)\n",
      "Train Epoch: 9 [64640/251450]\tLoss: 0.0093 (0.0105) \tAcc: 100.00% (98.19%) \tEmb_Norm: 6.87 (7.59)\n",
      "Train Epoch: 9 [65280/251450]\tLoss: 0.0039 (0.0105) \tAcc: 100.00% (98.19%) \tEmb_Norm: 6.86 (7.59)\n",
      "Train Epoch: 9 [65920/251450]\tLoss: 0.0000 (0.0104) \tAcc: 100.00% (98.20%) \tEmb_Norm: 6.85 (7.58)\n",
      "Train Epoch: 9 [66560/251450]\tLoss: 0.0269 (0.0104) \tAcc: 90.62% (98.20%) \tEmb_Norm: 6.83 (7.57)\n",
      "Train Epoch: 9 [67200/251450]\tLoss: 0.0031 (0.0105) \tAcc: 100.00% (98.18%) \tEmb_Norm: 6.82 (7.57)\n",
      "Train Epoch: 9 [67840/251450]\tLoss: 0.0041 (0.0105) \tAcc: 100.00% (98.18%) \tEmb_Norm: 6.80 (7.56)\n",
      "Train Epoch: 9 [68480/251450]\tLoss: 0.0053 (0.0105) \tAcc: 100.00% (98.19%) \tEmb_Norm: 6.79 (7.55)\n",
      "Train Epoch: 9 [69120/251450]\tLoss: 0.0006 (0.0104) \tAcc: 100.00% (98.20%) \tEmb_Norm: 6.78 (7.55)\n",
      "Train Epoch: 9 [69760/251450]\tLoss: 0.0298 (0.0104) \tAcc: 93.75% (98.20%) \tEmb_Norm: 6.76 (7.54)\n",
      "Train Epoch: 9 [70400/251450]\tLoss: 0.0263 (0.0104) \tAcc: 93.75% (98.20%) \tEmb_Norm: 6.75 (7.53)\n",
      "Train Epoch: 9 [71040/251450]\tLoss: 0.0019 (0.0105) \tAcc: 100.00% (98.20%) \tEmb_Norm: 6.73 (7.52)\n",
      "Train Epoch: 9 [71680/251450]\tLoss: 0.0173 (0.0105) \tAcc: 93.75% (98.20%) \tEmb_Norm: 6.72 (7.52)\n",
      "Train Epoch: 9 [72320/251450]\tLoss: 0.0184 (0.0104) \tAcc: 93.75% (98.21%) \tEmb_Norm: 6.71 (7.51)\n",
      "Train Epoch: 9 [72960/251450]\tLoss: 0.0209 (0.0105) \tAcc: 96.88% (98.20%) \tEmb_Norm: 6.70 (7.50)\n",
      "Train Epoch: 9 [73600/251450]\tLoss: 0.0054 (0.0105) \tAcc: 100.00% (98.21%) \tEmb_Norm: 6.68 (7.50)\n",
      "Train Epoch: 9 [74240/251450]\tLoss: 0.0021 (0.0105) \tAcc: 100.00% (98.20%) \tEmb_Norm: 6.67 (7.49)\n",
      "Train Epoch: 9 [74880/251450]\tLoss: 0.0026 (0.0105) \tAcc: 100.00% (98.20%) \tEmb_Norm: 6.66 (7.48)\n",
      "Train Epoch: 9 [75520/251450]\tLoss: 0.0083 (0.0105) \tAcc: 96.88% (98.19%) \tEmb_Norm: 6.64 (7.47)\n",
      "Train Epoch: 9 [76160/251450]\tLoss: 0.0115 (0.0105) \tAcc: 96.88% (98.19%) \tEmb_Norm: 6.63 (7.47)\n",
      "Train Epoch: 9 [76800/251450]\tLoss: 0.0255 (0.0105) \tAcc: 96.88% (98.19%) \tEmb_Norm: 6.62 (7.46)\n",
      "Train Epoch: 9 [77440/251450]\tLoss: 0.0086 (0.0105) \tAcc: 96.88% (98.20%) \tEmb_Norm: 6.60 (7.45)\n",
      "Train Epoch: 9 [78080/251450]\tLoss: 0.0075 (0.0106) \tAcc: 100.00% (98.19%) \tEmb_Norm: 6.59 (7.45)\n",
      "Train Epoch: 9 [78720/251450]\tLoss: 0.0247 (0.0106) \tAcc: 96.88% (98.19%) \tEmb_Norm: 6.57 (7.44)\n",
      "Train Epoch: 9 [79360/251450]\tLoss: 0.0054 (0.0106) \tAcc: 100.00% (98.19%) \tEmb_Norm: 6.56 (7.43)\n",
      "Train Epoch: 9 [80000/251450]\tLoss: 0.0169 (0.0106) \tAcc: 96.88% (98.19%) \tEmb_Norm: 6.55 (7.42)\n",
      "Train Epoch: 9 [80640/251450]\tLoss: 0.0017 (0.0106) \tAcc: 100.00% (98.18%) \tEmb_Norm: 6.54 (7.42)\n",
      "Train Epoch: 9 [81280/251450]\tLoss: 0.0158 (0.0106) \tAcc: 96.88% (98.19%) \tEmb_Norm: 6.52 (7.41)\n",
      "Train Epoch: 9 [81920/251450]\tLoss: 0.0000 (0.0106) \tAcc: 100.00% (98.19%) \tEmb_Norm: 6.51 (7.40)\n",
      "Train Epoch: 9 [82560/251450]\tLoss: 0.0036 (0.0106) \tAcc: 100.00% (98.19%) \tEmb_Norm: 6.49 (7.40)\n",
      "Train Epoch: 9 [83200/251450]\tLoss: 0.0127 (0.0106) \tAcc: 96.88% (98.19%) \tEmb_Norm: 6.48 (7.39)\n",
      "Train Epoch: 9 [83840/251450]\tLoss: 0.0009 (0.0106) \tAcc: 100.00% (98.19%) \tEmb_Norm: 6.47 (7.38)\n",
      "Train Epoch: 9 [84480/251450]\tLoss: 0.0143 (0.0106) \tAcc: 93.75% (98.19%) \tEmb_Norm: 6.46 (7.38)\n",
      "Train Epoch: 9 [85120/251450]\tLoss: 0.0089 (0.0106) \tAcc: 100.00% (98.19%) \tEmb_Norm: 6.44 (7.37)\n",
      "Train Epoch: 9 [85760/251450]\tLoss: 0.0040 (0.0106) \tAcc: 100.00% (98.19%) \tEmb_Norm: 6.43 (7.36)\n",
      "Train Epoch: 9 [86400/251450]\tLoss: 0.0000 (0.0106) \tAcc: 100.00% (98.20%) \tEmb_Norm: 6.42 (7.36)\n",
      "Train Epoch: 9 [87040/251450]\tLoss: 0.0266 (0.0106) \tAcc: 96.88% (98.20%) \tEmb_Norm: 6.40 (7.35)\n",
      "Train Epoch: 9 [87680/251450]\tLoss: 0.0119 (0.0106) \tAcc: 96.88% (98.21%) \tEmb_Norm: 6.39 (7.34)\n",
      "Train Epoch: 9 [88320/251450]\tLoss: 0.0068 (0.0106) \tAcc: 100.00% (98.21%) \tEmb_Norm: 6.38 (7.33)\n",
      "Train Epoch: 9 [88960/251450]\tLoss: 0.0270 (0.0106) \tAcc: 96.88% (98.21%) \tEmb_Norm: 6.37 (7.33)\n",
      "Train Epoch: 9 [89600/251450]\tLoss: 0.0045 (0.0106) \tAcc: 100.00% (98.21%) \tEmb_Norm: 6.36 (7.32)\n",
      "Train Epoch: 9 [90240/251450]\tLoss: 0.0094 (0.0106) \tAcc: 100.00% (98.22%) \tEmb_Norm: 6.34 (7.31)\n",
      "Train Epoch: 9 [90880/251450]\tLoss: 0.0096 (0.0106) \tAcc: 100.00% (98.22%) \tEmb_Norm: 6.33 (7.31)\n",
      "Train Epoch: 9 [91520/251450]\tLoss: 0.0096 (0.0106) \tAcc: 100.00% (98.22%) \tEmb_Norm: 6.32 (7.30)\n",
      "Train Epoch: 9 [92160/251450]\tLoss: 0.0118 (0.0106) \tAcc: 96.88% (98.22%) \tEmb_Norm: 6.31 (7.29)\n",
      "Train Epoch: 9 [92800/251450]\tLoss: 0.0172 (0.0106) \tAcc: 100.00% (98.23%) \tEmb_Norm: 6.30 (7.29)\n",
      "Train Epoch: 9 [93440/251450]\tLoss: 0.0043 (0.0106) \tAcc: 100.00% (98.22%) \tEmb_Norm: 6.29 (7.28)\n",
      "Train Epoch: 9 [94080/251450]\tLoss: 0.0004 (0.0106) \tAcc: 100.00% (98.22%) \tEmb_Norm: 6.27 (7.27)\n",
      "Train Epoch: 9 [94720/251450]\tLoss: 0.0067 (0.0106) \tAcc: 100.00% (98.23%) \tEmb_Norm: 6.26 (7.27)\n",
      "Train Epoch: 9 [95360/251450]\tLoss: 0.0091 (0.0106) \tAcc: 100.00% (98.22%) \tEmb_Norm: 6.25 (7.26)\n",
      "Train Epoch: 9 [96000/251450]\tLoss: 0.0048 (0.0107) \tAcc: 100.00% (98.23%) \tEmb_Norm: 6.24 (7.25)\n",
      "Train Epoch: 9 [96640/251450]\tLoss: 0.0047 (0.0106) \tAcc: 100.00% (98.23%) \tEmb_Norm: 6.23 (7.25)\n",
      "Train Epoch: 9 [97280/251450]\tLoss: 0.0170 (0.0107) \tAcc: 96.88% (98.22%) \tEmb_Norm: 6.21 (7.24)\n",
      "Train Epoch: 9 [97920/251450]\tLoss: 0.0031 (0.0107) \tAcc: 100.00% (98.22%) \tEmb_Norm: 6.20 (7.23)\n",
      "Train Epoch: 9 [98560/251450]\tLoss: 0.0090 (0.0107) \tAcc: 100.00% (98.22%) \tEmb_Norm: 6.19 (7.23)\n",
      "Train Epoch: 9 [99200/251450]\tLoss: 0.0043 (0.0107) \tAcc: 100.00% (98.22%) \tEmb_Norm: 6.18 (7.22)\n",
      "Train Epoch: 9 [99840/251450]\tLoss: 0.0314 (0.0107) \tAcc: 96.88% (98.22%) \tEmb_Norm: 6.17 (7.21)\n",
      "Train Epoch: 9 [100480/251450]\tLoss: 0.0113 (0.0107) \tAcc: 100.00% (98.23%) \tEmb_Norm: 6.16 (7.21)\n",
      "Train Epoch: 9 [101120/251450]\tLoss: 0.0154 (0.0107) \tAcc: 100.00% (98.23%) \tEmb_Norm: 6.15 (7.20)\n",
      "Train Epoch: 9 [101760/251450]\tLoss: 0.0363 (0.0107) \tAcc: 93.75% (98.22%) \tEmb_Norm: 6.13 (7.19)\n",
      "Train Epoch: 9 [102400/251450]\tLoss: 0.0000 (0.0107) \tAcc: 100.00% (98.22%) \tEmb_Norm: 6.12 (7.19)\n",
      "Train Epoch: 9 [103040/251450]\tLoss: 0.0125 (0.0107) \tAcc: 96.88% (98.22%) \tEmb_Norm: 6.11 (7.18)\n",
      "Train Epoch: 9 [103680/251450]\tLoss: 0.0006 (0.0107) \tAcc: 100.00% (98.23%) \tEmb_Norm: 6.10 (7.17)\n",
      "Train Epoch: 9 [104320/251450]\tLoss: 0.0113 (0.0107) \tAcc: 96.88% (98.23%) \tEmb_Norm: 6.08 (7.17)\n",
      "Train Epoch: 9 [104960/251450]\tLoss: 0.0059 (0.0107) \tAcc: 100.00% (98.23%) \tEmb_Norm: 6.07 (7.16)\n",
      "Train Epoch: 9 [105600/251450]\tLoss: 0.0083 (0.0107) \tAcc: 96.88% (98.22%) \tEmb_Norm: 6.06 (7.15)\n",
      "Train Epoch: 9 [106240/251450]\tLoss: 0.0034 (0.0107) \tAcc: 100.00% (98.22%) \tEmb_Norm: 6.04 (7.15)\n",
      "Train Epoch: 9 [106880/251450]\tLoss: 0.0179 (0.0108) \tAcc: 96.88% (98.22%) \tEmb_Norm: 6.03 (7.14)\n",
      "Train Epoch: 9 [107520/251450]\tLoss: 0.0172 (0.0107) \tAcc: 93.75% (98.22%) \tEmb_Norm: 6.02 (7.13)\n",
      "Train Epoch: 9 [108160/251450]\tLoss: 0.0202 (0.0108) \tAcc: 93.75% (98.22%) \tEmb_Norm: 6.01 (7.13)\n",
      "Train Epoch: 9 [108800/251450]\tLoss: 0.0000 (0.0108) \tAcc: 100.00% (98.22%) \tEmb_Norm: 6.00 (7.12)\n",
      "Train Epoch: 9 [109440/251450]\tLoss: 0.0119 (0.0108) \tAcc: 96.88% (98.21%) \tEmb_Norm: 5.99 (7.11)\n",
      "Train Epoch: 9 [110080/251450]\tLoss: 0.0016 (0.0108) \tAcc: 100.00% (98.21%) \tEmb_Norm: 5.97 (7.11)\n",
      "Train Epoch: 9 [110720/251450]\tLoss: 0.0118 (0.0108) \tAcc: 100.00% (98.21%) \tEmb_Norm: 5.96 (7.10)\n",
      "Train Epoch: 9 [111360/251450]\tLoss: 0.0268 (0.0108) \tAcc: 93.75% (98.21%) \tEmb_Norm: 5.95 (7.09)\n",
      "Train Epoch: 9 [112000/251450]\tLoss: 0.0233 (0.0108) \tAcc: 93.75% (98.20%) \tEmb_Norm: 5.94 (7.09)\n",
      "Train Epoch: 9 [112640/251450]\tLoss: 0.0000 (0.0109) \tAcc: 100.00% (98.20%) \tEmb_Norm: 5.93 (7.08)\n",
      "Train Epoch: 9 [113280/251450]\tLoss: 0.0126 (0.0109) \tAcc: 96.88% (98.20%) \tEmb_Norm: 5.92 (7.07)\n",
      "Train Epoch: 9 [113920/251450]\tLoss: 0.0086 (0.0109) \tAcc: 100.00% (98.21%) \tEmb_Norm: 5.91 (7.07)\n",
      "Train Epoch: 9 [114560/251450]\tLoss: 0.0047 (0.0109) \tAcc: 100.00% (98.21%) \tEmb_Norm: 5.90 (7.06)\n",
      "Train Epoch: 9 [115200/251450]\tLoss: 0.0131 (0.0109) \tAcc: 100.00% (98.21%) \tEmb_Norm: 5.89 (7.05)\n",
      "Train Epoch: 9 [115840/251450]\tLoss: 0.0075 (0.0108) \tAcc: 100.00% (98.22%) \tEmb_Norm: 5.87 (7.05)\n",
      "Train Epoch: 9 [116480/251450]\tLoss: 0.0066 (0.0108) \tAcc: 96.88% (98.21%) \tEmb_Norm: 5.86 (7.04)\n",
      "Train Epoch: 9 [117120/251450]\tLoss: 0.0004 (0.0108) \tAcc: 100.00% (98.22%) \tEmb_Norm: 5.85 (7.03)\n",
      "Train Epoch: 9 [117760/251450]\tLoss: 0.0000 (0.0108) \tAcc: 100.00% (98.22%) \tEmb_Norm: 5.84 (7.03)\n",
      "Train Epoch: 9 [118400/251450]\tLoss: 0.0090 (0.0108) \tAcc: 100.00% (98.23%) \tEmb_Norm: 5.83 (7.02)\n",
      "Train Epoch: 9 [119040/251450]\tLoss: 0.0067 (0.0108) \tAcc: 100.00% (98.23%) \tEmb_Norm: 5.82 (7.01)\n",
      "Train Epoch: 9 [119680/251450]\tLoss: 0.0197 (0.0108) \tAcc: 96.88% (98.22%) \tEmb_Norm: 5.81 (7.01)\n",
      "Train Epoch: 9 [120320/251450]\tLoss: 0.0069 (0.0108) \tAcc: 96.88% (98.23%) \tEmb_Norm: 5.80 (7.00)\n",
      "Train Epoch: 9 [120960/251450]\tLoss: 0.0086 (0.0108) \tAcc: 96.88% (98.23%) \tEmb_Norm: 5.79 (7.00)\n",
      "Train Epoch: 9 [121600/251450]\tLoss: 0.0000 (0.0108) \tAcc: 100.00% (98.23%) \tEmb_Norm: 5.78 (6.99)\n",
      "Train Epoch: 9 [122240/251450]\tLoss: 0.0102 (0.0108) \tAcc: 96.88% (98.23%) \tEmb_Norm: 5.77 (6.98)\n",
      "Train Epoch: 9 [122880/251450]\tLoss: 0.0171 (0.0109) \tAcc: 100.00% (98.22%) \tEmb_Norm: 5.76 (6.98)\n",
      "Train Epoch: 9 [123520/251450]\tLoss: 0.0012 (0.0108) \tAcc: 100.00% (98.23%) \tEmb_Norm: 5.75 (6.97)\n",
      "Train Epoch: 9 [124160/251450]\tLoss: 0.0198 (0.0108) \tAcc: 93.75% (98.23%) \tEmb_Norm: 5.74 (6.96)\n",
      "Train Epoch: 9 [124800/251450]\tLoss: 0.0088 (0.0108) \tAcc: 100.00% (98.24%) \tEmb_Norm: 5.73 (6.96)\n",
      "Train Epoch: 9 [125440/251450]\tLoss: 0.0070 (0.0108) \tAcc: 100.00% (98.23%) \tEmb_Norm: 5.72 (6.95)\n",
      "Train Epoch: 9 [126080/251450]\tLoss: 0.0311 (0.0108) \tAcc: 96.88% (98.23%) \tEmb_Norm: 5.71 (6.95)\n",
      "Train Epoch: 9 [126720/251450]\tLoss: 0.0069 (0.0108) \tAcc: 100.00% (98.23%) \tEmb_Norm: 5.69 (6.94)\n",
      "Train Epoch: 9 [127360/251450]\tLoss: 0.0044 (0.0108) \tAcc: 100.00% (98.24%) \tEmb_Norm: 5.69 (6.93)\n",
      "Train Epoch: 9 [128000/251450]\tLoss: 0.0063 (0.0108) \tAcc: 100.00% (98.24%) \tEmb_Norm: 5.68 (6.93)\n",
      "Train Epoch: 9 [128640/251450]\tLoss: 0.0198 (0.0108) \tAcc: 100.00% (98.25%) \tEmb_Norm: 5.67 (6.92)\n",
      "Train Epoch: 9 [129280/251450]\tLoss: 0.0071 (0.0108) \tAcc: 100.00% (98.25%) \tEmb_Norm: 5.66 (6.91)\n",
      "Train Epoch: 9 [129920/251450]\tLoss: 0.0103 (0.0108) \tAcc: 96.88% (98.24%) \tEmb_Norm: 5.64 (6.91)\n",
      "Train Epoch: 9 [130560/251450]\tLoss: 0.0127 (0.0108) \tAcc: 100.00% (98.24%) \tEmb_Norm: 5.63 (6.90)\n",
      "Train Epoch: 9 [131200/251450]\tLoss: 0.0252 (0.0108) \tAcc: 96.88% (98.24%) \tEmb_Norm: 5.62 (6.90)\n",
      "Train Epoch: 9 [131840/251450]\tLoss: 0.0327 (0.0108) \tAcc: 90.62% (98.24%) \tEmb_Norm: 5.61 (6.89)\n",
      "Train Epoch: 9 [132480/251450]\tLoss: 0.0059 (0.0108) \tAcc: 100.00% (98.25%) \tEmb_Norm: 5.60 (6.88)\n",
      "Train Epoch: 9 [133120/251450]\tLoss: 0.0022 (0.0108) \tAcc: 100.00% (98.25%) \tEmb_Norm: 5.59 (6.88)\n",
      "Train Epoch: 9 [133760/251450]\tLoss: 0.0000 (0.0108) \tAcc: 100.00% (98.25%) \tEmb_Norm: 5.58 (6.87)\n",
      "Train Epoch: 9 [134400/251450]\tLoss: 0.0113 (0.0108) \tAcc: 100.00% (98.24%) \tEmb_Norm: 5.57 (6.86)\n",
      "Train Epoch: 9 [135040/251450]\tLoss: 0.0042 (0.0108) \tAcc: 100.00% (98.25%) \tEmb_Norm: 5.56 (6.86)\n",
      "Train Epoch: 9 [135680/251450]\tLoss: 0.0057 (0.0108) \tAcc: 100.00% (98.25%) \tEmb_Norm: 5.55 (6.85)\n",
      "Train Epoch: 9 [136320/251450]\tLoss: 0.0164 (0.0108) \tAcc: 100.00% (98.25%) \tEmb_Norm: 5.55 (6.85)\n",
      "Train Epoch: 9 [136960/251450]\tLoss: 0.0222 (0.0108) \tAcc: 96.88% (98.25%) \tEmb_Norm: 5.54 (6.84)\n",
      "Train Epoch: 9 [137600/251450]\tLoss: 0.0209 (0.0109) \tAcc: 100.00% (98.25%) \tEmb_Norm: 5.53 (6.83)\n",
      "Train Epoch: 9 [138240/251450]\tLoss: 0.0196 (0.0109) \tAcc: 93.75% (98.25%) \tEmb_Norm: 5.52 (6.83)\n",
      "Train Epoch: 9 [138880/251450]\tLoss: 0.0075 (0.0109) \tAcc: 100.00% (98.25%) \tEmb_Norm: 5.51 (6.82)\n",
      "Train Epoch: 9 [139520/251450]\tLoss: 0.0153 (0.0109) \tAcc: 96.88% (98.25%) \tEmb_Norm: 5.50 (6.82)\n",
      "Train Epoch: 9 [140160/251450]\tLoss: 0.0035 (0.0109) \tAcc: 100.00% (98.25%) \tEmb_Norm: 5.49 (6.81)\n",
      "Train Epoch: 9 [140800/251450]\tLoss: 0.0082 (0.0109) \tAcc: 100.00% (98.26%) \tEmb_Norm: 5.48 (6.80)\n",
      "Train Epoch: 9 [141440/251450]\tLoss: 0.0010 (0.0108) \tAcc: 100.00% (98.26%) \tEmb_Norm: 5.47 (6.80)\n",
      "Train Epoch: 9 [142080/251450]\tLoss: 0.0328 (0.0109) \tAcc: 90.62% (98.25%) \tEmb_Norm: 5.46 (6.79)\n",
      "Train Epoch: 9 [142720/251450]\tLoss: 0.0054 (0.0109) \tAcc: 100.00% (98.26%) \tEmb_Norm: 5.45 (6.79)\n",
      "Train Epoch: 9 [143360/251450]\tLoss: 0.0043 (0.0109) \tAcc: 100.00% (98.26%) \tEmb_Norm: 5.45 (6.78)\n",
      "Train Epoch: 9 [144000/251450]\tLoss: 0.0025 (0.0109) \tAcc: 100.00% (98.26%) \tEmb_Norm: 5.43 (6.77)\n",
      "Train Epoch: 9 [144640/251450]\tLoss: 0.0248 (0.0109) \tAcc: 96.88% (98.26%) \tEmb_Norm: 5.42 (6.77)\n",
      "Train Epoch: 9 [145280/251450]\tLoss: 0.0187 (0.0109) \tAcc: 96.88% (98.26%) \tEmb_Norm: 5.41 (6.76)\n",
      "Train Epoch: 9 [145920/251450]\tLoss: 0.0144 (0.0109) \tAcc: 100.00% (98.26%) \tEmb_Norm: 5.40 (6.76)\n",
      "Train Epoch: 9 [146560/251450]\tLoss: 0.0098 (0.0109) \tAcc: 100.00% (98.26%) \tEmb_Norm: 5.40 (6.75)\n",
      "Train Epoch: 9 [147200/251450]\tLoss: 0.0033 (0.0109) \tAcc: 100.00% (98.26%) \tEmb_Norm: 5.39 (6.74)\n",
      "Train Epoch: 9 [147840/251450]\tLoss: 0.0117 (0.0109) \tAcc: 100.00% (98.26%) \tEmb_Norm: 5.38 (6.74)\n",
      "Train Epoch: 9 [148480/251450]\tLoss: 0.0131 (0.0109) \tAcc: 96.88% (98.26%) \tEmb_Norm: 5.37 (6.73)\n",
      "Train Epoch: 9 [149120/251450]\tLoss: 0.0161 (0.0109) \tAcc: 96.88% (98.26%) \tEmb_Norm: 5.36 (6.73)\n",
      "Train Epoch: 9 [149760/251450]\tLoss: 0.0000 (0.0109) \tAcc: 100.00% (98.26%) \tEmb_Norm: 5.35 (6.72)\n",
      "Train Epoch: 9 [150400/251450]\tLoss: 0.0243 (0.0109) \tAcc: 96.88% (98.26%) \tEmb_Norm: 5.34 (6.71)\n",
      "Train Epoch: 9 [151040/251450]\tLoss: 0.0019 (0.0109) \tAcc: 100.00% (98.26%) \tEmb_Norm: 5.33 (6.71)\n",
      "Train Epoch: 9 [151680/251450]\tLoss: 0.0025 (0.0109) \tAcc: 100.00% (98.26%) \tEmb_Norm: 5.32 (6.70)\n",
      "Train Epoch: 9 [152320/251450]\tLoss: 0.0009 (0.0109) \tAcc: 100.00% (98.26%) \tEmb_Norm: 5.31 (6.70)\n",
      "Train Epoch: 9 [152960/251450]\tLoss: 0.0012 (0.0109) \tAcc: 100.00% (98.26%) \tEmb_Norm: 5.30 (6.69)\n",
      "Train Epoch: 9 [153600/251450]\tLoss: 0.0091 (0.0109) \tAcc: 100.00% (98.26%) \tEmb_Norm: 5.29 (6.69)\n",
      "Train Epoch: 9 [154240/251450]\tLoss: 0.0076 (0.0109) \tAcc: 100.00% (98.25%) \tEmb_Norm: 5.29 (6.68)\n",
      "Train Epoch: 9 [154880/251450]\tLoss: 0.0120 (0.0109) \tAcc: 96.88% (98.25%) \tEmb_Norm: 5.28 (6.67)\n",
      "Train Epoch: 9 [155520/251450]\tLoss: 0.0036 (0.0109) \tAcc: 100.00% (98.25%) \tEmb_Norm: 5.27 (6.67)\n",
      "Train Epoch: 9 [156160/251450]\tLoss: 0.0084 (0.0109) \tAcc: 100.00% (98.25%) \tEmb_Norm: 5.26 (6.66)\n",
      "Train Epoch: 9 [156800/251450]\tLoss: 0.0153 (0.0109) \tAcc: 96.88% (98.25%) \tEmb_Norm: 5.26 (6.66)\n",
      "Train Epoch: 9 [157440/251450]\tLoss: 0.0289 (0.0110) \tAcc: 96.88% (98.25%) \tEmb_Norm: 5.25 (6.65)\n",
      "Train Epoch: 9 [158080/251450]\tLoss: 0.0129 (0.0110) \tAcc: 96.88% (98.24%) \tEmb_Norm: 5.24 (6.65)\n",
      "Train Epoch: 9 [158720/251450]\tLoss: 0.0074 (0.0110) \tAcc: 100.00% (98.24%) \tEmb_Norm: 5.23 (6.64)\n",
      "Train Epoch: 9 [159360/251450]\tLoss: 0.0028 (0.0110) \tAcc: 100.00% (98.24%) \tEmb_Norm: 5.22 (6.63)\n",
      "Train Epoch: 9 [160000/251450]\tLoss: 0.0285 (0.0110) \tAcc: 93.75% (98.24%) \tEmb_Norm: 5.21 (6.63)\n",
      "Train Epoch: 9 [160640/251450]\tLoss: 0.0196 (0.0110) \tAcc: 100.00% (98.24%) \tEmb_Norm: 5.20 (6.62)\n",
      "Train Epoch: 9 [161280/251450]\tLoss: 0.0000 (0.0110) \tAcc: 100.00% (98.24%) \tEmb_Norm: 5.19 (6.62)\n",
      "Train Epoch: 9 [161920/251450]\tLoss: 0.0268 (0.0110) \tAcc: 96.88% (98.25%) \tEmb_Norm: 5.19 (6.61)\n",
      "Train Epoch: 9 [162560/251450]\tLoss: 0.0269 (0.0110) \tAcc: 96.88% (98.25%) \tEmb_Norm: 5.18 (6.61)\n",
      "Train Epoch: 9 [163200/251450]\tLoss: 0.0188 (0.0110) \tAcc: 93.75% (98.25%) \tEmb_Norm: 5.17 (6.60)\n",
      "Train Epoch: 9 [163840/251450]\tLoss: 0.0273 (0.0110) \tAcc: 96.88% (98.25%) \tEmb_Norm: 5.16 (6.59)\n",
      "Train Epoch: 9 [164480/251450]\tLoss: 0.0210 (0.0110) \tAcc: 93.75% (98.25%) \tEmb_Norm: 5.16 (6.59)\n",
      "Train Epoch: 9 [165120/251450]\tLoss: 0.0142 (0.0110) \tAcc: 96.88% (98.25%) \tEmb_Norm: 5.15 (6.58)\n",
      "Train Epoch: 9 [165760/251450]\tLoss: 0.0239 (0.0110) \tAcc: 93.75% (98.24%) \tEmb_Norm: 5.14 (6.58)\n",
      "Train Epoch: 9 [166400/251450]\tLoss: 0.0228 (0.0110) \tAcc: 96.88% (98.24%) \tEmb_Norm: 5.13 (6.57)\n",
      "Train Epoch: 9 [167040/251450]\tLoss: 0.0456 (0.0110) \tAcc: 93.75% (98.24%) \tEmb_Norm: 5.12 (6.57)\n",
      "Train Epoch: 9 [167680/251450]\tLoss: 0.0039 (0.0110) \tAcc: 100.00% (98.23%) \tEmb_Norm: 5.11 (6.56)\n",
      "Train Epoch: 9 [168320/251450]\tLoss: 0.0042 (0.0110) \tAcc: 100.00% (98.23%) \tEmb_Norm: 5.11 (6.56)\n",
      "Train Epoch: 9 [168960/251450]\tLoss: 0.0240 (0.0110) \tAcc: 96.88% (98.23%) \tEmb_Norm: 5.10 (6.55)\n",
      "Train Epoch: 9 [169600/251450]\tLoss: 0.0237 (0.0111) \tAcc: 100.00% (98.23%) \tEmb_Norm: 5.10 (6.54)\n",
      "Train Epoch: 9 [170240/251450]\tLoss: 0.0424 (0.0111) \tAcc: 90.62% (98.23%) \tEmb_Norm: 5.09 (6.54)\n",
      "Train Epoch: 9 [170880/251450]\tLoss: 0.0060 (0.0111) \tAcc: 100.00% (98.23%) \tEmb_Norm: 5.08 (6.53)\n",
      "Train Epoch: 9 [171520/251450]\tLoss: 0.0039 (0.0111) \tAcc: 100.00% (98.23%) \tEmb_Norm: 5.07 (6.53)\n",
      "Train Epoch: 9 [172160/251450]\tLoss: 0.0000 (0.0111) \tAcc: 100.00% (98.23%) \tEmb_Norm: 5.06 (6.52)\n",
      "Train Epoch: 9 [172800/251450]\tLoss: 0.0125 (0.0111) \tAcc: 96.88% (98.23%) \tEmb_Norm: 5.05 (6.52)\n",
      "Train Epoch: 9 [173440/251450]\tLoss: 0.0223 (0.0111) \tAcc: 93.75% (98.23%) \tEmb_Norm: 5.04 (6.51)\n",
      "Train Epoch: 9 [174080/251450]\tLoss: 0.0000 (0.0111) \tAcc: 100.00% (98.23%) \tEmb_Norm: 5.04 (6.51)\n",
      "Train Epoch: 9 [174720/251450]\tLoss: 0.0008 (0.0111) \tAcc: 100.00% (98.24%) \tEmb_Norm: 5.03 (6.50)\n",
      "Train Epoch: 9 [175360/251450]\tLoss: 0.0031 (0.0111) \tAcc: 100.00% (98.24%) \tEmb_Norm: 5.02 (6.50)\n",
      "Train Epoch: 9 [176000/251450]\tLoss: 0.0208 (0.0111) \tAcc: 96.88% (98.24%) \tEmb_Norm: 5.01 (6.49)\n",
      "Train Epoch: 9 [176640/251450]\tLoss: 0.0074 (0.0111) \tAcc: 96.88% (98.24%) \tEmb_Norm: 5.01 (6.49)\n",
      "Train Epoch: 9 [177280/251450]\tLoss: 0.0203 (0.0111) \tAcc: 96.88% (98.23%) \tEmb_Norm: 5.00 (6.48)\n",
      "Train Epoch: 9 [177920/251450]\tLoss: 0.0041 (0.0111) \tAcc: 100.00% (98.24%) \tEmb_Norm: 5.00 (6.47)\n",
      "Train Epoch: 9 [178560/251450]\tLoss: 0.0059 (0.0111) \tAcc: 100.00% (98.24%) \tEmb_Norm: 4.99 (6.47)\n",
      "Train Epoch: 9 [179200/251450]\tLoss: 0.0166 (0.0111) \tAcc: 96.88% (98.24%) \tEmb_Norm: 4.98 (6.46)\n",
      "Train Epoch: 9 [179840/251450]\tLoss: 0.0000 (0.0111) \tAcc: 100.00% (98.24%) \tEmb_Norm: 4.98 (6.46)\n",
      "Train Epoch: 9 [180480/251450]\tLoss: 0.0020 (0.0111) \tAcc: 100.00% (98.24%) \tEmb_Norm: 4.97 (6.45)\n",
      "Train Epoch: 9 [181120/251450]\tLoss: 0.0031 (0.0111) \tAcc: 100.00% (98.24%) \tEmb_Norm: 4.96 (6.45)\n",
      "Train Epoch: 9 [181760/251450]\tLoss: 0.0308 (0.0111) \tAcc: 96.88% (98.24%) \tEmb_Norm: 4.95 (6.44)\n",
      "Train Epoch: 9 [182400/251450]\tLoss: 0.0048 (0.0111) \tAcc: 100.00% (98.23%) \tEmb_Norm: 4.94 (6.44)\n",
      "Train Epoch: 9 [183040/251450]\tLoss: 0.0115 (0.0111) \tAcc: 100.00% (98.24%) \tEmb_Norm: 4.93 (6.43)\n",
      "Train Epoch: 9 [183680/251450]\tLoss: 0.0055 (0.0111) \tAcc: 100.00% (98.23%) \tEmb_Norm: 4.92 (6.43)\n",
      "Train Epoch: 9 [184320/251450]\tLoss: 0.0075 (0.0111) \tAcc: 100.00% (98.23%) \tEmb_Norm: 4.92 (6.42)\n",
      "Train Epoch: 9 [184960/251450]\tLoss: 0.0289 (0.0111) \tAcc: 93.75% (98.23%) \tEmb_Norm: 4.91 (6.42)\n",
      "Train Epoch: 9 [185600/251450]\tLoss: 0.0067 (0.0111) \tAcc: 100.00% (98.23%) \tEmb_Norm: 4.91 (6.41)\n",
      "Train Epoch: 9 [186240/251450]\tLoss: 0.0219 (0.0111) \tAcc: 96.88% (98.24%) \tEmb_Norm: 4.90 (6.41)\n",
      "Train Epoch: 9 [186880/251450]\tLoss: 0.0007 (0.0111) \tAcc: 100.00% (98.24%) \tEmb_Norm: 4.89 (6.40)\n",
      "Train Epoch: 9 [187520/251450]\tLoss: 0.0122 (0.0111) \tAcc: 96.88% (98.24%) \tEmb_Norm: 4.89 (6.40)\n",
      "Train Epoch: 9 [188160/251450]\tLoss: 0.0073 (0.0111) \tAcc: 100.00% (98.24%) \tEmb_Norm: 4.88 (6.39)\n",
      "Train Epoch: 9 [188800/251450]\tLoss: 0.0031 (0.0111) \tAcc: 100.00% (98.24%) \tEmb_Norm: 4.87 (6.39)\n",
      "Train Epoch: 9 [189440/251450]\tLoss: 0.0023 (0.0111) \tAcc: 100.00% (98.24%) \tEmb_Norm: 4.87 (6.38)\n",
      "Train Epoch: 9 [190080/251450]\tLoss: 0.0273 (0.0111) \tAcc: 93.75% (98.24%) \tEmb_Norm: 4.86 (6.38)\n",
      "Train Epoch: 9 [190720/251450]\tLoss: 0.0039 (0.0111) \tAcc: 100.00% (98.24%) \tEmb_Norm: 4.85 (6.37)\n",
      "Train Epoch: 9 [191360/251450]\tLoss: 0.0262 (0.0111) \tAcc: 96.88% (98.24%) \tEmb_Norm: 4.85 (6.37)\n",
      "Train Epoch: 9 [192000/251450]\tLoss: 0.0043 (0.0112) \tAcc: 100.00% (98.24%) \tEmb_Norm: 4.84 (6.36)\n",
      "Train Epoch: 9 [192640/251450]\tLoss: 0.0113 (0.0112) \tAcc: 96.88% (98.24%) \tEmb_Norm: 4.84 (6.36)\n",
      "Train Epoch: 9 [193280/251450]\tLoss: 0.0101 (0.0112) \tAcc: 100.00% (98.24%) \tEmb_Norm: 4.83 (6.35)\n",
      "Train Epoch: 9 [193920/251450]\tLoss: 0.0133 (0.0112) \tAcc: 100.00% (98.24%) \tEmb_Norm: 4.82 (6.34)\n",
      "Train Epoch: 9 [194560/251450]\tLoss: 0.0090 (0.0112) \tAcc: 96.88% (98.24%) \tEmb_Norm: 4.81 (6.34)\n",
      "Train Epoch: 9 [195200/251450]\tLoss: 0.0134 (0.0112) \tAcc: 100.00% (98.24%) \tEmb_Norm: 4.81 (6.33)\n",
      "Train Epoch: 9 [195840/251450]\tLoss: 0.0046 (0.0112) \tAcc: 100.00% (98.24%) \tEmb_Norm: 4.80 (6.33)\n",
      "Train Epoch: 9 [196480/251450]\tLoss: 0.0115 (0.0112) \tAcc: 100.00% (98.24%) \tEmb_Norm: 4.79 (6.32)\n",
      "Train Epoch: 9 [197120/251450]\tLoss: 0.0037 (0.0112) \tAcc: 100.00% (98.24%) \tEmb_Norm: 4.78 (6.32)\n",
      "Train Epoch: 9 [197760/251450]\tLoss: 0.0387 (0.0112) \tAcc: 90.62% (98.24%) \tEmb_Norm: 4.78 (6.31)\n",
      "Train Epoch: 9 [198400/251450]\tLoss: 0.0050 (0.0112) \tAcc: 100.00% (98.24%) \tEmb_Norm: 4.77 (6.31)\n",
      "Train Epoch: 9 [199040/251450]\tLoss: 0.0028 (0.0112) \tAcc: 100.00% (98.24%) \tEmb_Norm: 4.76 (6.31)\n",
      "Train Epoch: 9 [199680/251450]\tLoss: 0.0049 (0.0112) \tAcc: 100.00% (98.24%) \tEmb_Norm: 4.75 (6.30)\n",
      "Train Epoch: 9 [200320/251450]\tLoss: 0.0094 (0.0112) \tAcc: 100.00% (98.25%) \tEmb_Norm: 4.75 (6.30)\n",
      "Train Epoch: 9 [200960/251450]\tLoss: 0.0096 (0.0112) \tAcc: 100.00% (98.25%) \tEmb_Norm: 4.75 (6.29)\n",
      "Train Epoch: 9 [201600/251450]\tLoss: 0.0102 (0.0112) \tAcc: 100.00% (98.25%) \tEmb_Norm: 4.74 (6.29)\n",
      "=============== TEST epoch 9 ===============\n",
      "\n",
      "Test set: Average loss: 0.0191, Accuracy: 96.82%\n",
      "\n",
      "=============== checkpoint epoch 9 ===============\n",
      "=============== TRAIN epoch 10 ===============\n",
      "Train Epoch: 10 [0/251450]\tLoss: 0.0106 (0.0106) \tAcc: 100.00% (100.00%) \tEmb_Norm: 4.73 (4.73)\n",
      "Train Epoch: 10 [640/251450]\tLoss: 0.0050 (0.0112) \tAcc: 100.00% (98.66%) \tEmb_Norm: 4.73 (4.73)\n",
      "Train Epoch: 10 [1280/251450]\tLoss: 0.0139 (0.0134) \tAcc: 100.00% (98.25%) \tEmb_Norm: 4.73 (4.73)\n",
      "Train Epoch: 10 [1920/251450]\tLoss: 0.0266 (0.0131) \tAcc: 93.75% (98.05%) \tEmb_Norm: 4.72 (4.73)\n",
      "Train Epoch: 10 [2560/251450]\tLoss: 0.0027 (0.0132) \tAcc: 100.00% (98.03%) \tEmb_Norm: 4.71 (4.72)\n",
      "Train Epoch: 10 [3200/251450]\tLoss: 0.0457 (0.0129) \tAcc: 93.75% (98.11%) \tEmb_Norm: 4.71 (4.72)\n",
      "Train Epoch: 10 [3840/251450]\tLoss: 0.0089 (0.0128) \tAcc: 100.00% (98.09%) \tEmb_Norm: 4.70 (4.72)\n",
      "Train Epoch: 10 [4480/251450]\tLoss: 0.0018 (0.0123) \tAcc: 100.00% (98.27%) \tEmb_Norm: 4.69 (4.72)\n",
      "Train Epoch: 10 [5120/251450]\tLoss: 0.0070 (0.0123) \tAcc: 96.88% (98.23%) \tEmb_Norm: 4.68 (4.71)\n",
      "Train Epoch: 10 [5760/251450]\tLoss: 0.0143 (0.0125) \tAcc: 100.00% (98.22%) \tEmb_Norm: 4.68 (4.71)\n",
      "Train Epoch: 10 [6400/251450]\tLoss: 0.0006 (0.0122) \tAcc: 100.00% (98.26%) \tEmb_Norm: 4.68 (4.71)\n",
      "Train Epoch: 10 [7040/251450]\tLoss: 0.0064 (0.0123) \tAcc: 100.00% (98.23%) \tEmb_Norm: 4.67 (4.70)\n",
      "Train Epoch: 10 [7680/251450]\tLoss: 0.0158 (0.0124) \tAcc: 96.88% (98.25%) \tEmb_Norm: 4.67 (4.70)\n",
      "Train Epoch: 10 [8320/251450]\tLoss: 0.0273 (0.0122) \tAcc: 93.75% (98.28%) \tEmb_Norm: 4.66 (4.70)\n",
      "Train Epoch: 10 [8960/251450]\tLoss: 0.0042 (0.0124) \tAcc: 100.00% (98.24%) \tEmb_Norm: 4.66 (4.69)\n",
      "Train Epoch: 10 [9600/251450]\tLoss: 0.0138 (0.0122) \tAcc: 96.88% (98.29%) \tEmb_Norm: 4.65 (4.69)\n",
      "Train Epoch: 10 [10240/251450]\tLoss: 0.0082 (0.0122) \tAcc: 100.00% (98.31%) \tEmb_Norm: 4.65 (4.69)\n",
      "Train Epoch: 10 [10880/251450]\tLoss: 0.0041 (0.0124) \tAcc: 100.00% (98.25%) \tEmb_Norm: 4.64 (4.69)\n",
      "Train Epoch: 10 [11520/251450]\tLoss: 0.0385 (0.0124) \tAcc: 90.62% (98.28%) \tEmb_Norm: 4.64 (4.68)\n",
      "Train Epoch: 10 [12160/251450]\tLoss: 0.0128 (0.0123) \tAcc: 96.88% (98.25%) \tEmb_Norm: 4.63 (4.68)\n",
      "Train Epoch: 10 [12800/251450]\tLoss: 0.0135 (0.0122) \tAcc: 100.00% (98.30%) \tEmb_Norm: 4.62 (4.68)\n",
      "Train Epoch: 10 [13440/251450]\tLoss: 0.0035 (0.0120) \tAcc: 100.00% (98.34%) \tEmb_Norm: 4.62 (4.68)\n",
      "Train Epoch: 10 [14080/251450]\tLoss: 0.0241 (0.0120) \tAcc: 100.00% (98.36%) \tEmb_Norm: 4.61 (4.67)\n",
      "Train Epoch: 10 [14720/251450]\tLoss: 0.0105 (0.0118) \tAcc: 100.00% (98.40%) \tEmb_Norm: 4.60 (4.67)\n",
      "Train Epoch: 10 [15360/251450]\tLoss: 0.0104 (0.0119) \tAcc: 100.00% (98.38%) \tEmb_Norm: 4.60 (4.67)\n",
      "Train Epoch: 10 [16000/251450]\tLoss: 0.0041 (0.0119) \tAcc: 100.00% (98.37%) \tEmb_Norm: 4.59 (4.66)\n",
      "Train Epoch: 10 [16640/251450]\tLoss: 0.0090 (0.0120) \tAcc: 100.00% (98.37%) \tEmb_Norm: 4.59 (4.66)\n",
      "Train Epoch: 10 [17280/251450]\tLoss: 0.0026 (0.0120) \tAcc: 100.00% (98.37%) \tEmb_Norm: 4.58 (4.66)\n",
      "Train Epoch: 10 [17920/251450]\tLoss: 0.0046 (0.0119) \tAcc: 100.00% (98.37%) \tEmb_Norm: 4.58 (4.66)\n",
      "Train Epoch: 10 [18560/251450]\tLoss: 0.0147 (0.0119) \tAcc: 96.88% (98.38%) \tEmb_Norm: 4.57 (4.65)\n",
      "Train Epoch: 10 [19200/251450]\tLoss: 0.0179 (0.0121) \tAcc: 100.00% (98.31%) \tEmb_Norm: 4.56 (4.65)\n",
      "Train Epoch: 10 [19840/251450]\tLoss: 0.0052 (0.0121) \tAcc: 100.00% (98.33%) \tEmb_Norm: 4.56 (4.65)\n",
      "Train Epoch: 10 [20480/251450]\tLoss: 0.0280 (0.0121) \tAcc: 93.75% (98.33%) \tEmb_Norm: 4.56 (4.64)\n",
      "Train Epoch: 10 [21120/251450]\tLoss: 0.0071 (0.0120) \tAcc: 96.88% (98.35%) \tEmb_Norm: 4.55 (4.64)\n",
      "Train Epoch: 10 [21760/251450]\tLoss: 0.0137 (0.0119) \tAcc: 100.00% (98.37%) \tEmb_Norm: 4.54 (4.64)\n",
      "Train Epoch: 10 [22400/251450]\tLoss: 0.0070 (0.0119) \tAcc: 100.00% (98.38%) \tEmb_Norm: 4.54 (4.64)\n",
      "Train Epoch: 10 [23040/251450]\tLoss: 0.0061 (0.0119) \tAcc: 100.00% (98.40%) \tEmb_Norm: 4.54 (4.63)\n",
      "Train Epoch: 10 [23680/251450]\tLoss: 0.0026 (0.0118) \tAcc: 100.00% (98.41%) \tEmb_Norm: 4.53 (4.63)\n",
      "Train Epoch: 10 [24320/251450]\tLoss: 0.0278 (0.0118) \tAcc: 93.75% (98.41%) \tEmb_Norm: 4.53 (4.63)\n",
      "Train Epoch: 10 [24960/251450]\tLoss: 0.0017 (0.0119) \tAcc: 100.00% (98.40%) \tEmb_Norm: 4.52 (4.63)\n",
      "Train Epoch: 10 [25600/251450]\tLoss: 0.0139 (0.0119) \tAcc: 96.88% (98.38%) \tEmb_Norm: 4.52 (4.62)\n",
      "Train Epoch: 10 [26240/251450]\tLoss: 0.0425 (0.0120) \tAcc: 90.62% (98.37%) \tEmb_Norm: 4.51 (4.62)\n",
      "Train Epoch: 10 [26880/251450]\tLoss: 0.0078 (0.0120) \tAcc: 96.88% (98.38%) \tEmb_Norm: 4.51 (4.62)\n",
      "Train Epoch: 10 [27520/251450]\tLoss: 0.0059 (0.0120) \tAcc: 100.00% (98.38%) \tEmb_Norm: 4.50 (4.61)\n",
      "Train Epoch: 10 [28160/251450]\tLoss: 0.0108 (0.0119) \tAcc: 100.00% (98.39%) \tEmb_Norm: 4.50 (4.61)\n",
      "Train Epoch: 10 [28800/251450]\tLoss: 0.0181 (0.0119) \tAcc: 96.88% (98.40%) \tEmb_Norm: 4.49 (4.61)\n",
      "Train Epoch: 10 [29440/251450]\tLoss: 0.0122 (0.0118) \tAcc: 100.00% (98.41%) \tEmb_Norm: 4.48 (4.61)\n",
      "Train Epoch: 10 [30080/251450]\tLoss: 0.0280 (0.0119) \tAcc: 96.88% (98.38%) \tEmb_Norm: 4.47 (4.60)\n",
      "Train Epoch: 10 [30720/251450]\tLoss: 0.0042 (0.0119) \tAcc: 100.00% (98.40%) \tEmb_Norm: 4.47 (4.60)\n",
      "Train Epoch: 10 [31360/251450]\tLoss: 0.0127 (0.0119) \tAcc: 96.88% (98.38%) \tEmb_Norm: 4.46 (4.60)\n",
      "Train Epoch: 10 [32000/251450]\tLoss: 0.0049 (0.0119) \tAcc: 100.00% (98.38%) \tEmb_Norm: 4.46 (4.60)\n",
      "Train Epoch: 10 [32640/251450]\tLoss: 0.0113 (0.0118) \tAcc: 100.00% (98.39%) \tEmb_Norm: 4.45 (4.59)\n",
      "Train Epoch: 10 [33280/251450]\tLoss: 0.0179 (0.0119) \tAcc: 96.88% (98.38%) \tEmb_Norm: 4.45 (4.59)\n",
      "Train Epoch: 10 [33920/251450]\tLoss: 0.0192 (0.0120) \tAcc: 100.00% (98.36%) \tEmb_Norm: 4.45 (4.59)\n",
      "Train Epoch: 10 [34560/251450]\tLoss: 0.0344 (0.0121) \tAcc: 93.75% (98.35%) \tEmb_Norm: 4.45 (4.59)\n",
      "Train Epoch: 10 [35200/251450]\tLoss: 0.0000 (0.0121) \tAcc: 100.00% (98.36%) \tEmb_Norm: 4.44 (4.58)\n",
      "Train Epoch: 10 [35840/251450]\tLoss: 0.0190 (0.0120) \tAcc: 93.75% (98.35%) \tEmb_Norm: 4.43 (4.58)\n",
      "Train Epoch: 10 [36480/251450]\tLoss: 0.0226 (0.0121) \tAcc: 96.88% (98.34%) \tEmb_Norm: 4.42 (4.58)\n",
      "Train Epoch: 10 [37120/251450]\tLoss: 0.0043 (0.0121) \tAcc: 100.00% (98.34%) \tEmb_Norm: 4.42 (4.57)\n",
      "Train Epoch: 10 [37760/251450]\tLoss: 0.0032 (0.0121) \tAcc: 100.00% (98.34%) \tEmb_Norm: 4.42 (4.57)\n",
      "Train Epoch: 10 [38400/251450]\tLoss: 0.0312 (0.0121) \tAcc: 93.75% (98.34%) \tEmb_Norm: 4.42 (4.57)\n",
      "Train Epoch: 10 [39040/251450]\tLoss: 0.0129 (0.0122) \tAcc: 100.00% (98.33%) \tEmb_Norm: 4.41 (4.57)\n",
      "Train Epoch: 10 [39680/251450]\tLoss: 0.0066 (0.0122) \tAcc: 100.00% (98.33%) \tEmb_Norm: 4.41 (4.56)\n",
      "Train Epoch: 10 [40320/251450]\tLoss: 0.0270 (0.0122) \tAcc: 96.88% (98.32%) \tEmb_Norm: 4.41 (4.56)\n",
      "Train Epoch: 10 [40960/251450]\tLoss: 0.0122 (0.0122) \tAcc: 100.00% (98.33%) \tEmb_Norm: 4.40 (4.56)\n",
      "Train Epoch: 10 [41600/251450]\tLoss: 0.0137 (0.0122) \tAcc: 96.88% (98.34%) \tEmb_Norm: 4.40 (4.56)\n",
      "Train Epoch: 10 [42240/251450]\tLoss: 0.0029 (0.0122) \tAcc: 100.00% (98.34%) \tEmb_Norm: 4.40 (4.55)\n",
      "Train Epoch: 10 [42880/251450]\tLoss: 0.0000 (0.0121) \tAcc: 100.00% (98.34%) \tEmb_Norm: 4.39 (4.55)\n",
      "Train Epoch: 10 [43520/251450]\tLoss: 0.0175 (0.0121) \tAcc: 96.88% (98.35%) \tEmb_Norm: 4.38 (4.55)\n",
      "Train Epoch: 10 [44160/251450]\tLoss: 0.0183 (0.0121) \tAcc: 96.88% (98.34%) \tEmb_Norm: 4.38 (4.55)\n",
      "Train Epoch: 10 [44800/251450]\tLoss: 0.0118 (0.0121) \tAcc: 96.88% (98.34%) \tEmb_Norm: 4.37 (4.54)\n",
      "Train Epoch: 10 [45440/251450]\tLoss: 0.0000 (0.0121) \tAcc: 100.00% (98.35%) \tEmb_Norm: 4.36 (4.54)\n",
      "Train Epoch: 10 [46080/251450]\tLoss: 0.0021 (0.0121) \tAcc: 100.00% (98.36%) \tEmb_Norm: 4.36 (4.54)\n",
      "Train Epoch: 10 [46720/251450]\tLoss: 0.0071 (0.0120) \tAcc: 100.00% (98.36%) \tEmb_Norm: 4.36 (4.54)\n",
      "Train Epoch: 10 [47360/251450]\tLoss: 0.0278 (0.0121) \tAcc: 93.75% (98.36%) \tEmb_Norm: 4.36 (4.53)\n",
      "Train Epoch: 10 [48000/251450]\tLoss: 0.0034 (0.0121) \tAcc: 100.00% (98.36%) \tEmb_Norm: 4.35 (4.53)\n",
      "Train Epoch: 10 [48640/251450]\tLoss: 0.0142 (0.0121) \tAcc: 96.88% (98.35%) \tEmb_Norm: 4.35 (4.53)\n",
      "Train Epoch: 10 [49280/251450]\tLoss: 0.0098 (0.0121) \tAcc: 100.00% (98.35%) \tEmb_Norm: 4.35 (4.53)\n",
      "Train Epoch: 10 [49920/251450]\tLoss: 0.0104 (0.0121) \tAcc: 100.00% (98.35%) \tEmb_Norm: 4.34 (4.53)\n",
      "Train Epoch: 10 [50560/251450]\tLoss: 0.0336 (0.0121) \tAcc: 87.50% (98.35%) \tEmb_Norm: 4.33 (4.52)\n",
      "Train Epoch: 10 [51200/251450]\tLoss: 0.0312 (0.0121) \tAcc: 93.75% (98.34%) \tEmb_Norm: 4.33 (4.52)\n",
      "Train Epoch: 10 [51840/251450]\tLoss: 0.0102 (0.0121) \tAcc: 100.00% (98.35%) \tEmb_Norm: 4.33 (4.52)\n",
      "Train Epoch: 10 [52480/251450]\tLoss: 0.0088 (0.0121) \tAcc: 100.00% (98.35%) \tEmb_Norm: 4.32 (4.52)\n",
      "Train Epoch: 10 [53120/251450]\tLoss: 0.0286 (0.0121) \tAcc: 96.88% (98.36%) \tEmb_Norm: 4.32 (4.51)\n",
      "Train Epoch: 10 [53760/251450]\tLoss: 0.0167 (0.0121) \tAcc: 96.88% (98.36%) \tEmb_Norm: 4.31 (4.51)\n",
      "Train Epoch: 10 [54400/251450]\tLoss: 0.0033 (0.0121) \tAcc: 100.00% (98.36%) \tEmb_Norm: 4.31 (4.51)\n",
      "Train Epoch: 10 [55040/251450]\tLoss: 0.0101 (0.0121) \tAcc: 100.00% (98.37%) \tEmb_Norm: 4.31 (4.51)\n",
      "Train Epoch: 10 [55680/251450]\tLoss: 0.0079 (0.0121) \tAcc: 100.00% (98.37%) \tEmb_Norm: 4.30 (4.50)\n",
      "Train Epoch: 10 [56320/251450]\tLoss: 0.0047 (0.0121) \tAcc: 100.00% (98.38%) \tEmb_Norm: 4.30 (4.50)\n",
      "Train Epoch: 10 [56960/251450]\tLoss: 0.0281 (0.0121) \tAcc: 96.88% (98.38%) \tEmb_Norm: 4.29 (4.50)\n",
      "Train Epoch: 10 [57600/251450]\tLoss: 0.0165 (0.0121) \tAcc: 96.88% (98.37%) \tEmb_Norm: 4.29 (4.50)\n",
      "Train Epoch: 10 [58240/251450]\tLoss: 0.0015 (0.0121) \tAcc: 100.00% (98.38%) \tEmb_Norm: 4.29 (4.49)\n",
      "Train Epoch: 10 [58880/251450]\tLoss: 0.0050 (0.0121) \tAcc: 100.00% (98.37%) \tEmb_Norm: 4.29 (4.49)\n",
      "Train Epoch: 10 [59520/251450]\tLoss: 0.0198 (0.0122) \tAcc: 96.88% (98.36%) \tEmb_Norm: 4.29 (4.49)\n",
      "Train Epoch: 10 [60160/251450]\tLoss: 0.0033 (0.0122) \tAcc: 100.00% (98.35%) \tEmb_Norm: 4.28 (4.49)\n",
      "Train Epoch: 10 [60800/251450]\tLoss: 0.0190 (0.0122) \tAcc: 96.88% (98.35%) \tEmb_Norm: 4.28 (4.49)\n",
      "Train Epoch: 10 [61440/251450]\tLoss: 0.0003 (0.0122) \tAcc: 100.00% (98.35%) \tEmb_Norm: 4.27 (4.48)\n",
      "Train Epoch: 10 [62080/251450]\tLoss: 0.0081 (0.0122) \tAcc: 96.88% (98.35%) \tEmb_Norm: 4.27 (4.48)\n",
      "Train Epoch: 10 [62720/251450]\tLoss: 0.0154 (0.0122) \tAcc: 100.00% (98.36%) \tEmb_Norm: 4.26 (4.48)\n",
      "Train Epoch: 10 [63360/251450]\tLoss: 0.0070 (0.0122) \tAcc: 100.00% (98.36%) \tEmb_Norm: 4.26 (4.48)\n",
      "Train Epoch: 10 [64000/251450]\tLoss: 0.0228 (0.0122) \tAcc: 93.75% (98.37%) \tEmb_Norm: 4.26 (4.47)\n",
      "Train Epoch: 10 [64640/251450]\tLoss: 0.0261 (0.0122) \tAcc: 93.75% (98.36%) \tEmb_Norm: 4.25 (4.47)\n",
      "Train Epoch: 10 [65280/251450]\tLoss: 0.0141 (0.0122) \tAcc: 96.88% (98.36%) \tEmb_Norm: 4.25 (4.47)\n",
      "Train Epoch: 10 [65920/251450]\tLoss: 0.0063 (0.0122) \tAcc: 100.00% (98.35%) \tEmb_Norm: 4.24 (4.47)\n",
      "Train Epoch: 10 [66560/251450]\tLoss: 0.0033 (0.0122) \tAcc: 100.00% (98.35%) \tEmb_Norm: 4.24 (4.47)\n",
      "Train Epoch: 10 [67200/251450]\tLoss: 0.0038 (0.0122) \tAcc: 100.00% (98.35%) \tEmb_Norm: 4.23 (4.46)\n",
      "Train Epoch: 10 [67840/251450]\tLoss: 0.0112 (0.0122) \tAcc: 96.88% (98.35%) \tEmb_Norm: 4.23 (4.46)\n",
      "Train Epoch: 10 [68480/251450]\tLoss: 0.0134 (0.0122) \tAcc: 96.88% (98.35%) \tEmb_Norm: 4.23 (4.46)\n",
      "Train Epoch: 10 [69120/251450]\tLoss: 0.0087 (0.0122) \tAcc: 96.88% (98.35%) \tEmb_Norm: 4.22 (4.46)\n",
      "Train Epoch: 10 [69760/251450]\tLoss: 0.0000 (0.0122) \tAcc: 100.00% (98.35%) \tEmb_Norm: 4.22 (4.46)\n",
      "Train Epoch: 10 [70400/251450]\tLoss: 0.0207 (0.0122) \tAcc: 96.88% (98.35%) \tEmb_Norm: 4.22 (4.45)\n",
      "Train Epoch: 10 [71040/251450]\tLoss: 0.0069 (0.0121) \tAcc: 100.00% (98.35%) \tEmb_Norm: 4.21 (4.45)\n",
      "Train Epoch: 10 [71680/251450]\tLoss: 0.0044 (0.0121) \tAcc: 100.00% (98.35%) \tEmb_Norm: 4.21 (4.45)\n",
      "Train Epoch: 10 [72320/251450]\tLoss: 0.0093 (0.0121) \tAcc: 100.00% (98.36%) \tEmb_Norm: 4.20 (4.45)\n",
      "Train Epoch: 10 [72960/251450]\tLoss: 0.0025 (0.0121) \tAcc: 100.00% (98.36%) \tEmb_Norm: 4.20 (4.44)\n",
      "Train Epoch: 10 [73600/251450]\tLoss: 0.0064 (0.0121) \tAcc: 100.00% (98.36%) \tEmb_Norm: 4.19 (4.44)\n",
      "Train Epoch: 10 [74240/251450]\tLoss: 0.0054 (0.0121) \tAcc: 100.00% (98.37%) \tEmb_Norm: 4.19 (4.44)\n",
      "Train Epoch: 10 [74880/251450]\tLoss: 0.0082 (0.0121) \tAcc: 100.00% (98.36%) \tEmb_Norm: 4.19 (4.44)\n",
      "Train Epoch: 10 [75520/251450]\tLoss: 0.0025 (0.0121) \tAcc: 100.00% (98.37%) \tEmb_Norm: 4.19 (4.44)\n",
      "Train Epoch: 10 [76160/251450]\tLoss: 0.0058 (0.0121) \tAcc: 100.00% (98.36%) \tEmb_Norm: 4.18 (4.43)\n",
      "Train Epoch: 10 [76800/251450]\tLoss: 0.0024 (0.0121) \tAcc: 100.00% (98.37%) \tEmb_Norm: 4.17 (4.43)\n",
      "Train Epoch: 10 [77440/251450]\tLoss: 0.0085 (0.0121) \tAcc: 100.00% (98.37%) \tEmb_Norm: 4.17 (4.43)\n",
      "Train Epoch: 10 [78080/251450]\tLoss: 0.0047 (0.0121) \tAcc: 100.00% (98.37%) \tEmb_Norm: 4.17 (4.43)\n",
      "Train Epoch: 10 [78720/251450]\tLoss: 0.0160 (0.0121) \tAcc: 96.88% (98.37%) \tEmb_Norm: 4.17 (4.43)\n",
      "Train Epoch: 10 [79360/251450]\tLoss: 0.0216 (0.0121) \tAcc: 100.00% (98.37%) \tEmb_Norm: 4.16 (4.42)\n",
      "Train Epoch: 10 [80000/251450]\tLoss: 0.0061 (0.0121) \tAcc: 100.00% (98.37%) \tEmb_Norm: 4.16 (4.42)\n",
      "Train Epoch: 10 [80640/251450]\tLoss: 0.0112 (0.0121) \tAcc: 100.00% (98.37%) \tEmb_Norm: 4.16 (4.42)\n",
      "Train Epoch: 10 [81280/251450]\tLoss: 0.0009 (0.0121) \tAcc: 100.00% (98.37%) \tEmb_Norm: 4.15 (4.42)\n",
      "Train Epoch: 10 [81920/251450]\tLoss: 0.0158 (0.0121) \tAcc: 96.88% (98.37%) \tEmb_Norm: 4.15 (4.41)\n",
      "Train Epoch: 10 [82560/251450]\tLoss: 0.0178 (0.0121) \tAcc: 100.00% (98.38%) \tEmb_Norm: 4.14 (4.41)\n",
      "Train Epoch: 10 [83200/251450]\tLoss: 0.0081 (0.0121) \tAcc: 100.00% (98.38%) \tEmb_Norm: 4.14 (4.41)\n",
      "Train Epoch: 10 [83840/251450]\tLoss: 0.0028 (0.0120) \tAcc: 100.00% (98.38%) \tEmb_Norm: 4.14 (4.41)\n",
      "Train Epoch: 10 [84480/251450]\tLoss: 0.0150 (0.0121) \tAcc: 100.00% (98.38%) \tEmb_Norm: 4.13 (4.41)\n",
      "Train Epoch: 10 [85120/251450]\tLoss: 0.0160 (0.0120) \tAcc: 96.88% (98.39%) \tEmb_Norm: 4.13 (4.40)\n",
      "Train Epoch: 10 [85760/251450]\tLoss: 0.0066 (0.0120) \tAcc: 100.00% (98.39%) \tEmb_Norm: 4.12 (4.40)\n",
      "Train Epoch: 10 [86400/251450]\tLoss: 0.0068 (0.0120) \tAcc: 100.00% (98.39%) \tEmb_Norm: 4.12 (4.40)\n",
      "Train Epoch: 10 [87040/251450]\tLoss: 0.0132 (0.0121) \tAcc: 96.88% (98.38%) \tEmb_Norm: 4.12 (4.40)\n",
      "Train Epoch: 10 [87680/251450]\tLoss: 0.0047 (0.0121) \tAcc: 100.00% (98.38%) \tEmb_Norm: 4.12 (4.40)\n",
      "Train Epoch: 10 [88320/251450]\tLoss: 0.0291 (0.0121) \tAcc: 96.88% (98.38%) \tEmb_Norm: 4.12 (4.39)\n",
      "Train Epoch: 10 [88960/251450]\tLoss: 0.0101 (0.0121) \tAcc: 100.00% (98.39%) \tEmb_Norm: 4.12 (4.39)\n",
      "Train Epoch: 10 [89600/251450]\tLoss: 0.0173 (0.0120) \tAcc: 96.88% (98.39%) \tEmb_Norm: 4.11 (4.39)\n",
      "Train Epoch: 10 [90240/251450]\tLoss: 0.0083 (0.0121) \tAcc: 96.88% (98.39%) \tEmb_Norm: 4.12 (4.39)\n",
      "Train Epoch: 10 [90880/251450]\tLoss: 0.0135 (0.0121) \tAcc: 96.88% (98.39%) \tEmb_Norm: 4.11 (4.39)\n",
      "Train Epoch: 10 [91520/251450]\tLoss: 0.0162 (0.0121) \tAcc: 96.88% (98.39%) \tEmb_Norm: 4.11 (4.38)\n",
      "Train Epoch: 10 [92160/251450]\tLoss: 0.0079 (0.0121) \tAcc: 100.00% (98.39%) \tEmb_Norm: 4.11 (4.38)\n",
      "Train Epoch: 10 [92800/251450]\tLoss: 0.0058 (0.0121) \tAcc: 100.00% (98.39%) \tEmb_Norm: 4.11 (4.38)\n",
      "Train Epoch: 10 [93440/251450]\tLoss: 0.0174 (0.0121) \tAcc: 100.00% (98.39%) \tEmb_Norm: 4.11 (4.38)\n",
      "Train Epoch: 10 [94080/251450]\tLoss: 0.0130 (0.0121) \tAcc: 96.88% (98.39%) \tEmb_Norm: 4.10 (4.38)\n",
      "Train Epoch: 10 [94720/251450]\tLoss: 0.0099 (0.0121) \tAcc: 100.00% (98.39%) \tEmb_Norm: 4.10 (4.38)\n",
      "Train Epoch: 10 [95360/251450]\tLoss: 0.0173 (0.0121) \tAcc: 96.88% (98.39%) \tEmb_Norm: 4.10 (4.37)\n",
      "Train Epoch: 10 [96000/251450]\tLoss: 0.0000 (0.0121) \tAcc: 100.00% (98.40%) \tEmb_Norm: 4.09 (4.37)\n",
      "Train Epoch: 10 [96640/251450]\tLoss: 0.0076 (0.0121) \tAcc: 100.00% (98.40%) \tEmb_Norm: 4.09 (4.37)\n",
      "Train Epoch: 10 [97280/251450]\tLoss: 0.0218 (0.0121) \tAcc: 100.00% (98.39%) \tEmb_Norm: 4.09 (4.37)\n",
      "Train Epoch: 10 [97920/251450]\tLoss: 0.0066 (0.0121) \tAcc: 100.00% (98.40%) \tEmb_Norm: 4.08 (4.37)\n",
      "Train Epoch: 10 [98560/251450]\tLoss: 0.0303 (0.0121) \tAcc: 93.75% (98.39%) \tEmb_Norm: 4.08 (4.36)\n",
      "Train Epoch: 10 [99200/251450]\tLoss: 0.0338 (0.0121) \tAcc: 90.62% (98.39%) \tEmb_Norm: 4.08 (4.36)\n",
      "Train Epoch: 10 [99840/251450]\tLoss: 0.0341 (0.0121) \tAcc: 93.75% (98.39%) \tEmb_Norm: 4.08 (4.36)\n",
      "Train Epoch: 10 [100480/251450]\tLoss: 0.0006 (0.0121) \tAcc: 100.00% (98.40%) \tEmb_Norm: 4.07 (4.36)\n",
      "Train Epoch: 10 [101120/251450]\tLoss: 0.0080 (0.0121) \tAcc: 100.00% (98.39%) \tEmb_Norm: 4.07 (4.36)\n",
      "Train Epoch: 10 [101760/251450]\tLoss: 0.0235 (0.0121) \tAcc: 93.75% (98.39%) \tEmb_Norm: 4.07 (4.36)\n",
      "Train Epoch: 10 [102400/251450]\tLoss: 0.0000 (0.0121) \tAcc: 100.00% (98.40%) \tEmb_Norm: 4.07 (4.35)\n",
      "Train Epoch: 10 [103040/251450]\tLoss: 0.0185 (0.0121) \tAcc: 96.88% (98.40%) \tEmb_Norm: 4.06 (4.35)\n",
      "Train Epoch: 10 [103680/251450]\tLoss: 0.0037 (0.0121) \tAcc: 100.00% (98.40%) \tEmb_Norm: 4.06 (4.35)\n",
      "Train Epoch: 10 [104320/251450]\tLoss: 0.0275 (0.0121) \tAcc: 96.88% (98.40%) \tEmb_Norm: 4.06 (4.35)\n",
      "Train Epoch: 10 [104960/251450]\tLoss: 0.0108 (0.0121) \tAcc: 100.00% (98.39%) \tEmb_Norm: 4.07 (4.35)\n",
      "Train Epoch: 10 [105600/251450]\tLoss: 0.0039 (0.0121) \tAcc: 100.00% (98.39%) \tEmb_Norm: 4.07 (4.34)\n",
      "Train Epoch: 10 [106240/251450]\tLoss: 0.0101 (0.0121) \tAcc: 100.00% (98.39%) \tEmb_Norm: 4.07 (4.34)\n",
      "Train Epoch: 10 [106880/251450]\tLoss: 0.0191 (0.0121) \tAcc: 93.75% (98.39%) \tEmb_Norm: 4.07 (4.34)\n",
      "Train Epoch: 10 [107520/251450]\tLoss: 0.0224 (0.0122) \tAcc: 96.88% (98.38%) \tEmb_Norm: 4.07 (4.34)\n",
      "Train Epoch: 10 [108160/251450]\tLoss: 0.0000 (0.0122) \tAcc: 100.00% (98.37%) \tEmb_Norm: 4.07 (4.34)\n",
      "Train Epoch: 10 [108800/251450]\tLoss: 0.0101 (0.0122) \tAcc: 100.00% (98.37%) \tEmb_Norm: 4.06 (4.34)\n",
      "Train Epoch: 10 [109440/251450]\tLoss: 0.0085 (0.0121) \tAcc: 96.88% (98.38%) \tEmb_Norm: 4.06 (4.33)\n",
      "Train Epoch: 10 [110080/251450]\tLoss: 0.0126 (0.0122) \tAcc: 96.88% (98.38%) \tEmb_Norm: 4.06 (4.33)\n",
      "Train Epoch: 10 [110720/251450]\tLoss: 0.0111 (0.0122) \tAcc: 100.00% (98.37%) \tEmb_Norm: 4.05 (4.33)\n",
      "Train Epoch: 10 [111360/251450]\tLoss: 0.0271 (0.0122) \tAcc: 96.88% (98.38%) \tEmb_Norm: 4.05 (4.33)\n",
      "Train Epoch: 10 [112000/251450]\tLoss: 0.0187 (0.0122) \tAcc: 96.88% (98.38%) \tEmb_Norm: 4.04 (4.33)\n",
      "Train Epoch: 10 [112640/251450]\tLoss: 0.0044 (0.0122) \tAcc: 100.00% (98.38%) \tEmb_Norm: 4.04 (4.33)\n",
      "Train Epoch: 10 [113280/251450]\tLoss: 0.0006 (0.0122) \tAcc: 100.00% (98.38%) \tEmb_Norm: 4.04 (4.33)\n",
      "Train Epoch: 10 [113920/251450]\tLoss: 0.0048 (0.0122) \tAcc: 100.00% (98.38%) \tEmb_Norm: 4.03 (4.32)\n",
      "Train Epoch: 10 [114560/251450]\tLoss: 0.0096 (0.0122) \tAcc: 96.88% (98.38%) \tEmb_Norm: 4.03 (4.32)\n",
      "Train Epoch: 10 [115200/251450]\tLoss: 0.0060 (0.0122) \tAcc: 100.00% (98.38%) \tEmb_Norm: 4.03 (4.32)\n",
      "Train Epoch: 10 [115840/251450]\tLoss: 0.0011 (0.0121) \tAcc: 100.00% (98.38%) \tEmb_Norm: 4.03 (4.32)\n",
      "Train Epoch: 10 [116480/251450]\tLoss: 0.0046 (0.0122) \tAcc: 100.00% (98.38%) \tEmb_Norm: 4.02 (4.32)\n",
      "Train Epoch: 10 [117120/251450]\tLoss: 0.0012 (0.0121) \tAcc: 100.00% (98.38%) \tEmb_Norm: 4.02 (4.32)\n",
      "Train Epoch: 10 [117760/251450]\tLoss: 0.0108 (0.0122) \tAcc: 100.00% (98.38%) \tEmb_Norm: 4.02 (4.31)\n",
      "Train Epoch: 10 [118400/251450]\tLoss: 0.0165 (0.0122) \tAcc: 96.88% (98.38%) \tEmb_Norm: 4.02 (4.31)\n",
      "Train Epoch: 10 [119040/251450]\tLoss: 0.0086 (0.0122) \tAcc: 96.88% (98.37%) \tEmb_Norm: 4.01 (4.31)\n",
      "Train Epoch: 10 [119680/251450]\tLoss: 0.0207 (0.0122) \tAcc: 93.75% (98.38%) \tEmb_Norm: 4.01 (4.31)\n",
      "Train Epoch: 10 [120320/251450]\tLoss: 0.0054 (0.0122) \tAcc: 100.00% (98.38%) \tEmb_Norm: 4.01 (4.31)\n",
      "Train Epoch: 10 [120960/251450]\tLoss: 0.0080 (0.0122) \tAcc: 100.00% (98.38%) \tEmb_Norm: 4.01 (4.31)\n",
      "Train Epoch: 10 [121600/251450]\tLoss: 0.0025 (0.0121) \tAcc: 100.00% (98.38%) \tEmb_Norm: 4.01 (4.30)\n",
      "Train Epoch: 10 [122240/251450]\tLoss: 0.0164 (0.0122) \tAcc: 100.00% (98.38%) \tEmb_Norm: 4.01 (4.30)\n",
      "Train Epoch: 10 [122880/251450]\tLoss: 0.0117 (0.0122) \tAcc: 96.88% (98.38%) \tEmb_Norm: 4.01 (4.30)\n",
      "Train Epoch: 10 [123520/251450]\tLoss: 0.0333 (0.0122) \tAcc: 90.62% (98.38%) \tEmb_Norm: 4.00 (4.30)\n",
      "Train Epoch: 10 [124160/251450]\tLoss: 0.0097 (0.0122) \tAcc: 96.88% (98.37%) \tEmb_Norm: 4.00 (4.30)\n",
      "Train Epoch: 10 [124800/251450]\tLoss: 0.0122 (0.0122) \tAcc: 96.88% (98.38%) \tEmb_Norm: 4.00 (4.30)\n",
      "Train Epoch: 10 [125440/251450]\tLoss: 0.0203 (0.0122) \tAcc: 100.00% (98.38%) \tEmb_Norm: 4.00 (4.30)\n",
      "Train Epoch: 10 [126080/251450]\tLoss: 0.0373 (0.0122) \tAcc: 87.50% (98.37%) \tEmb_Norm: 3.99 (4.29)\n",
      "Train Epoch: 10 [126720/251450]\tLoss: 0.0052 (0.0122) \tAcc: 100.00% (98.37%) \tEmb_Norm: 3.99 (4.29)\n",
      "Train Epoch: 10 [127360/251450]\tLoss: 0.0108 (0.0122) \tAcc: 100.00% (98.37%) \tEmb_Norm: 3.99 (4.29)\n",
      "Train Epoch: 10 [128000/251450]\tLoss: 0.0139 (0.0122) \tAcc: 100.00% (98.37%) \tEmb_Norm: 3.99 (4.29)\n",
      "Train Epoch: 10 [128640/251450]\tLoss: 0.0230 (0.0122) \tAcc: 93.75% (98.36%) \tEmb_Norm: 3.98 (4.29)\n",
      "Train Epoch: 10 [129280/251450]\tLoss: 0.0035 (0.0122) \tAcc: 100.00% (98.36%) \tEmb_Norm: 3.98 (4.29)\n",
      "Train Epoch: 10 [129920/251450]\tLoss: 0.0035 (0.0122) \tAcc: 100.00% (98.36%) \tEmb_Norm: 3.98 (4.28)\n",
      "Train Epoch: 10 [130560/251450]\tLoss: 0.0212 (0.0122) \tAcc: 96.88% (98.36%) \tEmb_Norm: 3.98 (4.28)\n",
      "Train Epoch: 10 [131200/251450]\tLoss: 0.0035 (0.0122) \tAcc: 100.00% (98.35%) \tEmb_Norm: 3.97 (4.28)\n",
      "Train Epoch: 10 [131840/251450]\tLoss: 0.0054 (0.0123) \tAcc: 100.00% (98.35%) \tEmb_Norm: 3.97 (4.28)\n",
      "Train Epoch: 10 [132480/251450]\tLoss: 0.0113 (0.0123) \tAcc: 96.88% (98.34%) \tEmb_Norm: 3.97 (4.28)\n",
      "Train Epoch: 10 [133120/251450]\tLoss: 0.0051 (0.0123) \tAcc: 100.00% (98.34%) \tEmb_Norm: 3.97 (4.28)\n",
      "Train Epoch: 10 [133760/251450]\tLoss: 0.0009 (0.0123) \tAcc: 100.00% (98.34%) \tEmb_Norm: 3.96 (4.28)\n",
      "Train Epoch: 10 [134400/251450]\tLoss: 0.0064 (0.0123) \tAcc: 100.00% (98.34%) \tEmb_Norm: 3.96 (4.27)\n",
      "Train Epoch: 10 [135040/251450]\tLoss: 0.0183 (0.0123) \tAcc: 96.88% (98.34%) \tEmb_Norm: 3.96 (4.27)\n",
      "Train Epoch: 10 [135680/251450]\tLoss: 0.0191 (0.0123) \tAcc: 96.88% (98.34%) \tEmb_Norm: 3.96 (4.27)\n",
      "Train Epoch: 10 [136320/251450]\tLoss: 0.0212 (0.0123) \tAcc: 96.88% (98.34%) \tEmb_Norm: 3.96 (4.27)\n",
      "Train Epoch: 10 [136960/251450]\tLoss: 0.0063 (0.0123) \tAcc: 100.00% (98.33%) \tEmb_Norm: 3.96 (4.27)\n",
      "Train Epoch: 10 [137600/251450]\tLoss: 0.0125 (0.0123) \tAcc: 96.88% (98.34%) \tEmb_Norm: 3.95 (4.27)\n",
      "Train Epoch: 10 [138240/251450]\tLoss: 0.0125 (0.0123) \tAcc: 100.00% (98.33%) \tEmb_Norm: 3.95 (4.27)\n",
      "Train Epoch: 10 [138880/251450]\tLoss: 0.0090 (0.0123) \tAcc: 96.88% (98.33%) \tEmb_Norm: 3.95 (4.26)\n",
      "Train Epoch: 10 [139520/251450]\tLoss: 0.0184 (0.0123) \tAcc: 100.00% (98.33%) \tEmb_Norm: 3.95 (4.26)\n",
      "Train Epoch: 10 [140160/251450]\tLoss: 0.0297 (0.0123) \tAcc: 93.75% (98.34%) \tEmb_Norm: 3.95 (4.26)\n",
      "Train Epoch: 10 [140800/251450]\tLoss: 0.0333 (0.0123) \tAcc: 96.88% (98.34%) \tEmb_Norm: 3.95 (4.26)\n",
      "Train Epoch: 10 [141440/251450]\tLoss: 0.0023 (0.0123) \tAcc: 100.00% (98.34%) \tEmb_Norm: 3.95 (4.26)\n",
      "Train Epoch: 10 [142080/251450]\tLoss: 0.0107 (0.0123) \tAcc: 96.88% (98.34%) \tEmb_Norm: 3.95 (4.26)\n",
      "Train Epoch: 10 [142720/251450]\tLoss: 0.0058 (0.0123) \tAcc: 100.00% (98.33%) \tEmb_Norm: 3.95 (4.26)\n",
      "Train Epoch: 10 [143360/251450]\tLoss: 0.0045 (0.0123) \tAcc: 100.00% (98.34%) \tEmb_Norm: 3.95 (4.25)\n",
      "Train Epoch: 10 [144000/251450]\tLoss: 0.0108 (0.0123) \tAcc: 96.88% (98.34%) \tEmb_Norm: 3.95 (4.25)\n",
      "Train Epoch: 10 [144640/251450]\tLoss: 0.0097 (0.0123) \tAcc: 96.88% (98.34%) \tEmb_Norm: 3.95 (4.25)\n",
      "Train Epoch: 10 [145280/251450]\tLoss: 0.0199 (0.0123) \tAcc: 100.00% (98.34%) \tEmb_Norm: 3.95 (4.25)\n",
      "Train Epoch: 10 [145920/251450]\tLoss: 0.0177 (0.0123) \tAcc: 100.00% (98.33%) \tEmb_Norm: 3.94 (4.25)\n",
      "Train Epoch: 10 [146560/251450]\tLoss: 0.0061 (0.0123) \tAcc: 100.00% (98.34%) \tEmb_Norm: 3.94 (4.25)\n",
      "Train Epoch: 10 [147200/251450]\tLoss: 0.0082 (0.0123) \tAcc: 100.00% (98.34%) \tEmb_Norm: 3.94 (4.25)\n",
      "Train Epoch: 10 [147840/251450]\tLoss: 0.0106 (0.0123) \tAcc: 100.00% (98.33%) \tEmb_Norm: 3.94 (4.24)\n",
      "Train Epoch: 10 [148480/251450]\tLoss: 0.0305 (0.0123) \tAcc: 96.88% (98.33%) \tEmb_Norm: 3.94 (4.24)\n",
      "Train Epoch: 10 [149120/251450]\tLoss: 0.0165 (0.0123) \tAcc: 100.00% (98.33%) \tEmb_Norm: 3.94 (4.24)\n",
      "Train Epoch: 10 [149760/251450]\tLoss: 0.0137 (0.0123) \tAcc: 96.88% (98.34%) \tEmb_Norm: 3.94 (4.24)\n",
      "Train Epoch: 10 [150400/251450]\tLoss: 0.0135 (0.0123) \tAcc: 100.00% (98.34%) \tEmb_Norm: 3.94 (4.24)\n",
      "Train Epoch: 10 [151040/251450]\tLoss: 0.0111 (0.0124) \tAcc: 100.00% (98.33%) \tEmb_Norm: 3.94 (4.24)\n",
      "Train Epoch: 10 [151680/251450]\tLoss: 0.0136 (0.0124) \tAcc: 96.88% (98.34%) \tEmb_Norm: 3.94 (4.24)\n",
      "Train Epoch: 10 [152320/251450]\tLoss: 0.0146 (0.0124) \tAcc: 96.88% (98.33%) \tEmb_Norm: 3.93 (4.24)\n",
      "Train Epoch: 10 [152960/251450]\tLoss: 0.0246 (0.0124) \tAcc: 93.75% (98.33%) \tEmb_Norm: 3.93 (4.23)\n",
      "Train Epoch: 10 [153600/251450]\tLoss: 0.0009 (0.0124) \tAcc: 100.00% (98.33%) \tEmb_Norm: 3.93 (4.23)\n",
      "Train Epoch: 10 [154240/251450]\tLoss: 0.0184 (0.0124) \tAcc: 100.00% (98.33%) \tEmb_Norm: 3.93 (4.23)\n",
      "Train Epoch: 10 [154880/251450]\tLoss: 0.0160 (0.0124) \tAcc: 100.00% (98.32%) \tEmb_Norm: 3.93 (4.23)\n",
      "Train Epoch: 10 [155520/251450]\tLoss: 0.0168 (0.0124) \tAcc: 96.88% (98.33%) \tEmb_Norm: 3.93 (4.23)\n",
      "Train Epoch: 10 [156160/251450]\tLoss: 0.0074 (0.0124) \tAcc: 100.00% (98.33%) \tEmb_Norm: 3.92 (4.23)\n",
      "Train Epoch: 10 [156800/251450]\tLoss: 0.0189 (0.0124) \tAcc: 100.00% (98.33%) \tEmb_Norm: 3.92 (4.23)\n",
      "Train Epoch: 10 [157440/251450]\tLoss: 0.0012 (0.0124) \tAcc: 100.00% (98.33%) \tEmb_Norm: 3.92 (4.23)\n",
      "Train Epoch: 10 [158080/251450]\tLoss: 0.0030 (0.0124) \tAcc: 100.00% (98.33%) \tEmb_Norm: 3.92 (4.22)\n",
      "Train Epoch: 10 [158720/251450]\tLoss: 0.0005 (0.0124) \tAcc: 100.00% (98.33%) \tEmb_Norm: 3.91 (4.22)\n",
      "Train Epoch: 10 [159360/251450]\tLoss: 0.0141 (0.0124) \tAcc: 100.00% (98.33%) \tEmb_Norm: 3.92 (4.22)\n",
      "Train Epoch: 10 [160000/251450]\tLoss: 0.0254 (0.0124) \tAcc: 100.00% (98.33%) \tEmb_Norm: 3.91 (4.22)\n",
      "Train Epoch: 10 [160640/251450]\tLoss: 0.0042 (0.0124) \tAcc: 100.00% (98.33%) \tEmb_Norm: 3.91 (4.22)\n",
      "Train Epoch: 10 [161280/251450]\tLoss: 0.0306 (0.0124) \tAcc: 93.75% (98.33%) \tEmb_Norm: 3.91 (4.22)\n",
      "Train Epoch: 10 [161920/251450]\tLoss: 0.0226 (0.0124) \tAcc: 96.88% (98.33%) \tEmb_Norm: 3.91 (4.22)\n",
      "Train Epoch: 10 [162560/251450]\tLoss: 0.0056 (0.0124) \tAcc: 100.00% (98.33%) \tEmb_Norm: 3.91 (4.22)\n",
      "Train Epoch: 10 [163200/251450]\tLoss: 0.0033 (0.0124) \tAcc: 100.00% (98.33%) \tEmb_Norm: 3.90 (4.21)\n",
      "Train Epoch: 10 [163840/251450]\tLoss: 0.0000 (0.0124) \tAcc: 100.00% (98.33%) \tEmb_Norm: 3.90 (4.21)\n",
      "Train Epoch: 10 [164480/251450]\tLoss: 0.0092 (0.0124) \tAcc: 100.00% (98.34%) \tEmb_Norm: 3.90 (4.21)\n",
      "Train Epoch: 10 [165120/251450]\tLoss: 0.0058 (0.0124) \tAcc: 100.00% (98.34%) \tEmb_Norm: 3.90 (4.21)\n",
      "Train Epoch: 10 [165760/251450]\tLoss: 0.0085 (0.0124) \tAcc: 96.88% (98.34%) \tEmb_Norm: 3.90 (4.21)\n",
      "Train Epoch: 10 [166400/251450]\tLoss: 0.0041 (0.0124) \tAcc: 100.00% (98.34%) \tEmb_Norm: 3.89 (4.21)\n",
      "Train Epoch: 10 [167040/251450]\tLoss: 0.0053 (0.0124) \tAcc: 100.00% (98.33%) \tEmb_Norm: 3.89 (4.21)\n",
      "Train Epoch: 10 [167680/251450]\tLoss: 0.0018 (0.0124) \tAcc: 100.00% (98.34%) \tEmb_Norm: 3.89 (4.21)\n",
      "Train Epoch: 10 [168320/251450]\tLoss: 0.0140 (0.0124) \tAcc: 100.00% (98.34%) \tEmb_Norm: 3.89 (4.20)\n",
      "Train Epoch: 10 [168960/251450]\tLoss: 0.0315 (0.0124) \tAcc: 90.62% (98.33%) \tEmb_Norm: 3.89 (4.20)\n",
      "Train Epoch: 10 [169600/251450]\tLoss: 0.0181 (0.0124) \tAcc: 100.00% (98.33%) \tEmb_Norm: 3.90 (4.20)\n",
      "Train Epoch: 10 [170240/251450]\tLoss: 0.0079 (0.0124) \tAcc: 100.00% (98.33%) \tEmb_Norm: 3.89 (4.20)\n",
      "Train Epoch: 10 [170880/251450]\tLoss: 0.0011 (0.0124) \tAcc: 100.00% (98.33%) \tEmb_Norm: 3.89 (4.20)\n",
      "Train Epoch: 10 [171520/251450]\tLoss: 0.0087 (0.0124) \tAcc: 100.00% (98.33%) \tEmb_Norm: 3.89 (4.20)\n",
      "Train Epoch: 10 [172160/251450]\tLoss: 0.0060 (0.0124) \tAcc: 100.00% (98.33%) \tEmb_Norm: 3.90 (4.20)\n",
      "Train Epoch: 10 [172800/251450]\tLoss: 0.0082 (0.0124) \tAcc: 100.00% (98.34%) \tEmb_Norm: 3.89 (4.20)\n",
      "Train Epoch: 10 [173440/251450]\tLoss: 0.0117 (0.0124) \tAcc: 100.00% (98.34%) \tEmb_Norm: 3.89 (4.20)\n",
      "Train Epoch: 10 [174080/251450]\tLoss: 0.0023 (0.0124) \tAcc: 100.00% (98.34%) \tEmb_Norm: 3.89 (4.19)\n",
      "Train Epoch: 10 [174720/251450]\tLoss: 0.0145 (0.0124) \tAcc: 96.88% (98.34%) \tEmb_Norm: 3.89 (4.19)\n",
      "Train Epoch: 10 [175360/251450]\tLoss: 0.0053 (0.0124) \tAcc: 100.00% (98.34%) \tEmb_Norm: 3.89 (4.19)\n",
      "Train Epoch: 10 [176000/251450]\tLoss: 0.0095 (0.0124) \tAcc: 100.00% (98.34%) \tEmb_Norm: 3.89 (4.19)\n",
      "Train Epoch: 10 [176640/251450]\tLoss: 0.0136 (0.0124) \tAcc: 100.00% (98.34%) \tEmb_Norm: 3.90 (4.19)\n",
      "Train Epoch: 10 [177280/251450]\tLoss: 0.0000 (0.0124) \tAcc: 100.00% (98.34%) \tEmb_Norm: 3.90 (4.19)\n",
      "Train Epoch: 10 [177920/251450]\tLoss: 0.0223 (0.0124) \tAcc: 93.75% (98.34%) \tEmb_Norm: 3.89 (4.19)\n",
      "Train Epoch: 10 [178560/251450]\tLoss: 0.0075 (0.0124) \tAcc: 100.00% (98.34%) \tEmb_Norm: 3.89 (4.19)\n",
      "Train Epoch: 10 [179200/251450]\tLoss: 0.0105 (0.0124) \tAcc: 100.00% (98.34%) \tEmb_Norm: 3.89 (4.19)\n",
      "Train Epoch: 10 [179840/251450]\tLoss: 0.0043 (0.0124) \tAcc: 100.00% (98.34%) \tEmb_Norm: 3.89 (4.18)\n",
      "Train Epoch: 10 [180480/251450]\tLoss: 0.0000 (0.0124) \tAcc: 100.00% (98.34%) \tEmb_Norm: 3.88 (4.18)\n",
      "Train Epoch: 10 [181120/251450]\tLoss: 0.0196 (0.0124) \tAcc: 93.75% (98.34%) \tEmb_Norm: 3.88 (4.18)\n",
      "Train Epoch: 10 [181760/251450]\tLoss: 0.0007 (0.0124) \tAcc: 100.00% (98.34%) \tEmb_Norm: 3.87 (4.18)\n",
      "Train Epoch: 10 [182400/251450]\tLoss: 0.0111 (0.0124) \tAcc: 96.88% (98.34%) \tEmb_Norm: 3.87 (4.18)\n",
      "Train Epoch: 10 [183040/251450]\tLoss: 0.0044 (0.0124) \tAcc: 100.00% (98.34%) \tEmb_Norm: 3.87 (4.18)\n",
      "Train Epoch: 10 [183680/251450]\tLoss: 0.0012 (0.0124) \tAcc: 100.00% (98.33%) \tEmb_Norm: 3.87 (4.18)\n",
      "Train Epoch: 10 [184320/251450]\tLoss: 0.0095 (0.0124) \tAcc: 100.00% (98.33%) \tEmb_Norm: 3.87 (4.18)\n",
      "Train Epoch: 10 [184960/251450]\tLoss: 0.0163 (0.0124) \tAcc: 96.88% (98.33%) \tEmb_Norm: 3.87 (4.18)\n",
      "Train Epoch: 10 [185600/251450]\tLoss: 0.0315 (0.0124) \tAcc: 96.88% (98.33%) \tEmb_Norm: 3.87 (4.18)\n",
      "Train Epoch: 10 [186240/251450]\tLoss: 0.0080 (0.0125) \tAcc: 100.00% (98.33%) \tEmb_Norm: 3.87 (4.17)\n",
      "Train Epoch: 10 [186880/251450]\tLoss: 0.0122 (0.0125) \tAcc: 100.00% (98.33%) \tEmb_Norm: 3.87 (4.17)\n",
      "Train Epoch: 10 [187520/251450]\tLoss: 0.0074 (0.0124) \tAcc: 100.00% (98.33%) \tEmb_Norm: 3.87 (4.17)\n",
      "Train Epoch: 10 [188160/251450]\tLoss: 0.0196 (0.0124) \tAcc: 93.75% (98.33%) \tEmb_Norm: 3.86 (4.17)\n",
      "Train Epoch: 10 [188800/251450]\tLoss: 0.0040 (0.0124) \tAcc: 100.00% (98.33%) \tEmb_Norm: 3.86 (4.17)\n",
      "Train Epoch: 10 [189440/251450]\tLoss: 0.0242 (0.0124) \tAcc: 96.88% (98.33%) \tEmb_Norm: 3.85 (4.17)\n",
      "Train Epoch: 10 [190080/251450]\tLoss: 0.0330 (0.0124) \tAcc: 90.62% (98.34%) \tEmb_Norm: 3.85 (4.17)\n",
      "Train Epoch: 10 [190720/251450]\tLoss: 0.0174 (0.0125) \tAcc: 96.88% (98.33%) \tEmb_Norm: 3.85 (4.17)\n",
      "Train Epoch: 10 [191360/251450]\tLoss: 0.0382 (0.0125) \tAcc: 90.62% (98.33%) \tEmb_Norm: 3.85 (4.17)\n",
      "Train Epoch: 10 [192000/251450]\tLoss: 0.0278 (0.0125) \tAcc: 93.75% (98.33%) \tEmb_Norm: 3.86 (4.16)\n",
      "Train Epoch: 10 [192640/251450]\tLoss: 0.0172 (0.0125) \tAcc: 96.88% (98.33%) \tEmb_Norm: 3.85 (4.16)\n",
      "Train Epoch: 10 [193280/251450]\tLoss: 0.0174 (0.0125) \tAcc: 100.00% (98.33%) \tEmb_Norm: 3.86 (4.16)\n",
      "Train Epoch: 10 [193920/251450]\tLoss: 0.0086 (0.0125) \tAcc: 100.00% (98.33%) \tEmb_Norm: 3.86 (4.16)\n",
      "Train Epoch: 10 [194560/251450]\tLoss: 0.0078 (0.0125) \tAcc: 100.00% (98.33%) \tEmb_Norm: 3.86 (4.16)\n",
      "Train Epoch: 10 [195200/251450]\tLoss: 0.0128 (0.0125) \tAcc: 96.88% (98.33%) \tEmb_Norm: 3.86 (4.16)\n",
      "Train Epoch: 10 [195840/251450]\tLoss: 0.0091 (0.0125) \tAcc: 100.00% (98.33%) \tEmb_Norm: 3.86 (4.16)\n",
      "Train Epoch: 10 [196480/251450]\tLoss: 0.0044 (0.0125) \tAcc: 100.00% (98.33%) \tEmb_Norm: 3.86 (4.16)\n",
      "Train Epoch: 10 [197120/251450]\tLoss: 0.0279 (0.0125) \tAcc: 93.75% (98.33%) \tEmb_Norm: 3.86 (4.16)\n",
      "Train Epoch: 10 [197760/251450]\tLoss: 0.0083 (0.0125) \tAcc: 100.00% (98.33%) \tEmb_Norm: 3.86 (4.16)\n",
      "Train Epoch: 10 [198400/251450]\tLoss: 0.0167 (0.0125) \tAcc: 100.00% (98.33%) \tEmb_Norm: 3.86 (4.15)\n",
      "Train Epoch: 10 [199040/251450]\tLoss: 0.0152 (0.0125) \tAcc: 96.88% (98.33%) \tEmb_Norm: 3.86 (4.15)\n",
      "Train Epoch: 10 [199680/251450]\tLoss: 0.0111 (0.0125) \tAcc: 100.00% (98.33%) \tEmb_Norm: 3.85 (4.15)\n",
      "Train Epoch: 10 [200320/251450]\tLoss: 0.0147 (0.0125) \tAcc: 100.00% (98.33%) \tEmb_Norm: 3.86 (4.15)\n",
      "Train Epoch: 10 [200960/251450]\tLoss: 0.0182 (0.0125) \tAcc: 96.88% (98.33%) \tEmb_Norm: 3.86 (4.15)\n",
      "Train Epoch: 10 [201600/251450]\tLoss: 0.0114 (0.0125) \tAcc: 100.00% (98.33%) \tEmb_Norm: 3.86 (4.15)\n",
      "=============== TEST epoch 10 ===============\n",
      "\n",
      "Test set: Average loss: 0.0207, Accuracy: 96.82%\n",
      "\n",
      "=============== checkpoint epoch 10 ===============\n"
     ]
    }
   ],
   "source": [
    "train_history = []\n",
    "val_history = []\n",
    "for epoch in range(1, 10+1):\n",
    "    print('=============== TRAIN epoch {} ==============='.format(epoch))\n",
    "    # train for one epoch\n",
    "    train_loss_avg = train(train_loader, tnet, criterion, optimizer, epoch)\n",
    "    train_history.append(train_loss_avg)\n",
    "\n",
    "    print('=============== TEST epoch {} ==============='.format(epoch))\n",
    "    # evaluate on validation set\n",
    "    acc, val_loss_avg = test(val_loader, tnet, criterion, epoch) # val\n",
    "    val_history.append(val_loss_avg)\n",
    "\n",
    "    print('=============== checkpoint epoch {} ==============='.format(epoch))\n",
    "    # remember best acc and save checkpoint\n",
    "    is_best = acc > best_acc\n",
    "    best_acc = max(acc, best_acc)\n",
    "    save_checkpoint({\n",
    "        'epoch': epoch + 1,\n",
    "        'state_dict': tnet.state_dict(),\n",
    "        'best_prec1': best_acc,\n",
    "    }, is_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "colab_type": "code",
    "id": "N2wakWQm86QK",
    "outputId": "1f054464-b455-4dde-a02f-d386f7a43a47"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD4CAYAAADiry33AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjIsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy8li6FKAAAgAElEQVR4nO3deXxU1f3/8dcnISSCLGFRwyIJLsi+\nBQQRQVEEMaGooJZatFarddefin5bF9oq+nW3tIpVv25FKda6QMENxQUpi4AgKKuy71BWgXB+f5wJ\nWZiEhMzkTmbez8djHkzu3LnzmfHhe86ce+455pxDRETiV1LQBYiISHQp6EVE4pyCXkQkzinoRUTi\nnIJeRCTOVQu6gOIaNGjgMjMzgy5DRKRKmTlz5kbnXMNwj8Vc0GdmZjJjxoygyxARqVLM7IeSHlPX\njYhInFPQi4jEOQW9iEici7k+ehGJX/v27WPlypXs2bMn6FKqrLS0NJo0aUJKSkqZn6OgF5FKs3Ll\nSmrVqkVmZiZmFnQ5VY5zjk2bNrFy5UqysrLK/Dx13YhIpdmzZw/169dXyB8hM6N+/frl/kWkoBeR\nSqWQr5gj+fziJuh//BGGD4fVq4OuREQktsRN0O/YAQ89BO++G3QlIhKrtm7dyl/+8pcjeu55553H\n1q1by7z/fffdxyOPPHJErxVpcRP0LVtC8+YKehEpWWlBv3///lKfO2HCBOrWrRuNsqIuboLeDHJy\n4KOPYNeuoKsRkVg0fPhwlixZQocOHbj99tv55JNP6NmzJ7m5ubRq1QqAn/3sZ3Tu3JnWrVszevTo\ng8/NzMxk48aNLF++nJYtW3LVVVfRunVr+vbty+7du0t93dmzZ9OtWzfatWvHoEGD2LJlCwBPPfUU\nrVq1ol27dlxyySUAfPrpp3To0IEOHTrQsWNHtm/fXuH3HVfDK3Ny4Mkn4cMPITc36GpEpDQ33wyz\nZ0f2mB06wBNPlPz4yJEjmTdvHrNDL/zJJ58wa9Ys5s2bd3C44gsvvEC9evXYvXs3Xbp04cILL6R+\n/fpFjrNo0SLGjBnDc889x5AhQ3jzzTf5xS9+UeLr/vKXv+Tpp5+mV69e3HPPPdx///088cQTjBw5\nkmXLlpGamnqwW+iRRx5h1KhR9OjRgx07dpCWllbBTyWOWvQAPXtC7drwzjtBVyIiVUXXrl2LjEl/\n6qmnaN++Pd26dWPFihUsWrTokOdkZWXRoUMHADp37szy5ctLPP62bdvYunUrvXr1AmDYsGFMmTIF\ngHbt2jF06FBeffVVqlXz7e4ePXpw66238tRTT7F169aD2ysirlr01atDv37w3ntw4AAkxdXXmEh8\nKa3lXZlq1qx58P4nn3zChx9+yNSpU6lRowa9e/cOO2Y9NTX14P3k5OTDdt2UZPz48UyZMoV3332X\nP/3pT3zzzTcMHz6cAQMGMGHCBHr06MGkSZM45ZRTjuj4+eIuCnNzYd060EzHIlJcrVq1Su3z3rZt\nG+np6dSoUYOFCxfy1VdfVfg169SpQ3p6Op999hkAr7zyCr169eLAgQOsWLGCM888k4ceeoht27ax\nY8cOlixZQtu2bbnzzjvp0qULCxcurHANcdWiB+jfH5KT/eibrl2DrkZEYkn9+vXp0aMHbdq0oX//\n/gwYMKDI4/369eOZZ56hZcuWtGjRgm7dukXkdV966SWuueYadu3aRfPmzXnxxRfJy8vjF7/4Bdu2\nbcM5x4033kjdunX5/e9/z+TJk0lKSqJ169b079+/wq9vzrkIvI3Iyc7OdhVdeKRXL9i2LfInekSk\nYhYsWEDLli2DLqPKC/c5mtlM51x2uP3L1HVjZv3M7DszW2xmw8M8foaZzTKz/WZ2UbHHhpnZotBt\nWDneyxHLyYE5c/zVsiIiie6wQW9mycAooD/QCrjUzFoV2+1H4HLg78WeWw+4FzgV6Arca2bpFS+7\ndDk5/l9dPCUiUrYWfVdgsXNuqXNuL/A6MLDwDs655c65ucCBYs89F/jAObfZObcF+ADoF4G6S9Wi\nBZx0koJeRATKFvSNgRWF/l4Z2lYWZXqumV1tZjPMbMaGDRvKeOjS5ebC5MkQgYvKRESqtJgYXumc\nG+2cy3bOZTds2DAix8zJgb174YMPInI4EZEqqyxBvwpoWujvJqFtZVGR51ZIjx6Qnq7uGxGRsgT9\ndOAkM8sys+rAJUBZJxmYBPQ1s/TQSdi+oW1RV62aH1M/fjzk5VXGK4pIPDr66KMBWL16NRdddFHY\nfXr37k24YeElba9shw1659x+4Hp8QC8Axjrn5pvZCDPLBTCzLma2EhgMPGtm80PP3Qz8Af9lMR0Y\nEdpWKXJyYMMGmDatsl5RROJVo0aNGDduXNBlHJEy9dE75yY45052zp3gnPtTaNs9zrl3QvenO+ea\nOOdqOufqO+daF3ruC865E0O3F6PzNsLr18+37NV9IyLgpykeNWrUwb/zFwfZsWMHffr0oVOnTrRt\n25a33377kOcuX76cNm3aALB7924uueQSWrZsyaBBg8o0182YMWNo27Ytbdq04c477wQgLy+Pyy+/\nnDZt2tC2bVsef/xxIPz0xRURd1MgFFa3Lpxxhg/6Bx8MuhoRKSKAeYovvvhibr75Zq677joAxo4d\ny6RJk0hLS+Ott96idu3abNy4kW7dupGbm1vi+qx//etfqVGjBgsWLGDu3Ll06tSp1LJWr17NnXfe\nycyZM0lPT6dv377861//omnTpqxatYp58+YBHJyqONz0xRURE6NuoiknB+bPh2XLgq5ERILWsWNH\n1q9fz+rVq5kzZw7p6ek0bdoU5xx333037dq14+yzz2bVqlWsW7euxONMmTLl4Pzz7dq1o127dqW+\n7vTp0+nduzcNGzakWrVqDB06lClTptC8eXOWLl3KDTfcwMSJE6ldu/bBYxafvrgi4rpFDz7ob7nF\nt+pvvDHoakTkoIDmKR48eDDjxo1j7dq1XHzxxQC89tprbNiwgZkzZ5KSkkJmZmbY6YkjLT09nTlz\n5jBp0iSeeeYZxo4dywsvvBB2+uKKBH7ct+hPOMGvJ6t+ehEB333z+uuvM27cOAYPHgz46YmPOeYY\nUlJSmDx5Mj/88EOpxzjjjDP4+9/9jC/z5s1j7ty5pe7ftWtXPv30UzZu3EheXh5jxoyhV69ebNy4\nkQMHDnDhhRfyxz/+kVmzZpU4fXFFxH2LHnyr/rHH/IyWdeoEXY2IBKl169Zs376dxo0bk5GRAcDQ\noUPJycmhbdu2ZGdnH3ahj2uvvZYrrriCli1b0rJlSzp37lzq/hkZGYwcOZIzzzwT5xwDBgxg4MCB\nzJkzhyuuuIIDB/zsMQ8++GCJ0xdXRFxOU1zc55/7ZQbfeAOGDInooUWkHDRNcWREZZriqq57d6hf\nX903IpKYEiLok5NhwACYMAH27w+6GhGRypUQQQ++n37zZpg6NehKRBJbrHUXVzVH8vklTND37Qsp\nKeq+EQlSWloamzZtUtgfIeccmzZtIi0trVzPS4hRNwC1a0Pv3vDOO/Dww0FXI5KYmjRpwsqVK4nU\nuhOJKC0tjSZNmpTrOQkT9OC7b268ERYt8itQiUjlSklJISsrK+gyEk7CdN2A1pIVkcSUUEGfmQlt\n2yroRSSxJFTQg2/Vf/YZbNkSdCUiIpUjIYM+Lw8mTgy6EhGRypFwQd+1KxxzjB99IyKSCBIu6JOS\n/FWy//437NsXdDUiItGXcEEPkJvrZ7L8/POgKxERib6EDPpzzoHUVI2+EZHEkJBBX7MmnHWW76fX\nldgiEu8SMujBj75ZsgQWLgy6EhGR6ErYoD//fP+vum9EJN4lbNA3bQodOijoRST+JWzQgx998+WX\nsGlT0JWIiERPQgd9Tg4cOOBXnhIRiVcJHfSdOkFGhrpvRCS+JXTQJyX5k7ITJ8LevUFXIyISHQkd\n9OC7b7Zvh08/DboSEZHoSPig79MH0tLUfSMi8Svhg75GDT8lwrvv6ipZEYlPCR/04Ltvli+H+fOD\nrkREJPLKFPRm1s/MvjOzxWY2PMzjqWb2RujxaWaWGdqeYmYvmdk3ZrbAzO6KbPmRoatkRSSeHTbo\nzSwZGAX0B1oBl5pZq2K7XQlscc6dCDwOPBTaPhhIdc61BToDv8n/EoglGRmQna3FSEQkPpWlRd8V\nWOycW+qc2wu8Dgwsts9A4KXQ/XFAHzMzwAE1zawacBSwF/hvRCqPsJwcmDYN1q8PuhIRkcgqS9A3\nBlYU+ntlaFvYfZxz+4FtQH186O8E1gA/Ao845zYXfwEzu9rMZpjZjA0bNpT7TURCTo4/GTt+fCAv\nLyISNdE+GdsVyAMaAVnAbWbWvPhOzrnRzrls51x2w4YNo1xSeB06QJMm6qcXkfhTlqBfBTQt9HeT\n0Law+4S6aeoAm4CfAxOdc/ucc+uBL4DsihYdDWa+Vf/++7BnT9DViIhETlmCfjpwkpllmVl14BKg\n+GnLd4BhofsXAR875xy+u+YsADOrCXQDYnapj5wc2LkTPvkk6EpERCLnsEEf6nO/HpgELADGOufm\nm9kIM8sN7fY8UN/MFgO3AvlDMEcBR5vZfPwXxovOubmRfhORcuaZfplBjb4RkXhiLsYuB83OznYz\nZswI7PUHDYKZM+GHH3x3johIVWBmM51zYbvGdWVsMTk5sGIFzJkTdCUiIpGhoC9mwADfktfoGxGJ\nFwr6Yo49Fk49VUEvIvFDQR9GTg5Mnw5r1gRdiYhIxSnow8jJ8f++916wdYiIRIKCPow2baBZM3Xf\niEh8UNCHkX+V7Icfwu7dQVcjIlIxCvoS5Ob6kP/oo6ArERGpGAV9CXr1glq11H0jIlWfgr4E1avD\nuef6E7IHDgRdjYjIkVPQlyInB1avhlmzgq5EROTIKehLcd55kJSk7hsRqdoU9KVo0AC6d1fQi0jV\npqA/jNxc+PprWLky6EpERI6Mgv4wdJWsiFR1CvrDOOUUOOEELUYiIlWXgv4w8q+S/fhjv8ygiEhV\no6Avg5wc+Okn+OCDoCsRESk/BX0Z9OwJdepo9I2IVE0K+jJISYH+/WH8eF0lKyJVj4K+jHJyYN06\nvyCJiEhVoqAvo/79ITlZo29EpOpR0JdRejqcfrr66UWk6lHQl0NODnzzDfzwQ9CViIiUnYK+HPKv\nklWrXkSqEgV9OZx8MrRooaAXkapFQV9OOTkweTL8979BVyIiUjbxE/SbNsGgQfDss7B8edReJicH\n9u2D99+P2kuIiERU/AT90qV+KahrroGsLN/HctNNMGFCRCepOe00PwJH3TciUlXET9B36eJb8gsW\nwBNPQPPm8NxzMGAA1KsH55wDjzzih804d8QvU62aX3lqwgTIy4tc+SIi0RI/QQ9+qslTTvEt+X//\nGzZv9n0s118Pa9bA7bdDu3bQpAn86lfwxhu+y6eccnJg40b46qsovAcRkQgzV4HWbTRkZ2e7GTNm\nROfgK1f64J84ET78ELZs8V8OXbvCuef6W9euvtleim3b/DKDt90GI0dGp1QRkfIws5nOueywjyVU\n0BeWl+cnrpk0yQf/f/7jZyyrWxfOPrsg+Js2Dfv0s8/2PxLmz49+qSIih1Na0Jep68bM+pnZd2a2\n2MyGh3k81czeCD0+zcwyCz3Wzsymmtl8M/vGzNKO9I1EVHIydOsG994LU6fChg0wdixccIH/+6qr\n4PjjoXVruPVW/0tg9+6DT8/JgW+/hSVLAnwPIiJlcNigN7NkYBTQH2gFXGpmrYrtdiWwxTl3IvA4\n8FDoudWAV4FrnHOtgd7AvohVH0n16sHgwfD887BiBcyb50/eNm4Mf/mLb93Xqwf9+sHjj3NBywWA\n0+gbEYl5ZWnRdwUWO+eWOuf2Aq8DA4vtMxB4KXR/HNDHzAzoC8x1zs0BcM5tcs7F/lgVM9+Sv+02\n35LfvNkPs/nNb/xEN7feStNzW7G6WjNOevgqePNN2Lo16KpFRMIqS9A3BlYU+ntlaFvYfZxz+4Ft\nQH3gZMCZ2SQzm2Vmd4R7ATO72sxmmNmMDRs2lPc9RF+NGn6e4iee8MM3ly+HZ59lU/MunL5mLFx0\nkT8726MHjBgBc+cGXbGIyEHRHl5ZDTgdGBr6d5CZ9Sm+k3NutHMu2zmX3bBhwyiXFAHNmsHVV/Pf\nF9+kPpv46P7P4a67/CWz990HHTr44Z0iIjGgLEG/Cig89KRJaFvYfUL98nWATfjW/xTn3Ebn3C5g\nAtCpokXHilNPhfQG1Xjx+x7whz/4kTvr1kHbtnDZZb6vX0QkYGUJ+unASWaWZWbVgUuA4ussvQMM\nC92/CPjY+XGbk4C2ZlYj9AXQC/g2MqUHLznZX3g7YQLs3x/a2LChH73z009wySW+lS8iEqDDBn2o\nz/16fGgvAMY65+ab2Qgzyw3t9jxQ38wWA7cCw0PP3QI8hv+ymA3Mcs6Nj/zbCE5Ojr/u6osvCm1s\n0QJGj4Yvv4Tf/S6w2kREIJEvmIqQ7dv9edgbbvCjMYu45ho/m+Z77/mmv4hIlFT4gikpWa1a0Lt3\nCbNZPv44tG8Pv/wl/PhjZZcmIgIo6CMiNxe+/97fijjqKPjHP3w/vfrrRSQgCvoIOP98/2/YVv1J\nJ/npkqdOhbvvrtS6RERAQR8RzZr52Y/fKT4WKd/FF8O11/pOfM2ZICKVTEEfITk5fuTN5s0l7PDY\nY9CxIwwb5qdREBGpJAr6CMnJ8TMfl3hBbFqaH1+/f79v4e/dW6n1iUjiUtBHSJcucOyxh+mZOfFE\nPzvmtGl+ygQRkUqgoI+QpCR/UnbixMMMrhk8GK67znflvP12pdUnIolLQR9BOTl+mcHPPjvMjo8+\nCp07w+WX+5kwRUSiSEEfQWefDamppYy+yZea6vvrDxxQf72IRJ2CPoJq1oQ+fXw//WFnlmjeHF54\nwc94eeedlVKfiCQmBX2EXXABLF0Kt9/uG+yluvBCuPFGv6DJW29VSn0ikngU9BF2+eX+XOujj/r7\nh5314OGH/ZCdK66AZcsqoUIRSTQK+ghLToann/brkLzyCgwcCDt3lvKE1FR44w1/f8gQP4+9iEgE\nKeijwMxPQz96NEya5PvtN20q5QlZWfDiizBjhu/zERGJIAV9FF11FYwbB7Nnw+mnH2am4kGD4Oab\n/c+BN9+stBpFJP4p6KNs0CDfql+9Gnr0gPnzS9n5oYega1f41a9gyZJKq1FE4puCvhL06gVTpvhp\nbnr29CsMhlW9uu+vT0pSf72IRIyCvpK0b+8Dvn59f2HVe++VsGNmJrz0EsyaBbfdVpklikicUtBX\noqwsP5Vxq1bws5/B//1fCTvm5sKtt8KoUX6FKhGRClDQV7JjjoHJk+HMM/3Q+YceKuEq2pEjoVs3\nuPJKWLy40usUkfihoA9ArVowfrxfRnb4cN9Dc8hVtCkp8PrrUK2a76/fsyeQWkWk6lPQB6R6dXjt\nNbjhBnj8cbjssjBzmzVrBi+/DF9/7btyRESOgII+QElJ8OST8MAD8Pe/+675HTuK7XT++f4iqr/+\nteAKWhGRclDQB8zMLzb1t7/BBx/4q2g3biy205/+BKed5q/AWrQokDpFpOpS0MeIK6/0E1jOneuv\noi2yfnh+f31Kil+havfuwOoUkapHQR9DcnPh/fdh3TrfgJ83r9CDTZv6WdLmzIFbbgmsRhGpehT0\nMaZnT38Vbf79zz8v9OB55/lFSp59FsaMCaQ+Eal6FPQxqG1bfxXtscfCOecUW5rwD3/wk+ZcfTV8\n911gNYpI1aGgj1HNmvnWfLt2fmK0558PPZDfX5+W5sfXq79eRA5DQR/DGjSAjz7yrfpf/xoefDB0\nFW2TJr6/fu5cuOmmoMsUkRinoI9xRx/tu26GDoW77/ZT1h84APTr58dlPvecv/JKRKQEZQp6M+tn\nZt+Z2WIzGx7m8VQzeyP0+DQzyyz2+PFmtsPM/l9kyk4s1av7C2RvuQWeesqH/t69wIgR/oztb34D\nCxcGXaaIxKjDBr2ZJQOjgP5AK+BSM2tVbLcrgS3OuROBx4GHij3+GPDvipebuJKS/ILjDz3ku+jP\nPx+2767mR9/UqOHH1+/aFXSZIhKDytKi7wosds4tdc7tBV4HBhbbZyDwUuj+OKCPmRmAmf0MWAaU\ntraSlIEZ3HGHX17244/hrLNgQ/XG8OqrfumqG24IukQRiUFlCfrGwIpCf68MbQu7j3NuP7ANqG9m\nRwN3AvdXvFTJd/nl8K9/+Wzv0QOWn9zXd+C/8ILv4xERKSTaJ2PvAx53zhWfqqsIM7vazGaY2YwN\nGzZEuaT4cP758OGHfl6c006DuRfc59csvPZa+PbboMsTkRhSlqBfBTQt9HeT0Law+5hZNaAOsAk4\nFXjYzJYDNwN3m9n1xV/AOTfaOZftnMtu2LBhud9EojrtND/WPjkZzjirGl/dNMYP0xk8GHbuDLo8\nEYkRZQn66cBJZpZlZtWBS4B3iu3zDjAsdP8i4GPn9XTOZTrnMoEngAecc3+OUO2CX5bwyy8hIwN6\nX5rBF799DRYsgOsP+T4VkQR12KAP9blfD0wCFgBjnXPzzWyEmeWGdnse3ye/GLgVOGQIpkRP06a+\nZd+hA5wx4mxmDvi9X5B29OgS1ikUkURiLsaCIDs7282YMSPoMqqknTt9r82kf+exJOscMpdNhhNO\n8FMlDBkC7dv7oTsiEnfMbKZzLjvcY7oyNo7UrAlvvw1DL0um9bJ3efmM59jf7AR4+GHo2BFatIDf\n/c5PnRBjX/AiEj0K+jiTkuJ7bW4cXpNhU37N8Qsm8Y+n1+KeHe1nSnvwQd+yb9kS7rkHvvlGoS8S\n5xT0cSgpyef5tGm+/37IbxvQ8+Wr+PrhD2DNGnjmGWjc2C9R2K6dP6N7771+YL6IxB0FfRzr2hWm\nTvXXUS1aBJ07w7X3HsOmi37jp8VcvdovOp6RAX/8I7RpA61bw/33+5E7IhIXdDI2QWzd6vP76aeh\ndm2f61dfDdWqhXZYuxb++U8YO9YvceWcD/7Bg/2J3FNOCbR+ESldaSdjFfQJZv58uPFGP1dO+/Y+\n+Hv2LLbTmjXw5pvwj3/AZ5/50G/btmD0zsknB1K7iJRMo27koNat/dQJ48bBli1wxhnw85/DqsLX\nOmdk+AuuPv0UVq70cyPXqQO//70fudOhAzzwgO8PEpGYp6BPQGZw4YW+G/6ee3yPTYsWMHIk/PRT\nsZ0bNfKzYn72mQ/9J57w4zj/5398y75jR3/md/HiQN6LiByeum6EZcvg1lv9jJgnnuizfMCAwzxp\nxQrfvTN2rD/jC9Cpk+/aGTwYmjePet0iUkBdN1KqrCx46y2YNMmfnD3/fH8rtZHetKlf1/DLL+GH\nH/yqKCkpMHy4vxq3Sxf43/+F5csr622ISAkU9HJQ374wZw488ogfeNO6tZ/mfkepk0wDxx/vfxJ8\n9ZUP9kce8YP577jDf4ucdZY/oIgEQkEvRVSvDrfdBt99B5de6rvfTznFr1hYpl6+Zs38AaZNg6VL\n/QEWLvRz5Z9zTkE3j4hUGgW9hJWR4adS+PJLOO44PzKnd2/f4i+zrCzflbNkCTz2mJ9j57TToH9/\n+M9/olS5iBSnoJdSde/uG+ejR/uFqzp1guuug82by3GQo46CW27xLfyHH4bp0+HUUyEnB77+Omq1\ni4inoJfDSk6Gq66C77+H3/7WT5Vz8snw7LOQl1eOA9WsCbff7of5PPAAfPGF/+a44ALf2heRqFDQ\nS5mlp/srab/+2s+OcM01fnDNF1+U80C1asFdd/nAv//+gst0hwzRerciUaCgl3Jr1w4mT4bXX4cN\nG+D00+Gyy/zMCeVSp46/YmvZMn/V7cSJ/htk6FB/NlhEIkJBL0fEDC6+2A+ouftuf93UySf7ofN7\n95bzYOnpMGKED/w77/RXbrVqBcOG+RO5IlIhCnqpkJo1/bT28+f7UTl33OHnP5s48QgOVr++H465\nbJk/eTt2rJ+b4de/1oVXIhWgoJeIOPFEePddGD/ej7fv3x8GDvQDbcrtmGP8RVdLl/rJ1V59FU46\nyZ8UWLEi4rWLxDsFvUTUeef51QlHjvRrm7Rq5RvnR9TlnpHhJ95ZssRPnv/CC/4b5YYb/KIpIlIm\nCnqJuNRU39X+/fd+IM2f/+yvru3VyzfOd+8u5wEbN4ZRo/zkO5df7sd3nnCC/wZZty4ab0Ekrijo\nJWoaNYKXX/a9LSNH+jnvL7vM5/ZNN8G8eeU84PHH+8H7+fMzPP20v/r2jjv88B8RCUtBL1F33HEF\nLfyPPoJzz/WN8rZt/YwIL74IO3eW44DNm/tunAUL/MT6jz7qA//uu2HTpqi9D5GqSkEvlSYpyU9k\nOWaMb90/+qifSuFXv/Kt/9/+FmbPLscBTzoJXnnF/zTIyfE/G7Ky4N57/SK5IgIo6CUgDRr4mY0X\nLPAzGOfm+kZ6x47+atvnnoPt28t4sJYt/bfH3Ln+58KIEZCZCX/4A/z3v9F8GyJVgoJeAmXmFyd/\n5RU/kObJJ2HPHj/IJiPDz7EzfXoZp0hu08YvaD57th/Uf889voU/cmQZJtUXiV9aSlBijnMFM2a+\n8Qbs2uWnwrnqKj87Qt26ZTzQzJm+G2f8eP8T4uyz4dhjC27HHVdw/5hj/GT8IlVUaUsJKuglpm3b\nBn//uw/92bP9jMdDhvgWf/fu/hfBYU2b5q+4nT/fD8csqU8oPT38l0C4W2pqRN+nSEUp6KXKc843\n0J97zgf/jh3+Yqyrr/ZDNuvVK8fBdu3ygR/utnZt0b9L6uOvW7fkL4HiXxJpaRH5DERKo6CXuLJj\nh58587nn/EJVqalw0UW+a+eMM8rYyi+r3bth/frwXwLFvyC2bQt/jNq1C0I/IwO6dfMnjVu1inCx\nksgU9BK35szxgf/KK77xffLJPvCHDYOGDSu5mD17Cr4USvqVsGJFwQRAjRv7Fdn79vXnDxo0qOSC\nJZ4o6CXu7drlB9yMHu3XuU1JgUGDfOifdZYfwx8zfvwR3n/f3z78ELZs8S37zp196J97rm/16+Sw\nlEOFg97M+gFPAsnA35xzI4s9ngq8DHQGNgEXO+eWm9k5wEigOrAXuN0593Fpr6Wgl4qaP9+38l9+\n2Wdo8+Z+puOhQ/0sCjElL9evnZIAAAnmSURBVA9mzPChP2kSfPWV33b00XDmmT70+/b1k7mpmyd+\nOee7/vbtO+KfohUKejNLBr4HzgFWAtOBS51z3xba57dAO+fcNWZ2CTDIOXexmXUE1jnnVptZG2CS\nc65xaa+noJdI2bMH3nzTh/6nn/ptHTr46ZMHDvT3Yy47t23zSyvmB/+yZX57VlZBa/+ss/zqXBL7\n8vL8PExr1vjb2rUF94tv27MHfv5zeO21I3qpigZ9d+A+59y5ob/vAnDOPVhon0mhfaaaWTVgLdDQ\nFTq4mRm+tZ/hnPuppNdT0Es0LFoEb70F77zju3ac86373Fwf+r16+e6emOKcn6I5P/Q//tifiU5O\n9l07+f37Xbr4bVJ59uwpGtolBfi6dXDgwKHPT0/3o7MyMore2reHPn2OqKSKBv1FQD/n3K9Df18G\nnOqcu77QPvNC+6wM/b0ktM/GYse5xjl3dpjXuBq4GuD444/v/MMPP5TzLYqU3fr18N578PbbPkP3\n7PEN5PPO86Hfr1+MNpj37YOpUwv692fM8F8Gdev6k7n5Lf6Y65+qIpzzZ/TL0vresuXQ5ycl+Qvv\niod38UA/7rioDLkNPOjNrDXwDtDXOVfqIqBq0Utl2rULPvjAh/6778LGjb5l37u3D/3cXGjaNOgq\nS7Bxo58OdNIkH/yrVvntLVoU9O336uX7+8XbssX/Sip8W7rUnyBfsyb8YgmpqaUHd/6tYcNAf1kF\n2nVjZk2Aj4ErnHNfHK5YBb0EJS/PN5jfftvfFi3y2zt1KujXb9cuBvv1wbdGv/22oLX/6ac+tFJS\n4PTTC1r77dvH2BCkCDtwwH/hFQ7xwqFevCV+7LH+bH1mZskt8bp1Y/Q/elEVDfpq+JOxfYBV+JOx\nP3fOzS+0z3VA20InYy9wzg0xs7rAp8D9zrl/lqVYBb3EioULC0L/q698ljZrVhD6PXvGYL9+vj17\n4PPPC1r7c+f67Q0bwjnn+BO6GRm+r7hu3YLbUUcFW3dZ7NnjT1IXD/ElS/z2nwqdAkxO9v/RTjjh\n0Fvz5nH1aycSwyvPA57AD698wTn3JzMbAcxwzr1jZmnAK0BHYDNwiXNuqZn9DrgLWFTocH2dc+tL\nei0FvcSidet8187bb/uh73v2+Fws3K9fu3bQVZZizRrfR5Xf4i9pRa7U1KLBX7fuoV8G4balp/sT\nG5Ea+x+uiyX/tmpV0elMa9YMH+QnnODPV1SrFpmaYpwumBKJoJ07C/r133vPd5VXr+6Hvef36zcu\ndRBxwA4c8IG5ebMP1K1bi95K2rZlC+zfX/qxa9Qo3xdEUlL4lnnxhWOOPbbkMG/YsEp0rUSbgl4k\nSvLy/HDN/C6exYv99uzsgi6eNm3iJIec8/3+h/tCKO3vcEMNwbe6S+tiqVmzct9rFaSgF6kEzvkV\ns/JDf9o0vz0rq6Cl37NnwvQkHMo5fx1A4fDfv9+fCE2gLpZoUdCLBGDt2qL9+j/95Hsuevb0c+l3\n7+6vdapRI+hKJR4o6EUCtmOHPwc6fjx88QV8953fnpzsp2LID/7u3X0DNy66eqRSKehFYsymTX7I\n5tSp/jZtmj/JC/6842mnFQR/585VY9SjBEtBLxLj9u+HefMKgv/LL/3gE/Bj9Tt2LNrqb9pUrX4p\nSkEvUgWtX1/Q6v/yS5g+veAK/UaNirb6O3XSMraJTkEvEgf27fMXuBZu9S9f7h+rXt2HfeHwj+mx\n/BJxCnqROLV2bdHgnzGjYAaApk2LBn+HDlq0KpY5B3v3HvkvMwW9SILYuxdmzy4a/itW+MfS0vyF\nXN27Q9eufnRPo0Z+Zl0NYY+8vDx/8fGGDYfeNm4Mv23IEHj11SN7PQW9SAJbtapo8M+a5b8Q8iUl\n+YkaGzUquDVufOj9evUS+wTw3r0lB3S4MN+8ueQLgevU8TM3FL916QIXXHBk9SnoReSgn37yI3xW\nrfK31asLbvl/b9p06POqVz/0SyDcl0IsTwi5f79fg2DnTn/Lv791a8mBnX/bti38Mc2gfv3wwZ1/\na9Cg6P1odKGVFvT6wSaSYFJT/dj8zp1L3mfPHj/hZbgvgdWrYc4cmDChYOx/YbVqhf8SKPx3Rkb4\nvuiSgjhS9wv/kilJSkrRYM7OLj2869WL/ZUcFfQicoi0ND9HT1ZW6ftt337ol0Dh+59/7v8NF7AN\nGvgpIXbtKl8QF5aU5Oc7q1HD/1v4fr164beHu5/fldKggb8fb11UCnoROWK1avmVC1u0KHkf53xX\nUPEvgdWrfZdJfuiWNZQL309Njb9QjgYFvYhElZlvKTdo4JdilMoXx4tHiogIKOhFROKegl5EJM4p\n6EVE4pyCXkQkzinoRUTinIJeRCTOKehFROJczE1qZmYbgB+CrqOCGgAbgy4ihujzKEqfRwF9FkVV\n5PNo5pxrGO6BmAv6eGBmM0qaRS4R6fMoSp9HAX0WRUXr81DXjYhInFPQi4jEOQV9dIwOuoAYo8+j\nKH0eBfRZFBWVz0N99CIicU4tehGROKegFxGJcwr6CDKzpmY22cy+NbP5ZnZT0DUFzcySzexrM3sv\n6FqCZmZ1zWycmS00swVm1j3omoJkZreE/j+ZZ2ZjzCwt6Joqk5m9YGbrzWxeoW31zOwDM1sU+jc9\nEq+loI+s/cBtzrlWQDfgOjNrFXBNQbsJWBB0ETHiSWCic+4UoD0J/LmYWWPgRiDbOdcGSAYuCbaq\nSvd/QL9i24YDHznnTgI+Cv1dYQr6CHLOrXHOzQrd347/H7lxsFUFx8yaAAOAvwVdS9DMrA5wBvA8\ngHNur3Nua7BVBa4acJSZVQNqAKsDrqdSOeemAJuLbR4IvBS6/xLws0i8loI+SswsE+gITAu2kkA9\nAdwBHAi6kBiQBWwAXgx1Zf3NzGoGXVRQnHOrgEeAH4E1wDbn3PvBVhUTjnXOrQndXwscG4mDKuij\nwMyOBt4EbnbO/TfoeoJgZucD651zM4OuJUZUAzoBf3XOdQR2EqGf5VVRqO95IP4LsBFQ08x+EWxV\nscX5se8RGf+uoI8wM0vBh/xrzrl/Bl1PgHoAuWa2HHgdOMvMXg22pECtBFY65/J/4Y3DB3+iOhtY\n5pzb4JzbB/wTOC3gmmLBOjPLAAj9uz4SB1XQR5CZGb4PdoFz7rGg6wmSc+4u51wT51wm/iTbx865\nhG2xOefWAivMrEVoUx/g2wBLCtqPQDczqxH6/6YPCXxyupB3gGGh+8OAtyNxUAV9ZPUALsO3XmeH\nbucFXZTEjBuA18xsLtABeCDgegIT+mUzDpgFfIPPooSaDsHMxgBTgRZmttLMrgRGAueY2SL8r56R\nEXktTYEgIhLf1KIXEYlzCnoRkTinoBcRiXMKehGROKegFxGJcwp6EZE4p6AXEYlz/x95Q/8T27up\nggAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "epochs = list(range(1,11))\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "plot_train = ax.plot(epochs, train_history, color='b', label='train loss')\n",
    "plot_test  = ax.plot(epochs, val_history, color='r', label='valid loss')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EamqSqdNlf9i"
   },
   "source": [
    "learning rate를 반으로 줄였더니 잘 출력된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8WPT5iMu9BEa"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QeBNZQL51GWr"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "-GrfwtRn13jz"
   ],
   "name": "10_triplet_loss_16dim.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
