{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adversarial AE\n",
    "\n",
    "#### Adversarial ae라고 naming 하기는 하였으나, 기존에 나와있는 vae를 이용한 구조는 아니고,\n",
    "#### 정민정님이 사용한것과 같은 형태의 AE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyreadr as py # library to read .Rdata files in python\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from time import time\n",
    "import pickle\n",
    "import datetime\n",
    "import random\n",
    "import math\n",
    "import csv\n",
    "import time\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import multilabel_confusion_matrix,confusion_matrix,classification_report\n",
    "from sklearn.preprocessing import StandardScaler,Normalizer\n",
    "from sklearn.model_selection import RandomizedSearchCV,GridSearchCV,KFold\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score,f1_score,log_loss,recall_score,classification_report\n",
    "from sklearn.preprocessing import scale, robust_scale, minmax_scale, maxabs_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data.dataset import Dataset  \n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from torchvision.utils import make_grid\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.model import AE_base\n",
    "from src.data.common.dataset import FontDataset, PickledImageProvider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from torch.optim import SGD, Adam\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "validation_split = .15\n",
    "test_split = .05\n",
    "shuffle_dataset = True\n",
    "random_seed = 42\n",
    "\n",
    "lr = 0.0001\n",
    "\n",
    "log_interval = 10\n",
    "epochs = 200\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data.common.dataset import FontDataset, PickledImageProvider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 1000 examples\n",
      "processed 2000 examples\n",
      "processed 3000 examples\n",
      "processed 4000 examples\n",
      "processed 5000 examples\n",
      "processed 6000 examples\n",
      "processed 7000 examples\n",
      "processed 8000 examples\n",
      "processed 9000 examples\n",
      "processed 10000 examples\n",
      "processed 11000 examples\n",
      "processed 12000 examples\n",
      "processed 13000 examples\n",
      "processed 14000 examples\n",
      "processed 15000 examples\n",
      "processed 16000 examples\n",
      "processed 17000 examples\n",
      "processed 18000 examples\n",
      "processed 19000 examples\n",
      "processed 20000 examples\n",
      "processed 21000 examples\n",
      "processed 22000 examples\n",
      "processed 23000 examples\n",
      "processed 24000 examples\n",
      "processed 25000 examples\n",
      "processed 26000 examples\n",
      "processed 27000 examples\n",
      "processed 28000 examples\n",
      "processed 29000 examples\n",
      "processed 30000 examples\n",
      "processed 31000 examples\n",
      "processed 32000 examples\n",
      "processed 33000 examples\n",
      "processed 34000 examples\n",
      "processed 35000 examples\n",
      "processed 36000 examples\n",
      "processed 37000 examples\n",
      "processed 38000 examples\n",
      "processed 39000 examples\n",
      "processed 40000 examples\n",
      "processed 41000 examples\n",
      "processed 42000 examples\n",
      "processed 43000 examples\n",
      "processed 44000 examples\n",
      "processed 45000 examples\n",
      "processed 46000 examples\n",
      "processed 47000 examples\n",
      "processed 48000 examples\n",
      "processed 49000 examples\n",
      "processed 50000 examples\n",
      "processed 51000 examples\n",
      "processed 52000 examples\n",
      "processed 53000 examples\n",
      "processed 54000 examples\n",
      "processed 55000 examples\n",
      "processed 56000 examples\n",
      "processed 57000 examples\n",
      "processed 58000 examples\n",
      "processed 59000 examples\n",
      "processed 60000 examples\n",
      "processed 61000 examples\n",
      "processed 62000 examples\n",
      "processed 63000 examples\n",
      "processed 64000 examples\n",
      "processed 65000 examples\n",
      "processed 66000 examples\n",
      "processed 67000 examples\n",
      "processed 68000 examples\n",
      "processed 69000 examples\n",
      "processed 70000 examples\n",
      "processed 71000 examples\n",
      "processed 72000 examples\n",
      "processed 73000 examples\n",
      "processed 74000 examples\n",
      "processed 75000 examples\n",
      "processed 76000 examples\n",
      "processed 77000 examples\n",
      "processed 78000 examples\n",
      "processed 79000 examples\n",
      "processed 80000 examples\n",
      "processed 81000 examples\n",
      "processed 82000 examples\n",
      "processed 83000 examples\n",
      "processed 84000 examples\n",
      "processed 85000 examples\n",
      "processed 86000 examples\n",
      "processed 87000 examples\n",
      "processed 88000 examples\n",
      "processed 89000 examples\n",
      "processed 90000 examples\n",
      "processed 91000 examples\n",
      "processed 92000 examples\n",
      "processed 93000 examples\n",
      "processed 94000 examples\n",
      "processed 95000 examples\n",
      "processed 96000 examples\n",
      "processed 97000 examples\n",
      "processed 98000 examples\n",
      "processed 99000 examples\n",
      "processed 100000 examples\n",
      "processed 101000 examples\n",
      "processed 102000 examples\n",
      "processed 103000 examples\n",
      "processed 104000 examples\n",
      "processed 105000 examples\n",
      "processed 106000 examples\n",
      "processed 107000 examples\n",
      "processed 108000 examples\n",
      "processed 109000 examples\n",
      "processed 110000 examples\n",
      "processed 111000 examples\n",
      "processed 112000 examples\n",
      "processed 113000 examples\n",
      "processed 114000 examples\n",
      "processed 115000 examples\n",
      "processed 116000 examples\n",
      "processed 117000 examples\n",
      "processed 118000 examples\n",
      "processed 119000 examples\n",
      "processed 120000 examples\n",
      "processed 121000 examples\n",
      "processed 122000 examples\n",
      "processed 123000 examples\n",
      "processed 124000 examples\n",
      "processed 125000 examples\n",
      "processed 126000 examples\n",
      "unpickled total 126100 examples\n",
      "saved total 25220 examples only for byte\n",
      "processed 1000 examples\n",
      "processed 2000 examples\n",
      "processed 3000 examples\n",
      "processed 4000 examples\n",
      "processed 5000 examples\n",
      "processed 6000 examples\n",
      "processed 7000 examples\n",
      "processed 8000 examples\n",
      "processed 9000 examples\n",
      "processed 10000 examples\n",
      "processed 11000 examples\n",
      "processed 12000 examples\n",
      "processed 13000 examples\n",
      "processed 14000 examples\n",
      "processed 15000 examples\n",
      "processed 16000 examples\n",
      "processed 17000 examples\n",
      "processed 18000 examples\n",
      "processed 19000 examples\n",
      "processed 20000 examples\n",
      "processed 21000 examples\n",
      "processed 22000 examples\n",
      "processed 23000 examples\n",
      "unpickled total 23400 examples\n",
      "saved total 4680 examples only for byte\n",
      "processed 1000 examples\n",
      "processed 2000 examples\n",
      "processed 3000 examples\n",
      "processed 4000 examples\n",
      "processed 5000 examples\n",
      "processed 6000 examples\n",
      "processed 7000 examples\n",
      "unpickled total 7800 examples\n",
      "saved total 1560 examples only for byte\n"
     ]
    }
   ],
   "source": [
    "# get Dataset\n",
    "data_dir = 'src/data/dataset/allfonts/'\n",
    "train_set = FontDataset(PickledImageProvider(data_dir+'train.obj'))\n",
    "valid_set = FontDataset(PickledImageProvider(data_dir+'val.obj'))\n",
    "test_set = FontDataset(PickledImageProvider(data_dir+'test.obj'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get idx samplers\n",
    "train_set_size = len(train_set)\n",
    "valid_set_size = len(valid_set)\n",
    "train_idxs = list(range(train_set_size))\n",
    "valid_idxs = list(range(valid_set_size))\n",
    "if shuffle_dataset:\n",
    "    np.random.seed(random_seed)\n",
    "    np.random.shuffle(train_idxs)\n",
    "    np.random.shuffle(valid_idxs)\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_idxs)\n",
    "valid_sampler = SubsetRandomSampler(valid_idxs)\n",
    "\n",
    "# get data_loaders\n",
    "train_loader = DataLoader(train_set, \n",
    "                      batch_size=batch_size,\n",
    "                      sampler=train_sampler\n",
    "                      )\n",
    "valid_loader = DataLoader(valid_set,\n",
    "                        batch_size=batch_size,\n",
    "                        sampler=valid_sampler\n",
    "                        )\n",
    "test_loader = DataLoader(test_set,\n",
    "                        batch_size=len(test_set)\n",
    "                        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_category_size=5; input_alpha_size=52; input_font_size=128*128\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()     \n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_font_size, 8192),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(8192, 4096),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(4096, 1024),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Sigmoid()\n",
    "        )        \n",
    "    \n",
    "    def forward(self, x_font):\n",
    "        x_font = x_font.view(x_font.shape[0], -1)\n",
    "        out = self.model(x_font)\n",
    "        \n",
    "        return out.squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.layers import Encoder_base, Decoder_base\n",
    "#from src.models.layers import Encoder_category, Decoder_category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 훈철 코드 \n",
    "input_category_size=5; input_alpha_size=52; input_font_size=128*128; input_noise_size = 16;\n",
    "output_font_size=128*128\n",
    "\n",
    "class AE_base(nn.Module):\n",
    "    def __init__(self, category_size=5, alpha_size=52, font_size=128*128,z_size=64):\n",
    "        super(AE_base, self).__init__()\n",
    "        self.Encoder = Encoder_base(input_category_size=category_size,\n",
    "                                    input_alpha_size=alpha_size,\n",
    "                                    input_font_size=font_size,\n",
    "                                    z_size=z_size)\n",
    "        self.Decoder = Decoder_base(z_latent_size=z_size,\n",
    "                                    z_category_size=category_size,\n",
    "                                    z_alpha_size=alpha_size,\n",
    "                                    output_font_size=font_size)\n",
    "    \n",
    "    def forward(self, x_font, alpha_vector, category_vector=None):\n",
    "        \n",
    "        origin_shape = x_font.shape\n",
    "        x_font = x_font.view(x_font.shape[0], -1)\n",
    "        alpha_vector = alpha_vector.view(alpha_vector.shape[0], -1)\n",
    "        \n",
    "        if category_vector is not None:\n",
    "            category_vector = category_vector.view(category_vector.shape[0], -1)\n",
    "        \n",
    "        if category_vector is not None:\n",
    "            x = torch.cat([x_font, category_vector, alpha_vector], dim=1)\n",
    "        else:\n",
    "            x = torch.cat([x_font, alpha_vector], dim=1)\n",
    "        \n",
    "        z_latent = self.Encoder(x)\n",
    "        \n",
    "        #z_latent = z_latent.view(z_latent.shape[0], -1)\n",
    "        if category_vector is not None:    \n",
    "            z = torch.cat([z_latent, category_vector, alpha_vector], dim=1)\n",
    "        else:\n",
    "            z = torch.cat([z_latent, alpha_vector], dim=1)\n",
    "        \n",
    "        x_hat = self.Decoder(z)\n",
    "        x_hat = x_hat.view(origin_shape)\n",
    "        \n",
    "        return x_hat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator = Discriminator().cuda()\n",
    "#generator = Generator().cuda()\n",
    "generator = AE_base().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "bce_criterion = nn.BCELoss().cuda()\n",
    "mse_criterion = nn.MSELoss().cuda()\n",
    "\n",
    "d_optimizer = torch.optim.Adam(discriminator.parameters(), lr=lr)\n",
    "g_optimizer = torch.optim.Adam(generator.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_discriminator(batch_size, generator, discriminator, images, alpha_vector, category_vector):\n",
    "    d_optimizer.zero_grad()\n",
    "    \n",
    "    #train with the real image (discriminator)\n",
    "    outputs = discriminator(images)\n",
    "    real_loss = bce_criterion(outputs, Variable(torch.ones(batch_size)).cuda()) \n",
    "    real_score = outputs \n",
    "        \n",
    "    #train with the real image (generator)\n",
    "    fake_images = generator(images, alpha_vector, category_vector) \n",
    "    outputs = discriminator(fake_images)\n",
    "    fake_loss = bce_criterion(outputs, Variable(torch.zeros(batch_size)).cuda()) \n",
    "    fake_score = outputs \n",
    "\n",
    "    d_loss = real_loss + fake_loss \n",
    "    d_loss.backward() \n",
    "    d_optimizer.step()\n",
    "    \n",
    "    return d_loss, real_score, fake_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_generator(batch_size, alpha_vector, category_vector, images, generator, discriminator):\n",
    "    \n",
    "    g_optimizer.zero_grad()\n",
    "   \n",
    "    #get output from generator and discriminator   \n",
    "    fake_images = generator(images, alpha_vector, category_vector) \n",
    "    outputs = discriminator(fake_images)\n",
    "        \n",
    "    g_mse_loss= mse_criterion(fake_images, images)\n",
    "    g_bce_loss = bce_criterion(outputs, Variable(torch.ones(batch_size)).cuda())\n",
    "    \n",
    "    g_loss = g_mse_loss + g_bce_loss\n",
    "    g_loss.backward()\n",
    "    g_optimizer.step()\n",
    "    \n",
    "    return g_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0  / \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-6e5633355c35>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[1;31m# Train the generator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m         \u001b[0mg_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_generator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtemp_batch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha_vector\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcategory_vector\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m1000\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\" / \"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-17-d2d7ba069dd9>\u001b[0m in \u001b[0;36mtrain_generator\u001b[1;34m(batch_size, alpha_vector, category_vector, images, generator, discriminator)\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfake_images\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0mg_mse_loss\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mmse_criterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfake_images\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m     \u001b[0mg_bce_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbce_criterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mVariable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mones\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\subin2\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    492\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 493\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    494\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\subin2\\lib\\site-packages\\torch\\nn\\modules\\loss.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m    441\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mweak_script_method\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    442\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 443\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    444\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    445\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\subin2\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mmse_loss\u001b[1;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[0;32m   2255\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2256\u001b[0m         \u001b[0mexpanded_input\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexpanded_target\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2257\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexpanded_input\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexpanded_target\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2258\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2259\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# set number of epochs and initialize figure counter\n",
    "num_epochs = 100\n",
    "num_batches = len(train_loader)\n",
    "num_fig = 0\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for n, (vectors, font, _) in enumerate(train_loader):\n",
    "                \n",
    "        alpha_vector = vectors['alphabet_vector']\n",
    "        category_vector = vectors['category_vector']\n",
    "        \n",
    "        images, alpha_vector = font.float().to(device), alpha_vector.float().to(device)\n",
    "        category_vector = category_vector.float().to(device)\n",
    " \n",
    "        temp_batch_size = alpha_vector.size()[0]\n",
    "    \n",
    "        # Train the discriminator\n",
    "        d_loss, real_score, fake_score =train_discriminator(temp_batch_size, generator, discriminator, images, alpha_vector, category_vector)\n",
    "       \n",
    "        \n",
    "        # Train the generator\n",
    "        g_loss = train_generator(temp_batch_size, alpha_vector, category_vector, images, generator, discriminator)\n",
    "     \n",
    "        if n % 1000 == 0: print(n, \" / \")\n",
    "            \n",
    "        \n",
    "    if epoch % 1 == 0:\n",
    "        print('Epoch [%d/%d], Step[%d/%d], d_loss: %.4f, g_loss: %.4f, ' \n",
    "              'D(x): %.2f, D(G(z)): %.2f' \n",
    "              %(epoch + 1, num_epochs, n+1, num_batches, d_loss.item(), g_loss.item(),\n",
    "                real_score.data.mean(), fake_score.data.mean()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save\n",
    "#savePath = \"D:/투빅스 2019/discriminator_1214.pth\"\n",
    "#torch.save(discriminator.state_dict(), savePath)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load\n",
    "#new_model = TestModel()\n",
    "#new_model.load_state_dict(torch.load(\"./output/test_model.pth\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = Variable(torch.randn(52, 16)).cuda()\n",
    "fake_alpha_vector =  Variable(torch.FloatTensor(np.eye(52))).cuda()\n",
    "fake_category_vector =  Variable(torch.FloatTensor(np.eye(5)[np.random.choice(5,52)])).cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = generator(z, fake_alpha_vector, fake_category_vector).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = make_grid(images, nrow=52, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = images.view(images.size(0), 128, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(images[3].cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 200))\n",
    "for i in range(52):\n",
    "    plt.subplot(50, 2, i+1)\n",
    "    plt.imshow(images[i].cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
